[2023-11-30 02:04:57,379] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task036
searching parameters: task036_10_10_50_0.5_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_10_50_0.5_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_10_50_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-30 02:05:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:05:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 02:06:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:07:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 24
expected_example_num: 100
selection_ratio: 0.24
finetune_vicuna!
{'loss': 2.3525, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.3434, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0792, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 173.5808, 'train_samples_per_second': 0.415, 'train_steps_per_second': 0.069, 'train_loss': 0.9250230590502421, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-30 02:10:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:11:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_10_50_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.4661592399715474

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_10_50_0.5_0.35_3_1/generated_contents/1
INFO 11-30 02:11:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:11:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_10_50_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.4722898582396806

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_10_50_0.5_0.35_3_1/generated_contents/2
INFO 11-30 02:12:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:12:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_10_50_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.47079784627725924

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_10_50_0.5_0.35_3_1/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-8 /data2/cyzhao/best_ckpt/NI_task036_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_10_50_0.5_0.35_3_1/checkpoint-12
searching parameters: task036_30_10_45_0.5_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-30 02:12:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:13:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 02:17:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:17:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 69
expected_example_num: 300
selection_ratio: 0.23
finetune_vicuna!
{'loss': 1.6793, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 1.4764, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 1.1036, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2734, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.4436, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1413, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0211, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.1246, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0687, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 185.8196, 'train_samples_per_second': 1.114, 'train_steps_per_second': 0.194, 'train_loss': 0.5924355857488182, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-30 02:21:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:21:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_45_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.48861197266798667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.35_3_1/generated_contents/1
INFO 11-30 02:22:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:22:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_45_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.4832417197772943

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.35_3_1/generated_contents/2
INFO 11-30 02:22:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:22:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_45_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.48230878893619406

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task036_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task036_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.35_3_1/checkpoint-36
searching parameters: task036_10_20_40_0.4_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-30 02:23:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:23:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 02:25:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:25:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 27
expected_example_num: 200
selection_ratio: 0.135
finetune_vicuna!
{'loss': 1.611, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.6652, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.3001, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 172.8169, 'train_samples_per_second': 0.469, 'train_steps_per_second': 0.087, 'train_loss': 0.7023818383614222, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-30 02:29:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:29:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.4_0.35_3_1 epoch 1

------------------------------------------------

0.4815287210871812

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.35_3_1/generated_contents/1
INFO 11-30 02:30:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:30:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.4_0.35_3_1 epoch 2

------------------------------------------------

0.5099972044943417

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.35_3_1/generated_contents/2
INFO 11-30 02:30:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:30:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.4_0.35_3_1 epoch 3

------------------------------------------------

0.5081933996647723

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task036_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-10 /data2/cyzhao/best_ckpt/NI_task036_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.35_3_1/checkpoint-15
searching parameters: task036_20_20_40_0.6_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.6_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.6_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-30 02:31:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:31:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 02:35:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:35:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 91
expected_example_num: 400
selection_ratio: 0.2275
finetune_vicuna!
{'loss': 1.6397, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 0.9828, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.7925, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.6929, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1927, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.3487, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2453, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.1703, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0321, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.0514, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0436, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.026, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 193.4541, 'train_samples_per_second': 1.411, 'train_steps_per_second': 0.248, 'train_loss': 0.4348262995481491, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-30 02:40:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:40:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_40_0.6_0.35_3_1 epoch 1

------------------------------------------------

0.4867459666817195

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.6_0.35_3_1/generated_contents/1
INFO 11-30 02:40:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:40:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_40_0.6_0.35_3_1 epoch 2

------------------------------------------------

0.47188831658196395

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.6_0.35_3_1/generated_contents/2
INFO 11-30 02:41:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:41:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_40_0.6_0.35_3_1 epoch 3

------------------------------------------------

0.48335374308323403

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.6_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.6_0.35_3_1/checkpoint-48
searching parameters: task036_20_15_50_0.5_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_15_50_0.5_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_15_50_0.5_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-30 02:41:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:42:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 02:45:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:45:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 70
expected_example_num: 300
selection_ratio: 0.23333333333333334
finetune_vicuna!
{'loss': 2.2517, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 1.8052, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 1.0118, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2901, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1402, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.3315, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0586, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0713, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0897, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 187.707, 'train_samples_per_second': 1.119, 'train_steps_per_second': 0.192, 'train_loss': 0.6722213079531988, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-30 02:49:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:49:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_15_50_0.5_0.3_3_1 epoch 1

------------------------------------------------

0.46915998943305515

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_15_50_0.5_0.3_3_1/generated_contents/1
INFO 11-30 02:50:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:50:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_15_50_0.5_0.3_3_1 epoch 2

------------------------------------------------

0.4827789172001201

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_15_50_0.5_0.3_3_1/generated_contents/2
INFO 11-30 02:50:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:51:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_15_50_0.5_0.3_3_1 epoch 3

------------------------------------------------

0.486656865943636

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_15_50_0.5_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_15_50_0.5_0.3_3_1/checkpoint-36
searching parameters: task036_10_15_45_0.6_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_15_45_0.6_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_15_45_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-30 02:51:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:51:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 02:53:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:53:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 28
expected_example_num: 150
selection_ratio: 0.18666666666666668
finetune_vicuna!
{'loss': 1.869, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.7674, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.291, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 170.653, 'train_samples_per_second': 0.492, 'train_steps_per_second': 0.088, 'train_loss': 0.7893944501876831, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-30 02:57:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:57:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_15_45_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.4083556837779151

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_15_45_0.6_0.4_3_1/generated_contents/1
INFO 11-30 02:57:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:58:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_15_45_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.4697600635929906

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_15_45_0.6_0.4_3_1/generated_contents/2
INFO 11-30 02:58:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:58:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_15_45_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.4709116051202764

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_15_45_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_15_45_0.6_0.4_3_1/checkpoint-15
searching parameters: task036_30_10_45_0.5_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-30 02:59:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 02:59:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 03:03:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:04:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 69
expected_example_num: 300
selection_ratio: 0.23
finetune_vicuna!
{'loss': 2.1075, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 1.6777, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.9203, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.3135, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.4888, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1557, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.025, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.1426, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0559, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 187.3397, 'train_samples_per_second': 1.105, 'train_steps_per_second': 0.192, 'train_loss': 0.6541092650343975, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-30 03:07:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:08:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_45_0.5_0.4_3_1 epoch 1

------------------------------------------------

0.4923303072938512

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.4_3_1/generated_contents/1
INFO 11-30 03:08:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:08:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_45_0.5_0.4_3_1 epoch 2

------------------------------------------------

0.4933282439556862

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.4_3_1/generated_contents/2
INFO 11-30 03:09:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:09:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_45_0.5_0.4_3_1 epoch 3

------------------------------------------------

0.4837346515978072

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_45_0.5_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_45_0.5_0.4_3_1/checkpoint-36
searching parameters: task036_10_15_40_0.6_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_15_40_0.6_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_15_40_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-30 03:09:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:10:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 03:11:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:11:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 28
expected_example_num: 150
selection_ratio: 0.18666666666666668
finetune_vicuna!
{'loss': 2.0926, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.7384, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1007, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 168.1673, 'train_samples_per_second': 0.5, 'train_steps_per_second': 0.089, 'train_loss': 0.7955590277910233, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-30 03:15:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:15:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_15_40_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.4516175748893307

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_15_40_0.6_0.4_3_1/generated_contents/1
INFO 11-30 03:16:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:16:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_15_40_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.47159419908381955

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_15_40_0.6_0.4_3_1/generated_contents/2
INFO 11-30 03:16:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:16:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_15_40_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.47507753487283555

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_15_40_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_15_40_0.6_0.4_3_1/checkpoint-15
searching parameters: task036_30_15_45_0.7_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_15_45_0.7_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_15_45_0.7_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-30 03:17:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:17:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 03:22:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:22:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 110
expected_example_num: 450
selection_ratio: 0.24444444444444444
finetune_vicuna!
{'loss': 2.3502, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 1.344, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.9399, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.6793, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.5398, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.1919, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.1879, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.2917, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.0717, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.0744, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0209, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.0504, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.038, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0608, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 194.8215, 'train_samples_per_second': 1.694, 'train_steps_per_second': 0.293, 'train_loss': 0.48008716365387827, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-30 03:27:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:27:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_15_45_0.7_0.35_3_1 epoch 1

------------------------------------------------

0.4813741164529177

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_15_45_0.7_0.35_3_1/generated_contents/1
INFO 11-30 03:27:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:28:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_15_45_0.7_0.35_3_1 epoch 2

------------------------------------------------

0.4878511387307272

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_15_45_0.7_0.35_3_1/generated_contents/2
INFO 11-30 03:28:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:28:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_15_45_0.7_0.35_3_1 epoch 3

------------------------------------------------

0.4925241005630743

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_15_45_0.7_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_15_45_0.7_0.35_3_1/checkpoint-57
searching parameters: task036_30_10_50_0.7_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_10_50_0.7_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_30_10_50_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-30 03:29:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:29:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 03:33:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:33:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 72
expected_example_num: 300
selection_ratio: 0.24
finetune_vicuna!
{'loss': 1.5318, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 1.2893, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 1.1453, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.34, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.3063, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.4422, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1609, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.111, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0722, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 182.6376, 'train_samples_per_second': 1.183, 'train_steps_per_second': 0.197, 'train_loss': 0.5998885358373324, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-30 03:37:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:37:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_50_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.46256265713441824

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_50_0.7_0.3_3_1/generated_contents/1
INFO 11-30 03:38:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:38:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_50_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.4688983682612251

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_50_0.7_0.3_3_1/generated_contents/2
INFO 11-30 03:38:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:39:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_30_10_50_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.4639361153589724

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_30_10_50_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_30_10_50_0.7_0.3_3_1/checkpoint-36
searching parameters: task036_10_20_40_0.4_0.35_3_1
searching parameters: task036_10_20_40_0.4_0.35_3_1
searching parameters: task036_10_20_40_0.4_0.35_3_1
searching parameters: task036_10_20_40_0.8_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.8_0.35_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.8_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-30 03:39:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:39:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 03:41:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:41:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 41
expected_example_num: 200
selection_ratio: 0.205
finetune_vicuna!
{'loss': 1.9981, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 1.0686, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2903, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.3098, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.06, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 172.1239, 'train_samples_per_second': 0.715, 'train_steps_per_second': 0.122, 'train_loss': 0.7101955902097481, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-30 03:45:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:45:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.8_0.35_3_1 epoch 1

------------------------------------------------

0.48081250139145054

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.8_0.35_3_1/generated_contents/1
INFO 11-30 03:46:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:46:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.8_0.35_3_1 epoch 2

------------------------------------------------

0.47975910593684157

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.8_0.35_3_1/generated_contents/2
INFO 11-30 03:46:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:47:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.8_0.35_3_1 epoch 3

------------------------------------------------

0.49315568873091764

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.8_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.8_0.35_3_1/checkpoint-21
searching parameters: task036_10_20_40_0.4_0.35_3_1
searching parameters: task036_20_20_40_0.4_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.4_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-30 03:47:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:47:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 03:51:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:51:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 87
expected_example_num: 400
selection_ratio: 0.2175
finetune_vicuna!
{'loss': 1.9932, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 1.0971, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.7967, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.5279, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.1229, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1381, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1672, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.2072, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0393, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0421, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0421, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 187.3935, 'train_samples_per_second': 1.393, 'train_steps_per_second': 0.24, 'train_loss': 0.4664135005739, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-30 03:55:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:55:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_40_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.4929533628203498

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.4_0.3_3_1/generated_contents/1
INFO 11-30 03:56:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:56:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_40_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.4787871287516204

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.4_0.3_3_1/generated_contents/2
INFO 11-30 03:56:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:57:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_40_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.4789345944004863

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_40_0.4_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_40_0.4_0.3_3_1/checkpoint-45
searching parameters: task036_10_20_40_0.4_0.35_3_1
searching parameters: task036_10_20_40_0.8_0.35_3_1
searching parameters: task036_10_20_40_0.4_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.3_3_1
/home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-30 03:57:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 03:57:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 03:59:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:00:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 46
expected_example_num: 200
selection_ratio: 0.23
finetune_vicuna!
{'loss': 2.3266, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.8399, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2893, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2187, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0858, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0425, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 169.8846, 'train_samples_per_second': 0.812, 'train_steps_per_second': 0.141, 'train_loss': 0.6338069140911102, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-30 04:03:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:03:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.4770881317668965

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.3_3_1/generated_contents/1
INFO 11-30 04:04:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:04:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.4799555800812041

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.3_3_1/generated_contents/2
INFO 11-30 04:04:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:05:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_10_20_40_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.4852705037690415

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_10_20_40_0.4_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_10_20_40_0.4_0.3_3_1/checkpoint-24
searching parameters: task036_20_20_50_0.4_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_20_50_0.4_0.4_3_1
/home/cyzhao/NI_task036_exp_1/task036_20_20_50_0.4_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-30 04:05:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:05:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-30 04:09:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:09:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 87
expected_example_num: 400
selection_ratio: 0.2175
finetune_vicuna!
{'loss': 1.9811, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 1.0607, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.7997, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.5064, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.151, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1678, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1618, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.2559, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0325, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0555, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0407, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 189.013, 'train_samples_per_second': 1.381, 'train_steps_per_second': 0.238, 'train_loss': 0.46922048926353455, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-30 04:13:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:14:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_50_0.4_0.4_3_1 epoch 1

------------------------------------------------

0.49037968647793195

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_50_0.4_0.4_3_1/generated_contents/1
INFO 11-30 04:14:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:14:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_50_0.4_0.4_3_1 epoch 2

------------------------------------------------

0.475760257618411

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_50_0.4_0.4_3_1/generated_contents/2
INFO 11-30 04:15:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:15:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task036_20_20_50_0.4_0.4_3_1 epoch 3

------------------------------------------------

0.47773667518243534

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/task036_20_20_50_0.4_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task036_20_20_50_0.4_0.4_3_1/checkpoint-45
{'generation_epochs': 10, 'generation_batch_size': 20, 'generation_top_k': 40, 'generation_temperature': 0.4, 'min_frequency': 0.35, 'training_epochs': 3}
test best ckpt.
INFO 11-30 04:15:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task036_exp_1', tokenizer='/data2/cyzhao/best_ckpt/NI_task036_exp_1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-30 04:16:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task036_exp_1

------------------------------------------------

0.5009224236742227

------------------------------------------------


The best ckpt on test set gain 0.5009224236742227
Genrated contents are stored in /home/cyzhao/NI_task036_exp_1/best_ckpt_generated_content
