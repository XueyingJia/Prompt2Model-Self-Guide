[2023-11-24 06:11:55,066] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NI_task935
searching parameters: NI_task935_20_20_40_0.9_0.35_125_4
/home/cyzhao/NI_task935_exp_2/NI_task935_20_20_40_0.9_0.35_125_4
/home/cyzhao/NI_task935_exp_2/NI_task935_20_20_40_0.9_0.35_125_4/config.json
generate_and_write_inputs!
INFO 11-24 06:12:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:12:19 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 06:18:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:18:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 287
expected_example_num: 400
selection_ratio: 0.7175
finetune_vicuna!
{'loss': 2.4572, 'learning_rate': 4.8611111111111115e-05, 'epoch': 0.11}
{'loss': 0.5819, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.22}
{'loss': 0.2377, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.33}
{'loss': 0.5434, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.44}
{'loss': 0.3083, 'learning_rate': 4.305555555555556e-05, 'epoch': 0.56}
{'loss': 0.3686, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.67}
{'loss': 0.1713, 'learning_rate': 4.027777777777778e-05, 'epoch': 0.78}
{'loss': 0.4305, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.89}
{'loss': 0.2661, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.0}
{'loss': 0.2158, 'learning_rate': 3.611111111111111e-05, 'epoch': 1.11}
{'loss': 0.2139, 'learning_rate': 3.472222222222222e-05, 'epoch': 1.22}
{'loss': 0.2776, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.33}
{'loss': 0.171, 'learning_rate': 3.194444444444444e-05, 'epoch': 1.44}
{'loss': 0.1808, 'learning_rate': 3.055555555555556e-05, 'epoch': 1.56}
{'loss': 0.1187, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.67}
{'loss': 0.1864, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.78}
{'loss': 0.0957, 'learning_rate': 2.6388888888888892e-05, 'epoch': 1.89}
{'loss': 0.1342, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.0789, 'learning_rate': 2.361111111111111e-05, 'epoch': 2.11}
{'loss': 0.0987, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.22}
{'loss': 0.06, 'learning_rate': 2.0833333333333336e-05, 'epoch': 2.33}
{'loss': 0.0587, 'learning_rate': 1.9444444444444445e-05, 'epoch': 2.44}
{'loss': 0.1266, 'learning_rate': 1.8055555555555555e-05, 'epoch': 2.56}
{'loss': 0.1683, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.67}
{'loss': 0.1034, 'learning_rate': 1.527777777777778e-05, 'epoch': 2.78}
{'loss': 0.0791, 'learning_rate': 1.388888888888889e-05, 'epoch': 2.89}
{'loss': 0.0748, 'learning_rate': 1.25e-05, 'epoch': 3.0}
{'loss': 0.0483, 'learning_rate': 1.1111111111111112e-05, 'epoch': 3.11}
{'loss': 0.0326, 'learning_rate': 9.722222222222223e-06, 'epoch': 3.22}
{'loss': 0.0525, 'learning_rate': 8.333333333333334e-06, 'epoch': 3.33}
{'loss': 0.0451, 'learning_rate': 6.944444444444445e-06, 'epoch': 3.44}
{'loss': 0.0346, 'learning_rate': 5.555555555555556e-06, 'epoch': 3.56}
{'loss': 0.0367, 'learning_rate': 4.166666666666667e-06, 'epoch': 3.67}
{'loss': 0.0141, 'learning_rate': 2.777777777777778e-06, 'epoch': 3.78}
{'loss': 0.0411, 'learning_rate': 1.388888888888889e-06, 'epoch': 3.89}
{'loss': 0.0329, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 440.0933, 'train_samples_per_second': 2.609, 'train_steps_per_second': 0.327, 'train_loss': 0.22625698977046543, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-72/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-108/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-144/optimizer.pt
validate!
last validate 0.
INFO 11-24 06:27:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:27:45 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_20_40_0.9_0.35_125_4 epoch 1

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_20_40_0.9_0.35_125_4/generated_contents/1
INFO 11-24 06:28:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-72', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-72', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:28:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_20_40_0.9_0.35_125_4 epoch 2

------------------------------------------------

0.622

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_20_40_0.9_0.35_125_4/generated_contents/2
INFO 11-24 06:29:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-108', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-108', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:29:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_20_40_0.9_0.35_125_4 epoch 3

------------------------------------------------

0.629

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_20_40_0.9_0.35_125_4/generated_contents/3
INFO 11-24 06:30:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-144', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-144', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:30:31 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_20_40_0.9_0.35_125_4 epoch 4

------------------------------------------------

0.631

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_20_40_0.9_0.35_125_4/generated_contents/4
mv /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-144 /data2/cyzhao/best_ckpt/NI_task935_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-36
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-72
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_20_40_0.9_0.35_125_4/checkpoint-108
searching parameters: NI_task935_40_30_50_0.8_0.35_120_5
/home/cyzhao/NI_task935_exp_2/NI_task935_40_30_50_0.8_0.35_120_5
/home/cyzhao/NI_task935_exp_2/NI_task935_40_30_50_0.8_0.35_120_5/config.json
generate_and_write_inputs!
INFO 11-24 06:31:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:31:32 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 06:44:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:44:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 885
expected_example_num: 1200
selection_ratio: 0.7375
finetune_vicuna!
{'loss': 2.6904, 'learning_rate': 4.963963963963964e-05, 'epoch': 0.04}
{'loss': 0.8093, 'learning_rate': 4.927927927927928e-05, 'epoch': 0.07}
{'loss': 0.4957, 'learning_rate': 4.891891891891892e-05, 'epoch': 0.11}
{'loss': 0.7829, 'learning_rate': 4.855855855855856e-05, 'epoch': 0.14}
{'loss': 0.1564, 'learning_rate': 4.8198198198198205e-05, 'epoch': 0.18}
{'loss': 0.206, 'learning_rate': 4.783783783783784e-05, 'epoch': 0.22}
{'loss': 0.2348, 'learning_rate': 4.747747747747748e-05, 'epoch': 0.25}
{'loss': 0.2642, 'learning_rate': 4.711711711711712e-05, 'epoch': 0.29}
{'loss': 0.541, 'learning_rate': 4.675675675675676e-05, 'epoch': 0.32}
{'loss': 0.5401, 'learning_rate': 4.6396396396396394e-05, 'epoch': 0.36}
{'loss': 0.4046, 'learning_rate': 4.603603603603604e-05, 'epoch': 0.4}
{'loss': 0.5352, 'learning_rate': 4.567567567567568e-05, 'epoch': 0.43}
{'loss': 0.2658, 'learning_rate': 4.531531531531532e-05, 'epoch': 0.47}
{'loss': 0.2382, 'learning_rate': 4.495495495495496e-05, 'epoch': 0.5}
{'loss': 0.2289, 'learning_rate': 4.4594594594594596e-05, 'epoch': 0.54}
{'loss': 0.2494, 'learning_rate': 4.423423423423423e-05, 'epoch': 0.58}
{'loss': 0.2177, 'learning_rate': 4.3873873873873876e-05, 'epoch': 0.61}
{'loss': 0.2676, 'learning_rate': 4.351351351351351e-05, 'epoch': 0.65}
{'loss': 0.2166, 'learning_rate': 4.3153153153153156e-05, 'epoch': 0.68}
{'loss': 0.2396, 'learning_rate': 4.27927927927928e-05, 'epoch': 0.72}
{'loss': 0.2499, 'learning_rate': 4.2432432432432435e-05, 'epoch': 0.76}
{'loss': 0.1901, 'learning_rate': 4.207207207207207e-05, 'epoch': 0.79}
{'loss': 0.1923, 'learning_rate': 4.1711711711711715e-05, 'epoch': 0.83}
{'loss': 0.1816, 'learning_rate': 4.135135135135135e-05, 'epoch': 0.86}
{'loss': 0.218, 'learning_rate': 4.099099099099099e-05, 'epoch': 0.9}
{'loss': 0.2136, 'learning_rate': 4.063063063063063e-05, 'epoch': 0.94}
{'loss': 0.2667, 'learning_rate': 4.0270270270270274e-05, 'epoch': 0.97}
{'loss': 0.1169, 'learning_rate': 3.990990990990991e-05, 'epoch': 1.01}
{'loss': 0.1358, 'learning_rate': 3.9549549549549554e-05, 'epoch': 1.05}
{'loss': 0.1673, 'learning_rate': 3.918918918918919e-05, 'epoch': 1.08}
{'loss': 0.2582, 'learning_rate': 3.882882882882883e-05, 'epoch': 1.12}
{'loss': 0.2491, 'learning_rate': 3.846846846846847e-05, 'epoch': 1.15}
{'loss': 0.241, 'learning_rate': 3.8108108108108106e-05, 'epoch': 1.19}
{'loss': 0.2455, 'learning_rate': 3.774774774774775e-05, 'epoch': 1.23}
{'loss': 0.1634, 'learning_rate': 3.738738738738739e-05, 'epoch': 1.26}
{'loss': 0.2797, 'learning_rate': 3.702702702702703e-05, 'epoch': 1.3}
{'loss': 0.2751, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}
{'loss': 0.2122, 'learning_rate': 3.630630630630631e-05, 'epoch': 1.37}
{'loss': 0.1029, 'learning_rate': 3.5945945945945945e-05, 'epoch': 1.41}
{'loss': 0.1359, 'learning_rate': 3.558558558558558e-05, 'epoch': 1.44}
{'loss': 0.0814, 'learning_rate': 3.5225225225225225e-05, 'epoch': 1.48}
{'loss': 0.1575, 'learning_rate': 3.486486486486487e-05, 'epoch': 1.51}
{'loss': 0.1857, 'learning_rate': 3.4504504504504505e-05, 'epoch': 1.55}
{'loss': 0.1191, 'learning_rate': 3.414414414414415e-05, 'epoch': 1.59}
{'loss': 0.1567, 'learning_rate': 3.3783783783783784e-05, 'epoch': 1.62}
{'loss': 0.1909, 'learning_rate': 3.342342342342342e-05, 'epoch': 1.66}
{'loss': 0.1663, 'learning_rate': 3.3063063063063064e-05, 'epoch': 1.69}
{'loss': 0.1707, 'learning_rate': 3.27027027027027e-05, 'epoch': 1.73}
{'loss': 0.1112, 'learning_rate': 3.2342342342342344e-05, 'epoch': 1.77}
{'loss': 0.2032, 'learning_rate': 3.198198198198199e-05, 'epoch': 1.8}
{'loss': 0.1315, 'learning_rate': 3.162162162162162e-05, 'epoch': 1.84}
{'loss': 0.1791, 'learning_rate': 3.1261261261261266e-05, 'epoch': 1.87}
{'loss': 0.1611, 'learning_rate': 3.09009009009009e-05, 'epoch': 1.91}
{'loss': 0.1224, 'learning_rate': 3.054054054054054e-05, 'epoch': 1.95}
{'loss': 0.2953, 'learning_rate': 3.0180180180180183e-05, 'epoch': 1.98}
{'loss': 0.1412, 'learning_rate': 2.9819819819819822e-05, 'epoch': 2.02}
{'loss': 0.0992, 'learning_rate': 2.945945945945946e-05, 'epoch': 2.05}
{'loss': 0.2083, 'learning_rate': 2.9099099099099102e-05, 'epoch': 2.09}
{'loss': 0.1519, 'learning_rate': 2.8738738738738742e-05, 'epoch': 2.13}
{'loss': 0.1965, 'learning_rate': 2.8378378378378378e-05, 'epoch': 2.16}
{'loss': 0.0745, 'learning_rate': 2.801801801801802e-05, 'epoch': 2.2}
{'loss': 0.0553, 'learning_rate': 2.7657657657657658e-05, 'epoch': 2.23}
{'loss': 0.0862, 'learning_rate': 2.7297297297297298e-05, 'epoch': 2.27}
{'loss': 0.1167, 'learning_rate': 2.693693693693694e-05, 'epoch': 2.31}
{'loss': 0.0445, 'learning_rate': 2.6576576576576577e-05, 'epoch': 2.34}
{'loss': 0.091, 'learning_rate': 2.6216216216216217e-05, 'epoch': 2.38}
{'loss': 0.0888, 'learning_rate': 2.585585585585586e-05, 'epoch': 2.41}
{'loss': 0.1417, 'learning_rate': 2.5495495495495497e-05, 'epoch': 2.45}
{'loss': 0.0423, 'learning_rate': 2.5135135135135133e-05, 'epoch': 2.49}
{'loss': 0.0882, 'learning_rate': 2.4774774774774777e-05, 'epoch': 2.52}
{'loss': 0.0755, 'learning_rate': 2.4414414414414416e-05, 'epoch': 2.56}
{'loss': 0.0756, 'learning_rate': 2.4054054054054056e-05, 'epoch': 2.59}
{'loss': 0.1053, 'learning_rate': 2.3693693693693696e-05, 'epoch': 2.63}
{'loss': 0.078, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}
{'loss': 0.0804, 'learning_rate': 2.2972972972972976e-05, 'epoch': 2.7}
{'loss': 0.0372, 'learning_rate': 2.2612612612612612e-05, 'epoch': 2.74}
{'loss': 0.1296, 'learning_rate': 2.2252252252252255e-05, 'epoch': 2.77}
{'loss': 0.0596, 'learning_rate': 2.1891891891891895e-05, 'epoch': 2.81}
{'loss': 0.0739, 'learning_rate': 2.153153153153153e-05, 'epoch': 2.85}
{'loss': 0.0541, 'learning_rate': 2.117117117117117e-05, 'epoch': 2.88}
{'loss': 0.0725, 'learning_rate': 2.0810810810810815e-05, 'epoch': 2.92}
{'loss': 0.0584, 'learning_rate': 2.045045045045045e-05, 'epoch': 2.95}
{'loss': 0.0317, 'learning_rate': 2.009009009009009e-05, 'epoch': 2.99}
{'loss': 0.0228, 'learning_rate': 1.972972972972973e-05, 'epoch': 3.03}
{'loss': 0.0096, 'learning_rate': 1.936936936936937e-05, 'epoch': 3.06}
{'loss': 0.0147, 'learning_rate': 1.900900900900901e-05, 'epoch': 3.1}
{'loss': 0.0577, 'learning_rate': 1.864864864864865e-05, 'epoch': 3.14}
{'loss': 0.0399, 'learning_rate': 1.828828828828829e-05, 'epoch': 3.17}
{'loss': 0.0946, 'learning_rate': 1.792792792792793e-05, 'epoch': 3.21}
{'loss': 0.0341, 'learning_rate': 1.756756756756757e-05, 'epoch': 3.24}
{'loss': 0.0062, 'learning_rate': 1.7207207207207206e-05, 'epoch': 3.28}
{'loss': 0.0082, 'learning_rate': 1.684684684684685e-05, 'epoch': 3.32}
{'loss': 0.0382, 'learning_rate': 1.648648648648649e-05, 'epoch': 3.35}
{'loss': 0.0357, 'learning_rate': 1.6126126126126126e-05, 'epoch': 3.39}
{'loss': 0.0152, 'learning_rate': 1.5765765765765765e-05, 'epoch': 3.42}
{'loss': 0.0411, 'learning_rate': 1.540540540540541e-05, 'epoch': 3.46}
{'loss': 0.1023, 'learning_rate': 1.5045045045045045e-05, 'epoch': 3.5}
{'loss': 0.0352, 'learning_rate': 1.4684684684684685e-05, 'epoch': 3.53}
{'loss': 0.0145, 'learning_rate': 1.4324324324324326e-05, 'epoch': 3.57}
{'loss': 0.0259, 'learning_rate': 1.3963963963963963e-05, 'epoch': 3.6}
{'loss': 0.0292, 'learning_rate': 1.3603603603603604e-05, 'epoch': 3.64}
{'loss': 0.0013, 'learning_rate': 1.3243243243243244e-05, 'epoch': 3.68}
{'loss': 0.0067, 'learning_rate': 1.2882882882882882e-05, 'epoch': 3.71}
{'loss': 0.0246, 'learning_rate': 1.2522522522522522e-05, 'epoch': 3.75}
{'loss': 0.1021, 'learning_rate': 1.2162162162162164e-05, 'epoch': 3.78}
{'loss': 0.0126, 'learning_rate': 1.1801801801801803e-05, 'epoch': 3.82}
{'loss': 0.0289, 'learning_rate': 1.1441441441441442e-05, 'epoch': 3.86}
{'loss': 0.0093, 'learning_rate': 1.1081081081081083e-05, 'epoch': 3.89}
{'loss': 0.0372, 'learning_rate': 1.0720720720720721e-05, 'epoch': 3.93}
{'loss': 0.016, 'learning_rate': 1.0360360360360361e-05, 'epoch': 3.96}
{'loss': 0.0181, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0054, 'learning_rate': 9.63963963963964e-06, 'epoch': 4.04}
{'loss': 0.0024, 'learning_rate': 9.279279279279279e-06, 'epoch': 4.07}
{'loss': 0.0166, 'learning_rate': 8.91891891891892e-06, 'epoch': 4.11}
{'loss': 0.0274, 'learning_rate': 8.558558558558558e-06, 'epoch': 4.14}
{'loss': 0.0013, 'learning_rate': 8.198198198198198e-06, 'epoch': 4.18}
{'loss': 0.0075, 'learning_rate': 7.837837837837838e-06, 'epoch': 4.22}
{'loss': 0.0071, 'learning_rate': 7.477477477477478e-06, 'epoch': 4.25}
{'loss': 0.0176, 'learning_rate': 7.117117117117117e-06, 'epoch': 4.29}
{'loss': 0.0017, 'learning_rate': 6.7567567567567575e-06, 'epoch': 4.32}
{'loss': 0.0031, 'learning_rate': 6.3963963963963965e-06, 'epoch': 4.36}
{'loss': 0.0022, 'learning_rate': 6.036036036036036e-06, 'epoch': 4.4}
{'loss': 0.0038, 'learning_rate': 5.675675675675676e-06, 'epoch': 4.43}
{'loss': 0.0053, 'learning_rate': 5.315315315315315e-06, 'epoch': 4.47}
{'loss': 0.0178, 'learning_rate': 4.954954954954955e-06, 'epoch': 4.5}
{'loss': 0.0115, 'learning_rate': 4.594594594594595e-06, 'epoch': 4.54}
{'loss': 0.0132, 'learning_rate': 4.234234234234235e-06, 'epoch': 4.58}
{'loss': 0.0126, 'learning_rate': 3.8738738738738744e-06, 'epoch': 4.61}
{'loss': 0.0024, 'learning_rate': 3.513513513513514e-06, 'epoch': 4.65}
{'loss': 0.0035, 'learning_rate': 3.153153153153153e-06, 'epoch': 4.68}
{'loss': 0.0103, 'learning_rate': 2.792792792792793e-06, 'epoch': 4.72}
{'loss': 0.0066, 'learning_rate': 2.432432432432433e-06, 'epoch': 4.76}
{'loss': 0.0021, 'learning_rate': 2.0720720720720723e-06, 'epoch': 4.79}
{'loss': 0.013, 'learning_rate': 1.7117117117117117e-06, 'epoch': 4.83}
{'loss': 0.0054, 'learning_rate': 1.3513513513513515e-06, 'epoch': 4.86}
{'loss': 0.0019, 'learning_rate': 9.90990990990991e-07, 'epoch': 4.9}
{'loss': 0.0066, 'learning_rate': 6.306306306306306e-07, 'epoch': 4.94}
{'loss': 0.0134, 'learning_rate': 2.702702702702703e-07, 'epoch': 4.97}
{'train_runtime': 923.0804, 'train_samples_per_second': 4.794, 'train_steps_per_second': 0.601, 'train_loss': 0.1426109937875456, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-222/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-444/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-333/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-111/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-555/optimizer.pt
validate!
last validate 0.
INFO 11-24 07:03:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-111', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-111', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:03:19 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_30_50_0.8_0.35_120_5 epoch 1

------------------------------------------------

0.596

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_30_50_0.8_0.35_120_5/generated_contents/1
INFO 11-24 07:04:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-222', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-222', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:04:14 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_30_50_0.8_0.35_120_5 epoch 2

------------------------------------------------

0.561

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_30_50_0.8_0.35_120_5/generated_contents/2
INFO 11-24 07:04:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-333', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-333', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:05:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_30_50_0.8_0.35_120_5 epoch 3

------------------------------------------------

0.612

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_30_50_0.8_0.35_120_5/generated_contents/3
INFO 11-24 07:05:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-444', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-444', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:06:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_30_50_0.8_0.35_120_5 epoch 4

------------------------------------------------

0.601

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_30_50_0.8_0.35_120_5/generated_contents/4
INFO 11-24 07:06:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-555', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-555', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:07:02 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_30_50_0.8_0.35_120_5 epoch 5

------------------------------------------------

0.601

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_30_50_0.8_0.35_120_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-111
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-222
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-333
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-444
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_30_50_0.8_0.35_120_5/checkpoint-555
searching parameters: NI_task935_10_10_45_0.8_0.35_130_4
/home/cyzhao/NI_task935_exp_2/NI_task935_10_10_45_0.8_0.35_130_4
/home/cyzhao/NI_task935_exp_2/NI_task935_10_10_45_0.8_0.35_130_4/config.json
generate_and_write_inputs!
INFO 11-24 07:07:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:08:06 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 07:09:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:09:44 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 65
expected_example_num: 100
selection_ratio: 0.65
finetune_vicuna!
{'loss': 1.9132, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.44}
{'loss': 0.7417, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.89}
{'loss': 0.4365, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.33}
{'loss': 0.3662, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.78}
{'loss': 0.6053, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.22}
{'loss': 0.3231, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.67}
{'loss': 0.1985, 'learning_rate': 1.1111111111111112e-05, 'epoch': 3.11}
{'loss': 0.2294, 'learning_rate': 5.555555555555556e-06, 'epoch': 3.56}
{'loss': 0.1365, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 247.5885, 'train_samples_per_second': 1.05, 'train_steps_per_second': 0.145, 'train_loss': 0.5500556462340884, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-24 07:14:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:14:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_10_10_45_0.8_0.35_130_4 epoch 1

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_10_10_45_0.8_0.35_130_4/generated_contents/1
INFO 11-24 07:15:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:15:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_10_10_45_0.8_0.35_130_4 epoch 2

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_10_10_45_0.8_0.35_130_4/generated_contents/2
INFO 11-24 07:16:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:16:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_10_10_45_0.8_0.35_130_4 epoch 3

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_10_10_45_0.8_0.35_130_4/generated_contents/3
INFO 11-24 07:17:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:17:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_10_10_45_0.8_0.35_130_4 epoch 4

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_10_10_45_0.8_0.35_130_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-27
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_10_10_45_0.8_0.35_130_4/checkpoint-36
searching parameters: NI_task935_30_15_40_1.0_0.3_130_3
/home/cyzhao/NI_task935_exp_2/NI_task935_30_15_40_1.0_0.3_130_3
/home/cyzhao/NI_task935_exp_2/NI_task935_30_15_40_1.0_0.3_130_3/config.json
generate_and_write_inputs!
INFO 11-24 07:18:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:18:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 07:23:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:24:02 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 327
expected_example_num: 450
selection_ratio: 0.7266666666666667
finetune_vicuna!
{'loss': 2.069, 'learning_rate': 4.8373983739837406e-05, 'epoch': 0.1}
{'loss': 1.1916, 'learning_rate': 4.6747967479674795e-05, 'epoch': 0.2}
{'loss': 0.8604, 'learning_rate': 4.51219512195122e-05, 'epoch': 0.29}
{'loss': 0.3073, 'learning_rate': 4.3495934959349595e-05, 'epoch': 0.39}
{'loss': 0.718, 'learning_rate': 4.186991869918699e-05, 'epoch': 0.49}
{'loss': 0.3522, 'learning_rate': 4.0243902439024395e-05, 'epoch': 0.59}
{'loss': 0.2711, 'learning_rate': 3.861788617886179e-05, 'epoch': 0.68}
{'loss': 0.2133, 'learning_rate': 3.699186991869919e-05, 'epoch': 0.78}
{'loss': 0.2905, 'learning_rate': 3.5365853658536584e-05, 'epoch': 0.88}
{'loss': 0.3278, 'learning_rate': 3.373983739837399e-05, 'epoch': 0.98}
{'loss': 0.2646, 'learning_rate': 3.2113821138211384e-05, 'epoch': 1.07}
{'loss': 0.2029, 'learning_rate': 3.048780487804878e-05, 'epoch': 1.17}
{'loss': 0.3074, 'learning_rate': 2.886178861788618e-05, 'epoch': 1.27}
{'loss': 0.2125, 'learning_rate': 2.7235772357723577e-05, 'epoch': 1.37}
{'loss': 0.2124, 'learning_rate': 2.5609756097560977e-05, 'epoch': 1.46}
{'loss': 0.2118, 'learning_rate': 2.3983739837398377e-05, 'epoch': 1.56}
{'loss': 0.1696, 'learning_rate': 2.2357723577235773e-05, 'epoch': 1.66}
{'loss': 0.1644, 'learning_rate': 2.073170731707317e-05, 'epoch': 1.76}
{'loss': 0.1949, 'learning_rate': 1.9105691056910573e-05, 'epoch': 1.85}
{'loss': 0.0949, 'learning_rate': 1.747967479674797e-05, 'epoch': 1.95}
{'loss': 0.2347, 'learning_rate': 1.5853658536585366e-05, 'epoch': 2.05}
{'loss': 0.1291, 'learning_rate': 1.4227642276422764e-05, 'epoch': 2.15}
{'loss': 0.0963, 'learning_rate': 1.2601626016260162e-05, 'epoch': 2.24}
{'loss': 0.1001, 'learning_rate': 1.0975609756097562e-05, 'epoch': 2.34}
{'loss': 0.0973, 'learning_rate': 9.34959349593496e-06, 'epoch': 2.44}
{'loss': 0.1468, 'learning_rate': 7.723577235772358e-06, 'epoch': 2.54}
{'loss': 0.0729, 'learning_rate': 6.0975609756097564e-06, 'epoch': 2.63}
{'loss': 0.087, 'learning_rate': 4.471544715447155e-06, 'epoch': 2.73}
{'loss': 0.0951, 'learning_rate': 2.8455284552845528e-06, 'epoch': 2.83}
{'loss': 0.0779, 'learning_rate': 1.2195121951219514e-06, 'epoch': 2.93}
{'train_runtime': 295.417, 'train_samples_per_second': 3.321, 'train_steps_per_second': 0.416, 'train_loss': 0.319511117247062, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-41/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-82/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-123/optimizer.pt
validate!
last validate 0.
INFO 11-24 07:30:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-41', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-41', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:30:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_30_15_40_1.0_0.3_130_3 epoch 1

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_30_15_40_1.0_0.3_130_3/generated_contents/1
INFO 11-24 07:31:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-82', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-82', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:31:33 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_30_15_40_1.0_0.3_130_3 epoch 2

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_30_15_40_1.0_0.3_130_3/generated_contents/2
INFO 11-24 07:32:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-123', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-123', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:32:28 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_30_15_40_1.0_0.3_130_3 epoch 3

------------------------------------------------

0.506

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_30_15_40_1.0_0.3_130_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-41
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-82
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_15_40_1.0_0.3_130_3/checkpoint-123
searching parameters: NI_task935_20_15_50_0.8_0.35_115_3
/home/cyzhao/NI_task935_exp_2/NI_task935_20_15_50_0.8_0.35_115_3
/home/cyzhao/NI_task935_exp_2/NI_task935_20_15_50_0.8_0.35_115_3/config.json
generate_and_write_inputs!
INFO 11-24 07:33:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:33:27 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 07:37:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:38:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 201
expected_example_num: 300
selection_ratio: 0.67
finetune_vicuna!
{'loss': 1.7013, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.15}
{'loss': 0.4584, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.2699, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.46}
{'loss': 0.5856, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.5604, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.77}
{'loss': 0.2736, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.2913, 'learning_rate': 3.205128205128206e-05, 'epoch': 1.08}
{'loss': 0.1463, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.1253, 'learning_rate': 2.6923076923076923e-05, 'epoch': 1.38}
{'loss': 0.1174, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1634, 'learning_rate': 2.1794871794871795e-05, 'epoch': 1.69}
{'loss': 0.1581, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1554, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.116, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.1241, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.31}
{'loss': 0.1448, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.2319, 'learning_rate': 6.41025641025641e-06, 'epoch': 2.62}
{'loss': 0.1021, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'loss': 0.0751, 'learning_rate': 1.282051282051282e-06, 'epoch': 2.92}
{'train_runtime': 255.0387, 'train_samples_per_second': 2.364, 'train_steps_per_second': 0.306, 'train_loss': 0.2991970909329561, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-78/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-52/optimizer.pt
validate!
last validate 0.
INFO 11-24 07:43:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:44:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_15_50_0.8_0.35_115_3 epoch 1

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_15_50_0.8_0.35_115_3/generated_contents/1
INFO 11-24 07:44:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-52', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-52', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:45:00 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_15_50_0.8_0.35_115_3 epoch 2

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_15_50_0.8_0.35_115_3/generated_contents/2
INFO 11-24 07:45:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-78', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:45:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_15_50_0.8_0.35_115_3 epoch 3

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_15_50_0.8_0.35_115_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-52
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_15_50_0.8_0.35_115_3/checkpoint-78
searching parameters: NI_task935_50_40_50_0.9_0.3_115_3
/home/cyzhao/NI_task935_exp_2/NI_task935_50_40_50_0.9_0.3_115_3
/home/cyzhao/NI_task935_exp_2/NI_task935_50_40_50_0.9_0.3_115_3/config.json
generate_and_write_inputs!
INFO 11-24 07:46:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:46:58 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:05:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:05:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 1442
expected_example_num: 2000
selection_ratio: 0.721
finetune_vicuna!
{'loss': 2.1247, 'learning_rate': 4.96316758747698e-05, 'epoch': 0.02}
{'loss': 1.0541, 'learning_rate': 4.9263351749539597e-05, 'epoch': 0.04}
{'loss': 1.0083, 'learning_rate': 4.8895027624309394e-05, 'epoch': 0.07}
{'loss': 0.3962, 'learning_rate': 4.852670349907919e-05, 'epoch': 0.09}
{'loss': 0.4655, 'learning_rate': 4.815837937384899e-05, 'epoch': 0.11}
{'loss': 0.5014, 'learning_rate': 4.7790055248618785e-05, 'epoch': 0.13}
{'loss': 1.2339, 'learning_rate': 4.742173112338858e-05, 'epoch': 0.15}
{'loss': 2.3635, 'learning_rate': 4.7053406998158386e-05, 'epoch': 0.18}
{'loss': 0.8351, 'learning_rate': 4.6685082872928176e-05, 'epoch': 0.2}
{'loss': 0.3377, 'learning_rate': 4.6316758747697973e-05, 'epoch': 0.22}
{'loss': 0.294, 'learning_rate': 4.594843462246777e-05, 'epoch': 0.24}
{'loss': 0.2209, 'learning_rate': 4.5580110497237574e-05, 'epoch': 0.27}
{'loss': 0.3294, 'learning_rate': 4.521178637200737e-05, 'epoch': 0.29}
{'loss': 0.1578, 'learning_rate': 4.484346224677717e-05, 'epoch': 0.31}
{'loss': 0.4055, 'learning_rate': 4.447513812154696e-05, 'epoch': 0.33}
{'loss': 0.2533, 'learning_rate': 4.4106813996316756e-05, 'epoch': 0.35}
{'loss': 0.2769, 'learning_rate': 4.373848987108656e-05, 'epoch': 0.38}
{'loss': 0.4804, 'learning_rate': 4.337016574585636e-05, 'epoch': 0.4}
{'loss': 0.3099, 'learning_rate': 4.3001841620626154e-05, 'epoch': 0.42}
{'loss': 0.2618, 'learning_rate': 4.263351749539595e-05, 'epoch': 0.44}
{'loss': 0.4035, 'learning_rate': 4.226519337016575e-05, 'epoch': 0.46}
{'loss': 0.246, 'learning_rate': 4.1896869244935545e-05, 'epoch': 0.49}
{'loss': 0.1509, 'learning_rate': 4.152854511970534e-05, 'epoch': 0.51}
{'loss': 0.2424, 'learning_rate': 4.116022099447514e-05, 'epoch': 0.53}
{'loss': 0.3055, 'learning_rate': 4.079189686924494e-05, 'epoch': 0.55}
{'loss': 0.1515, 'learning_rate': 4.0423572744014734e-05, 'epoch': 0.57}
{'loss': 0.1525, 'learning_rate': 4.005524861878453e-05, 'epoch': 0.6}
{'loss': 0.2031, 'learning_rate': 3.968692449355433e-05, 'epoch': 0.62}
{'loss': 0.2241, 'learning_rate': 3.9318600368324125e-05, 'epoch': 0.64}
{'loss': 0.1342, 'learning_rate': 3.895027624309392e-05, 'epoch': 0.66}
{'loss': 0.3267, 'learning_rate': 3.8581952117863726e-05, 'epoch': 0.69}
{'loss': 0.2174, 'learning_rate': 3.821362799263352e-05, 'epoch': 0.71}
{'loss': 0.187, 'learning_rate': 3.7845303867403314e-05, 'epoch': 0.73}
{'loss': 0.2073, 'learning_rate': 3.747697974217311e-05, 'epoch': 0.75}
{'loss': 0.2676, 'learning_rate': 3.710865561694291e-05, 'epoch': 0.77}
{'loss': 0.2304, 'learning_rate': 3.674033149171271e-05, 'epoch': 0.8}
{'loss': 0.1651, 'learning_rate': 3.637200736648251e-05, 'epoch': 0.82}
{'loss': 0.1866, 'learning_rate': 3.6003683241252306e-05, 'epoch': 0.84}
{'loss': 0.12, 'learning_rate': 3.5635359116022096e-05, 'epoch': 0.86}
{'loss': 0.1537, 'learning_rate': 3.52670349907919e-05, 'epoch': 0.88}
{'loss': 0.1493, 'learning_rate': 3.48987108655617e-05, 'epoch': 0.91}
{'loss': 0.2045, 'learning_rate': 3.4530386740331494e-05, 'epoch': 0.93}
{'loss': 0.1496, 'learning_rate': 3.416206261510129e-05, 'epoch': 0.95}
{'loss': 0.361, 'learning_rate': 3.379373848987109e-05, 'epoch': 0.97}
{'loss': 0.0914, 'learning_rate': 3.3425414364640886e-05, 'epoch': 0.99}
{'loss': 0.15, 'learning_rate': 3.305709023941068e-05, 'epoch': 1.02}
{'loss': 0.2279, 'learning_rate': 3.268876611418048e-05, 'epoch': 1.04}
{'loss': 0.0892, 'learning_rate': 3.232044198895028e-05, 'epoch': 1.06}
{'loss': 0.094, 'learning_rate': 3.1952117863720074e-05, 'epoch': 1.08}
{'loss': 0.0949, 'learning_rate': 3.158379373848988e-05, 'epoch': 1.1}
{'loss': 0.1645, 'learning_rate': 3.121546961325967e-05, 'epoch': 1.13}
{'loss': 0.1677, 'learning_rate': 3.0847145488029465e-05, 'epoch': 1.15}
{'loss': 0.0934, 'learning_rate': 3.0478821362799266e-05, 'epoch': 1.17}
{'loss': 0.1125, 'learning_rate': 3.0110497237569063e-05, 'epoch': 1.19}
{'loss': 0.1569, 'learning_rate': 2.974217311233886e-05, 'epoch': 1.22}
{'loss': 0.0779, 'learning_rate': 2.937384898710866e-05, 'epoch': 1.24}
{'loss': 0.1346, 'learning_rate': 2.900552486187845e-05, 'epoch': 1.26}
{'loss': 0.2237, 'learning_rate': 2.863720073664825e-05, 'epoch': 1.28}
{'loss': 0.1284, 'learning_rate': 2.826887661141805e-05, 'epoch': 1.3}
{'loss': 0.0744, 'learning_rate': 2.790055248618785e-05, 'epoch': 1.33}
{'loss': 0.1323, 'learning_rate': 2.7532228360957646e-05, 'epoch': 1.35}
{'loss': 0.1002, 'learning_rate': 2.716390423572744e-05, 'epoch': 1.37}
{'loss': 0.0604, 'learning_rate': 2.6795580110497237e-05, 'epoch': 1.39}
{'loss': 0.1694, 'learning_rate': 2.6427255985267034e-05, 'epoch': 1.41}
{'loss': 0.2172, 'learning_rate': 2.6058931860036835e-05, 'epoch': 1.44}
{'loss': 0.1173, 'learning_rate': 2.569060773480663e-05, 'epoch': 1.46}
{'loss': 0.1379, 'learning_rate': 2.532228360957643e-05, 'epoch': 1.48}
{'loss': 0.0708, 'learning_rate': 2.4953959484346226e-05, 'epoch': 1.5}
{'loss': 0.1711, 'learning_rate': 2.4585635359116023e-05, 'epoch': 1.52}
{'loss': 0.133, 'learning_rate': 2.421731123388582e-05, 'epoch': 1.55}
{'loss': 0.1087, 'learning_rate': 2.3848987108655617e-05, 'epoch': 1.57}
{'loss': 0.1262, 'learning_rate': 2.3480662983425418e-05, 'epoch': 1.59}
{'loss': 0.1967, 'learning_rate': 2.311233885819521e-05, 'epoch': 1.61}
{'loss': 0.083, 'learning_rate': 2.2744014732965012e-05, 'epoch': 1.64}
{'loss': 0.0936, 'learning_rate': 2.237569060773481e-05, 'epoch': 1.66}
{'loss': 0.0917, 'learning_rate': 2.2007366482504606e-05, 'epoch': 1.68}
{'loss': 0.1235, 'learning_rate': 2.1639042357274403e-05, 'epoch': 1.7}
{'loss': 0.0683, 'learning_rate': 2.12707182320442e-05, 'epoch': 1.72}
{'loss': 0.122, 'learning_rate': 2.0902394106813997e-05, 'epoch': 1.75}
{'loss': 0.0827, 'learning_rate': 2.0534069981583794e-05, 'epoch': 1.77}
{'loss': 0.2137, 'learning_rate': 2.0165745856353595e-05, 'epoch': 1.79}
{'loss': 0.0711, 'learning_rate': 1.979742173112339e-05, 'epoch': 1.81}
{'loss': 0.1688, 'learning_rate': 1.9429097605893186e-05, 'epoch': 1.83}
{'loss': 0.0676, 'learning_rate': 1.9060773480662986e-05, 'epoch': 1.86}
{'loss': 0.0884, 'learning_rate': 1.869244935543278e-05, 'epoch': 1.88}
{'loss': 0.0624, 'learning_rate': 1.832412523020258e-05, 'epoch': 1.9}
{'loss': 0.0577, 'learning_rate': 1.7955801104972378e-05, 'epoch': 1.92}
{'loss': 0.1138, 'learning_rate': 1.7587476979742175e-05, 'epoch': 1.94}
{'loss': 0.1248, 'learning_rate': 1.7219152854511972e-05, 'epoch': 1.97}
{'loss': 0.0889, 'learning_rate': 1.685082872928177e-05, 'epoch': 1.99}
{'loss': 0.1914, 'learning_rate': 1.6482504604051566e-05, 'epoch': 2.01}
{'loss': 0.0676, 'learning_rate': 1.6114180478821363e-05, 'epoch': 2.03}
{'loss': 0.0313, 'learning_rate': 1.574585635359116e-05, 'epoch': 2.06}
{'loss': 0.084, 'learning_rate': 1.5377532228360957e-05, 'epoch': 2.08}
{'loss': 0.0341, 'learning_rate': 1.5009208103130756e-05, 'epoch': 2.1}
{'loss': 0.056, 'learning_rate': 1.4640883977900552e-05, 'epoch': 2.12}
{'loss': 0.0901, 'learning_rate': 1.427255985267035e-05, 'epoch': 2.14}
{'loss': 0.0517, 'learning_rate': 1.3904235727440149e-05, 'epoch': 2.17}
{'loss': 0.0161, 'learning_rate': 1.3535911602209945e-05, 'epoch': 2.19}
{'loss': 0.0858, 'learning_rate': 1.3167587476979742e-05, 'epoch': 2.21}
{'loss': 0.1037, 'learning_rate': 1.279926335174954e-05, 'epoch': 2.23}
{'loss': 0.0174, 'learning_rate': 1.2430939226519338e-05, 'epoch': 2.25}
{'loss': 0.0355, 'learning_rate': 1.2062615101289135e-05, 'epoch': 2.28}
{'loss': 0.1087, 'learning_rate': 1.1694290976058933e-05, 'epoch': 2.3}
{'loss': 0.0486, 'learning_rate': 1.132596685082873e-05, 'epoch': 2.32}
{'loss': 0.0462, 'learning_rate': 1.0957642725598528e-05, 'epoch': 2.34}
{'loss': 0.032, 'learning_rate': 1.0589318600368325e-05, 'epoch': 2.36}
{'loss': 0.0334, 'learning_rate': 1.0220994475138122e-05, 'epoch': 2.39}
{'loss': 0.0309, 'learning_rate': 9.852670349907919e-06, 'epoch': 2.41}
{'loss': 0.0368, 'learning_rate': 9.484346224677716e-06, 'epoch': 2.43}
{'loss': 0.0463, 'learning_rate': 9.116022099447515e-06, 'epoch': 2.45}
{'loss': 0.0223, 'learning_rate': 8.747697974217312e-06, 'epoch': 2.48}
{'loss': 0.0615, 'learning_rate': 8.379373848987109e-06, 'epoch': 2.5}
{'loss': 0.0477, 'learning_rate': 8.011049723756906e-06, 'epoch': 2.52}
{'loss': 0.0154, 'learning_rate': 7.642725598526703e-06, 'epoch': 2.54}
{'loss': 0.0689, 'learning_rate': 7.2744014732965e-06, 'epoch': 2.56}
{'loss': 0.0309, 'learning_rate': 6.906077348066299e-06, 'epoch': 2.59}
{'loss': 0.0997, 'learning_rate': 6.537753222836096e-06, 'epoch': 2.61}
{'loss': 0.0429, 'learning_rate': 6.169429097605893e-06, 'epoch': 2.63}
{'loss': 0.0352, 'learning_rate': 5.8011049723756905e-06, 'epoch': 2.65}
{'loss': 0.0152, 'learning_rate': 5.4327808471454885e-06, 'epoch': 2.67}
{'loss': 0.043, 'learning_rate': 5.064456721915286e-06, 'epoch': 2.7}
{'loss': 0.0222, 'learning_rate': 4.696132596685083e-06, 'epoch': 2.72}
{'loss': 0.0205, 'learning_rate': 4.327808471454881e-06, 'epoch': 2.74}
{'loss': 0.0763, 'learning_rate': 3.959484346224679e-06, 'epoch': 2.76}
{'loss': 0.0446, 'learning_rate': 3.5911602209944753e-06, 'epoch': 2.78}
{'loss': 0.0613, 'learning_rate': 3.222836095764273e-06, 'epoch': 2.81}
{'loss': 0.0317, 'learning_rate': 2.8545119705340703e-06, 'epoch': 2.83}
{'loss': 0.0341, 'learning_rate': 2.4861878453038674e-06, 'epoch': 2.85}
{'loss': 0.0756, 'learning_rate': 2.117863720073665e-06, 'epoch': 2.87}
{'loss': 0.0325, 'learning_rate': 1.7495395948434623e-06, 'epoch': 2.9}
{'loss': 0.042, 'learning_rate': 1.3812154696132596e-06, 'epoch': 2.92}
{'loss': 0.0462, 'learning_rate': 1.0128913443830571e-06, 'epoch': 2.94}
{'loss': 0.0352, 'learning_rate': 6.445672191528545e-07, 'epoch': 2.96}
{'loss': 0.05, 'learning_rate': 2.7624309392265196e-07, 'epoch': 2.98}
{'train_runtime': 878.0812, 'train_samples_per_second': 4.927, 'train_steps_per_second': 0.618, 'train_loss': 0.1940329878575565, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-362/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-181/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-543/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:25:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-181', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-181', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:26:09 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_50_40_50_0.9_0.3_115_3 epoch 1

------------------------------------------------

0.562

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_50_40_50_0.9_0.3_115_3/generated_contents/1
INFO 11-24 08:26:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-362', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-362', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:27:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_50_40_50_0.9_0.3_115_3 epoch 2

------------------------------------------------

0.558

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_50_40_50_0.9_0.3_115_3/generated_contents/2
INFO 11-24 08:27:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-543', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-543', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:28:02 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_50_40_50_0.9_0.3_115_3 epoch 3

------------------------------------------------

0.592

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_50_40_50_0.9_0.3_115_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-181
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-362
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_40_50_0.9_0.3_115_3/checkpoint-543
searching parameters: NI_task935_40_10_50_1.0_0.3_120_4
/home/cyzhao/NI_task935_exp_2/NI_task935_40_10_50_1.0_0.3_120_4
/home/cyzhao/NI_task935_exp_2/NI_task935_40_10_50_1.0_0.3_120_4/config.json
generate_and_write_inputs!
INFO 11-24 08:28:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:29:05 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:38:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:38:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 279
expected_example_num: 400
selection_ratio: 0.6975
finetune_vicuna!
{'loss': 0.5504, 'learning_rate': 4.8571428571428576e-05, 'epoch': 0.11}
{'loss': 0.4035, 'learning_rate': 4.714285714285714e-05, 'epoch': 0.23}
{'loss': 0.4437, 'learning_rate': 4.5714285714285716e-05, 'epoch': 0.34}
{'loss': 0.3028, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.46}
{'loss': 0.2513, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.57}
{'loss': 0.4608, 'learning_rate': 4.1428571428571437e-05, 'epoch': 0.69}
{'loss': 0.152, 'learning_rate': 4e-05, 'epoch': 0.8}
{'loss': 0.1964, 'learning_rate': 3.857142857142858e-05, 'epoch': 0.91}
{'loss': 0.2988, 'learning_rate': 3.7142857142857143e-05, 'epoch': 1.03}
{'loss': 0.1445, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.14}
{'loss': 0.1017, 'learning_rate': 3.428571428571429e-05, 'epoch': 1.26}
{'loss': 0.1655, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.37}
{'loss': 0.1207, 'learning_rate': 3.142857142857143e-05, 'epoch': 1.49}
{'loss': 0.1717, 'learning_rate': 3e-05, 'epoch': 1.6}
{'loss': 0.0946, 'learning_rate': 2.857142857142857e-05, 'epoch': 1.71}
{'loss': 0.1176, 'learning_rate': 2.714285714285714e-05, 'epoch': 1.83}
{'loss': 0.1002, 'learning_rate': 2.5714285714285714e-05, 'epoch': 1.94}
{'loss': 0.1055, 'learning_rate': 2.4285714285714288e-05, 'epoch': 2.06}
{'loss': 0.0656, 'learning_rate': 2.2857142857142858e-05, 'epoch': 2.17}
{'loss': 0.0674, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.29}
{'loss': 0.0705, 'learning_rate': 2e-05, 'epoch': 2.4}
{'loss': 0.1062, 'learning_rate': 1.8571428571428572e-05, 'epoch': 2.51}
{'loss': 0.06, 'learning_rate': 1.7142857142857145e-05, 'epoch': 2.63}
{'loss': 0.0686, 'learning_rate': 1.5714285714285715e-05, 'epoch': 2.74}
{'loss': 0.1149, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.86}
{'loss': 0.0856, 'learning_rate': 1.2857142857142857e-05, 'epoch': 2.97}
{'loss': 0.0776, 'learning_rate': 1.1428571428571429e-05, 'epoch': 3.09}
{'loss': 0.0489, 'learning_rate': 1e-05, 'epoch': 3.2}
{'loss': 0.0373, 'learning_rate': 8.571428571428573e-06, 'epoch': 3.31}
{'loss': 0.0684, 'learning_rate': 7.142857142857143e-06, 'epoch': 3.43}
{'loss': 0.0327, 'learning_rate': 5.7142857142857145e-06, 'epoch': 3.54}
{'loss': 0.0326, 'learning_rate': 4.285714285714286e-06, 'epoch': 3.66}
{'loss': 0.0422, 'learning_rate': 2.8571428571428573e-06, 'epoch': 3.77}
{'loss': 0.0178, 'learning_rate': 1.4285714285714286e-06, 'epoch': 3.89}
{'loss': 0.0397, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 464.2914, 'train_samples_per_second': 2.404, 'train_steps_per_second': 0.302, 'train_loss': 0.14907805137336255, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-35/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-105/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-140/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-70/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:49:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-35', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-35', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:49:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_10_50_1.0_0.3_120_4 epoch 1

------------------------------------------------

0.493

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_10_50_1.0_0.3_120_4/generated_contents/1
INFO 11-24 08:50:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-70', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-70', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:50:55 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_10_50_1.0_0.3_120_4 epoch 2

------------------------------------------------

0.529

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_10_50_1.0_0.3_120_4/generated_contents/2
INFO 11-24 08:51:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-105', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-105', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:51:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_10_50_1.0_0.3_120_4 epoch 3

------------------------------------------------

0.617

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_10_50_1.0_0.3_120_4/generated_contents/3
INFO 11-24 08:52:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-140', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-140', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:52:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_40_10_50_1.0_0.3_120_4 epoch 4

------------------------------------------------

0.624

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_40_10_50_1.0_0.3_120_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-35
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-70
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-105
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_40_10_50_1.0_0.3_120_4/checkpoint-140
searching parameters: NI_task935_50_20_50_0.9_0.3_125_3
/home/cyzhao/NI_task935_exp_2/NI_task935_50_20_50_0.9_0.3_125_3
/home/cyzhao/NI_task935_exp_2/NI_task935_50_20_50_0.9_0.3_125_3/config.json
generate_and_write_inputs!
INFO 11-24 08:53:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:53:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 09:08:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:08:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 719
expected_example_num: 1000
selection_ratio: 0.719
finetune_vicuna!
{'loss': 3.171, 'learning_rate': 4.925925925925926e-05, 'epoch': 0.04}
{'loss': 0.7363, 'learning_rate': 4.851851851851852e-05, 'epoch': 0.09}
{'loss': 0.3757, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.13}
{'loss': 0.8139, 'learning_rate': 4.703703703703704e-05, 'epoch': 0.18}
{'loss': 0.4044, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.22}
{'loss': 0.2505, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.4377, 'learning_rate': 4.481481481481482e-05, 'epoch': 0.31}
{'loss': 0.4257, 'learning_rate': 4.4074074074074076e-05, 'epoch': 0.36}
{'loss': 0.3515, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.1885, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.1731, 'learning_rate': 4.185185185185185e-05, 'epoch': 0.49}
{'loss': 0.2598, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.2195, 'learning_rate': 4.0370370370370374e-05, 'epoch': 0.58}
{'loss': 0.187, 'learning_rate': 3.962962962962963e-05, 'epoch': 0.62}
{'loss': 0.4708, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1288, 'learning_rate': 3.814814814814815e-05, 'epoch': 0.71}
{'loss': 0.203, 'learning_rate': 3.740740740740741e-05, 'epoch': 0.76}
{'loss': 0.1933, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.1299, 'learning_rate': 3.592592592592593e-05, 'epoch': 0.84}
{'loss': 0.276, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.2743, 'learning_rate': 3.444444444444445e-05, 'epoch': 0.93}
{'loss': 0.4596, 'learning_rate': 3.3703703703703706e-05, 'epoch': 0.98}
{'loss': 0.1955, 'learning_rate': 3.2962962962962964e-05, 'epoch': 1.02}
{'loss': 0.1023, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.1024, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}
{'loss': 0.1345, 'learning_rate': 3.074074074074074e-05, 'epoch': 1.16}
{'loss': 0.1229, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.0736, 'learning_rate': 2.925925925925926e-05, 'epoch': 1.24}
{'loss': 0.092, 'learning_rate': 2.851851851851852e-05, 'epoch': 1.29}
{'loss': 0.1055, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1866, 'learning_rate': 2.7037037037037037e-05, 'epoch': 1.38}
{'loss': 0.0955, 'learning_rate': 2.6296296296296296e-05, 'epoch': 1.42}
{'loss': 0.0987, 'learning_rate': 2.5555555555555554e-05, 'epoch': 1.47}
{'loss': 0.209, 'learning_rate': 2.4814814814814816e-05, 'epoch': 1.51}
{'loss': 0.1555, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.56}
{'loss': 0.2761, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1203, 'learning_rate': 2.2592592592592594e-05, 'epoch': 1.64}
{'loss': 0.0996, 'learning_rate': 2.1851851851851852e-05, 'epoch': 1.69}
{'loss': 0.1167, 'learning_rate': 2.111111111111111e-05, 'epoch': 1.73}
{'loss': 0.0985, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.0416, 'learning_rate': 1.962962962962963e-05, 'epoch': 1.82}
{'loss': 0.124, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.1301, 'learning_rate': 1.814814814814815e-05, 'epoch': 1.91}
{'loss': 0.1194, 'learning_rate': 1.740740740740741e-05, 'epoch': 1.96}
{'loss': 0.0616, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0328, 'learning_rate': 1.5925925925925926e-05, 'epoch': 2.04}
{'loss': 0.0391, 'learning_rate': 1.5185185185185186e-05, 'epoch': 2.09}
{'loss': 0.0552, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.095, 'learning_rate': 1.3703703703703704e-05, 'epoch': 2.18}
{'loss': 0.0302, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0472, 'learning_rate': 1.2222222222222222e-05, 'epoch': 2.27}
{'loss': 0.0205, 'learning_rate': 1.1481481481481482e-05, 'epoch': 2.31}
{'loss': 0.0796, 'learning_rate': 1.074074074074074e-05, 'epoch': 2.36}
{'loss': 0.0419, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.041, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.44}
{'loss': 0.0783, 'learning_rate': 8.518518518518519e-06, 'epoch': 2.49}
{'loss': 0.0283, 'learning_rate': 7.777777777777777e-06, 'epoch': 2.53}
{'loss': 0.0524, 'learning_rate': 7.0370370370370375e-06, 'epoch': 2.58}
{'loss': 0.0499, 'learning_rate': 6.296296296296296e-06, 'epoch': 2.62}
{'loss': 0.0152, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0676, 'learning_rate': 4.814814814814815e-06, 'epoch': 2.71}
{'loss': 0.0199, 'learning_rate': 4.074074074074075e-06, 'epoch': 2.76}
{'loss': 0.0292, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0197, 'learning_rate': 2.5925925925925925e-06, 'epoch': 2.84}
{'loss': 0.0465, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.89}
{'loss': 0.0258, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'loss': 0.0477, 'learning_rate': 3.703703703703704e-07, 'epoch': 2.98}
{'train_runtime': 509.0155, 'train_samples_per_second': 4.238, 'train_steps_per_second': 0.53, 'train_loss': 0.20700542115502887, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-270/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-180/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-90/optimizer.pt
validate!
last validate 0.
INFO 11-24 09:20:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-90', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:20:55 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_50_20_50_0.9_0.3_125_3 epoch 1

------------------------------------------------

0.605

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_50_20_50_0.9_0.3_125_3/generated_contents/1
INFO 11-24 09:21:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-180', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-180', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:21:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_50_20_50_0.9_0.3_125_3 epoch 2

------------------------------------------------

0.659

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_50_20_50_0.9_0.3_125_3/generated_contents/2
INFO 11-24 09:22:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-270', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-270', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:22:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_50_20_50_0.9_0.3_125_3 epoch 3

------------------------------------------------

0.605

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_50_20_50_0.9_0.3_125_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task935_exp_2
mv /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-180 /data2/cyzhao/best_ckpt/NI_task935_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-90
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_50_20_50_0.9_0.3_125_3/checkpoint-270
searching parameters: NI_task935_30_20_40_0.8_0.35_125_3
/home/cyzhao/NI_task935_exp_2/NI_task935_30_20_40_0.8_0.35_125_3
/home/cyzhao/NI_task935_exp_2/NI_task935_30_20_40_0.8_0.35_125_3/config.json
generate_and_write_inputs!
INFO 11-24 09:23:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:23:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 09:29:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:29:40 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 450
expected_example_num: 600
selection_ratio: 0.75
finetune_vicuna!
{'loss': 1.6932, 'learning_rate': 4.883040935672515e-05, 'epoch': 0.07}
{'loss': 0.9026, 'learning_rate': 4.7660818713450294e-05, 'epoch': 0.14}
{'loss': 0.2939, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.3132, 'learning_rate': 4.5321637426900585e-05, 'epoch': 0.28}
{'loss': 0.3619, 'learning_rate': 4.4152046783625734e-05, 'epoch': 0.35}
{'loss': 0.252, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.2784, 'learning_rate': 4.1812865497076025e-05, 'epoch': 0.49}
{'loss': 0.2371, 'learning_rate': 4.0643274853801174e-05, 'epoch': 0.56}
{'loss': 0.2216, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.2609, 'learning_rate': 3.8304093567251465e-05, 'epoch': 0.7}
{'loss': 0.1575, 'learning_rate': 3.713450292397661e-05, 'epoch': 0.77}
{'loss': 0.1779, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.2691, 'learning_rate': 3.4795321637426905e-05, 'epoch': 0.91}
{'loss': 0.1357, 'learning_rate': 3.362573099415205e-05, 'epoch': 0.98}
{'loss': 0.1968, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.2518, 'learning_rate': 3.128654970760234e-05, 'epoch': 1.12}
{'loss': 0.1026, 'learning_rate': 3.0116959064327488e-05, 'epoch': 1.19}
{'loss': 0.2417, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.1316, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1326, 'learning_rate': 2.6608187134502928e-05, 'epoch': 1.4}
{'loss': 0.1554, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.0866, 'learning_rate': 2.4269005847953216e-05, 'epoch': 1.54}
{'loss': 0.2039, 'learning_rate': 2.309941520467836e-05, 'epoch': 1.61}
{'loss': 0.1187, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.0873, 'learning_rate': 2.0760233918128656e-05, 'epoch': 1.75}
{'loss': 0.0806, 'learning_rate': 1.9590643274853802e-05, 'epoch': 1.82}
{'loss': 0.0835, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.2472, 'learning_rate': 1.7251461988304093e-05, 'epoch': 1.96}
{'loss': 0.03, 'learning_rate': 1.608187134502924e-05, 'epoch': 2.04}
{'loss': 0.0693, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0716, 'learning_rate': 1.3742690058479531e-05, 'epoch': 2.18}
{'loss': 0.0829, 'learning_rate': 1.2573099415204679e-05, 'epoch': 2.25}
{'loss': 0.1088, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.0207, 'learning_rate': 1.023391812865497e-05, 'epoch': 2.39}
{'loss': 0.0737, 'learning_rate': 9.064327485380117e-06, 'epoch': 2.46}
{'loss': 0.0155, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0743, 'learning_rate': 6.725146198830409e-06, 'epoch': 2.6}
{'loss': 0.0333, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0463, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0744, 'learning_rate': 3.216374269005848e-06, 'epoch': 2.81}
{'loss': 0.08, 'learning_rate': 2.0467836257309943e-06, 'epoch': 2.88}
{'loss': 0.0591, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 330.3959, 'train_samples_per_second': 4.086, 'train_steps_per_second': 0.518, 'train_loss': 0.19958430715994527, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-114/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-171/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-24 09:37:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:37:20 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_30_20_40_0.8_0.35_125_3 epoch 1

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_30_20_40_0.8_0.35_125_3/generated_contents/1
INFO 11-24 09:37:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-114', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-114', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:38:13 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_30_20_40_0.8_0.35_125_3 epoch 2

------------------------------------------------

0.64

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_30_20_40_0.8_0.35_125_3/generated_contents/2
INFO 11-24 09:38:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-171', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-171', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:39:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_30_20_40_0.8_0.35_125_3 epoch 3

------------------------------------------------

0.61

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_30_20_40_0.8_0.35_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-57
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-114
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_30_20_40_0.8_0.35_125_3/checkpoint-171
searching parameters: NI_task935_20_30_45_0.9_0.4_120_4
/home/cyzhao/NI_task935_exp_2/NI_task935_20_30_45_0.9_0.4_120_4
/home/cyzhao/NI_task935_exp_2/NI_task935_20_30_45_0.9_0.4_120_4/config.json
generate_and_write_inputs!
INFO 11-24 09:39:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:40:09 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 09:46:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:47:06 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 396
expected_example_num: 600
selection_ratio: 0.66
finetune_vicuna!
{'loss': 1.6274, 'learning_rate': 4.9e-05, 'epoch': 0.08}
{'loss': 1.605, 'learning_rate': 4.8e-05, 'epoch': 0.16}
{'loss': 0.4434, 'learning_rate': 4.7e-05, 'epoch': 0.24}
{'loss': 0.5492, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.32}
{'loss': 0.2766, 'learning_rate': 4.5e-05, 'epoch': 0.4}
{'loss': 0.3245, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.48}
{'loss': 0.185, 'learning_rate': 4.3e-05, 'epoch': 0.56}
{'loss': 0.4628, 'learning_rate': 4.2e-05, 'epoch': 0.64}
{'loss': 0.4822, 'learning_rate': 4.1e-05, 'epoch': 0.72}
{'loss': 0.3597, 'learning_rate': 4e-05, 'epoch': 0.8}
{'loss': 0.1737, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.88}
{'loss': 0.1352, 'learning_rate': 3.8e-05, 'epoch': 0.96}
{'loss': 0.1739, 'learning_rate': 3.7e-05, 'epoch': 1.04}
{'loss': 0.1849, 'learning_rate': 3.6e-05, 'epoch': 1.12}
{'loss': 0.1129, 'learning_rate': 3.5e-05, 'epoch': 1.2}
{'loss': 0.1685, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.28}
{'loss': 0.1356, 'learning_rate': 3.3e-05, 'epoch': 1.36}
{'loss': 0.1488, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.44}
{'loss': 0.1181, 'learning_rate': 3.1e-05, 'epoch': 1.52}
{'loss': 0.0944, 'learning_rate': 3e-05, 'epoch': 1.6}
{'loss': 0.1231, 'learning_rate': 2.9e-05, 'epoch': 1.68}
{'loss': 0.1969, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.76}
{'loss': 0.337, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.84}
{'loss': 0.2307, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.92}
{'loss': 0.2453, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.0773, 'learning_rate': 2.4e-05, 'epoch': 2.08}
{'loss': 0.0733, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.16}
{'loss': 0.0964, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.24}
{'loss': 0.0702, 'learning_rate': 2.1e-05, 'epoch': 2.32}
{'loss': 0.0762, 'learning_rate': 2e-05, 'epoch': 2.4}
{'loss': 0.043, 'learning_rate': 1.9e-05, 'epoch': 2.48}
{'loss': 0.0679, 'learning_rate': 1.8e-05, 'epoch': 2.56}
{'loss': 0.1122, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.64}
{'loss': 0.0223, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.72}
{'loss': 0.0535, 'learning_rate': 1.5e-05, 'epoch': 2.8}
{'loss': 0.0357, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.88}
{'loss': 0.0537, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.96}
{'loss': 0.0626, 'learning_rate': 1.2e-05, 'epoch': 3.04}
{'loss': 0.0317, 'learning_rate': 1.1000000000000001e-05, 'epoch': 3.12}
{'loss': 0.0109, 'learning_rate': 1e-05, 'epoch': 3.2}
{'loss': 0.0262, 'learning_rate': 9e-06, 'epoch': 3.28}
{'loss': 0.0246, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.36}
{'loss': 0.0145, 'learning_rate': 7.000000000000001e-06, 'epoch': 3.44}
{'loss': 0.0179, 'learning_rate': 6e-06, 'epoch': 3.52}
{'loss': 0.0083, 'learning_rate': 5e-06, 'epoch': 3.6}
{'loss': 0.0167, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.68}
{'loss': 0.0197, 'learning_rate': 3e-06, 'epoch': 3.76}
{'loss': 0.0367, 'learning_rate': 2.0000000000000003e-06, 'epoch': 3.84}
{'loss': 0.0251, 'learning_rate': 1.0000000000000002e-06, 'epoch': 3.92}
{'loss': 0.0289, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 482.9679, 'train_samples_per_second': 3.28, 'train_steps_per_second': 0.414, 'train_loss': 0.20000600714236497, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-100/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-150/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-200/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-50/optimizer.pt
validate!
last validate 0.
INFO 11-24 09:57:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-50', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-50', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:57:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_30_45_0.9_0.4_120_4 epoch 1

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_30_45_0.9_0.4_120_4/generated_contents/1
INFO 11-24 09:58:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-100', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-100', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:59:01 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_30_45_0.9_0.4_120_4 epoch 2

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_30_45_0.9_0.4_120_4/generated_contents/2
INFO 11-24 09:59:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-150', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-150', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:00:04 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_30_45_0.9_0.4_120_4 epoch 3

------------------------------------------------

0.585

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_30_45_0.9_0.4_120_4/generated_contents/3
INFO 11-24 10:00:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-200', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-200', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:01:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task935_20_30_45_0.9_0.4_120_4 epoch 4

------------------------------------------------

0.588

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/NI_task935_20_30_45_0.9_0.4_120_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-50
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-100
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-150
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task935_20_30_45_0.9_0.4_120_4/checkpoint-200
{'generation_epochs': 50, 'generation_batch_size': 20, 'generation_top_k': 50, 'generation_temperature': 0.9, 'min_frequency': 0.3, 'min_input_length': 125, 'training_epochs': 3}
test best ckpt.
INFO 11-24 10:02:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task935_exp_2', tokenizer='/data2/cyzhao/best_ckpt/NI_task935_exp_2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:02:23 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task935_exp_2

------------------------------------------------

0.633

------------------------------------------------


The best ckpt on test set gain 0.633
Genrated contents are stored in /home/cyzhao/NI_task935_exp_2/best_ckpt_generated_content
