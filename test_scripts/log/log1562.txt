[2023-11-29 05:44:12,145] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1562
searching parameters: task1562_10_20_40_0.5_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_20_40_0.5_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_20_40_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 05:44:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:44:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 05:45:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:45:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 29
expected_example_num: 200
selection_ratio: 0.145
finetune_vicuna!
{'loss': 1.0907, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3293, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.121, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 181.6342, 'train_samples_per_second': 0.479, 'train_steps_per_second': 0.083, 'train_loss': 0.42268879612286886, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-29 05:49:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:49:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_40_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.5631492360260565

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_40_0.5_0.35_3_1/generated_contents/1
INFO 11-29 05:49:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:50:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_40_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.5897667138796981

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_40_0.5_0.35_3_1/generated_contents/2
INFO 11-29 05:50:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:50:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_40_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.5881654964301639

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_40_0.5_0.35_3_1/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-10 /data2/cyzhao/best_ckpt/NI_task1562_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_40_0.5_0.35_3_1/checkpoint-15
searching parameters: task1562_20_10_50_0.6_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_10_50_0.6_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_10_50_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 05:50:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:50:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 05:51:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:52:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 50
expected_example_num: 200
selection_ratio: 0.25
finetune_vicuna!
{'loss': 1.0019, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.5304, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.3346, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1237, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1494, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.041, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 198.9636, 'train_samples_per_second': 0.754, 'train_steps_per_second': 0.136, 'train_loss': 0.3278612593809764, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 05:56:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:56:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_10_50_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.6022941444624144

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_10_50_0.6_0.4_3_1/generated_contents/1
INFO 11-29 05:56:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:57:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_10_50_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.4957429056517954

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_10_50_0.6_0.4_3_1/generated_contents/2
INFO 11-29 05:57:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:57:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_10_50_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.48027834488437904

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_10_50_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1562_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-9 /data2/cyzhao/best_ckpt/NI_task1562_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_10_50_0.6_0.4_3_1/checkpoint-27
searching parameters: task1562_10_20_50_0.5_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_20_50_0.5_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_20_50_0.5_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 05:57:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:58:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 05:58:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:59:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 36
expected_example_num: 200
selection_ratio: 0.18
finetune_vicuna!
{'loss': 1.2474, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.455, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1718, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0781, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 180.214, 'train_samples_per_second': 0.599, 'train_steps_per_second': 0.1, 'train_loss': 0.4373418713609378, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:03:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:03:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_50_0.5_0.3_3_1 epoch 1

------------------------------------------------

0.6598148736026436

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_50_0.5_0.3_3_1/generated_contents/1
INFO 11-29 06:03:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:03:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_50_0.5_0.3_3_1 epoch 2

------------------------------------------------

0.6094500000958525

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_50_0.5_0.3_3_1/generated_contents/2
INFO 11-29 06:04:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:04:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_50_0.5_0.3_3_1 epoch 3

------------------------------------------------

0.6254952573220538

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_50_0.5_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1562_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-6 /data2/cyzhao/best_ckpt/NI_task1562_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_50_0.5_0.3_3_1/checkpoint-18
searching parameters: task1562_20_20_45_0.7_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_20_45_0.7_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_20_45_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:04:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:04:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:07:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:07:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 75
expected_example_num: 400
selection_ratio: 0.1875
finetune_vicuna!
{'loss': 0.8566, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.587, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.5962, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.2802, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.1291, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1266, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1096, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0613, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0465, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 195.2447, 'train_samples_per_second': 1.152, 'train_steps_per_second': 0.2, 'train_loss': 0.2901692596765665, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:12:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:12:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_20_45_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.5971034448041201

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_20_45_0.7_0.4_3_1/generated_contents/1
INFO 11-29 06:12:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:13:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_20_45_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.5978908937249507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_20_45_0.7_0.4_3_1/generated_contents/2
INFO 11-29 06:13:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:13:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_20_45_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.5938091135223302

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_20_45_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_20_45_0.7_0.4_3_1/checkpoint-39
searching parameters: task1562_30_20_40_0.5_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_30_20_40_0.5_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_30_20_40_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:13:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:13:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:15:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:15:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 62
expected_example_num: 600
selection_ratio: 0.10333333333333333
finetune_vicuna!
{'loss': 1.3561, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.5989, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.3905, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2295, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1847, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.134, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0837, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0552, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 200.7044, 'train_samples_per_second': 0.927, 'train_steps_per_second': 0.164, 'train_loss': 0.3700329711039861, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:20:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:21:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_20_40_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.5652824585606586

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_20_40_0.5_0.35_3_1/generated_contents/1
INFO 11-29 06:21:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:21:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_20_40_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.5854553656823341

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_20_40_0.5_0.35_3_1/generated_contents/2
INFO 11-29 06:21:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:21:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_20_40_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.5755038729678449

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_20_40_0.5_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_20_40_0.5_0.35_3_1/checkpoint-33
searching parameters: task1562_20_15_50_0.8_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.8_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:22:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:22:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:24:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:24:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 63
expected_example_num: 300
selection_ratio: 0.21
finetune_vicuna!
{'loss': 1.2978, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.9138, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4894, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.1912, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1919, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.1507, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0534, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.1009, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 195.184, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.169, 'train_loss': 0.4133520862369826, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:28:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:28:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.5469515867836237

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.8_0.3_3_1/generated_contents/1
INFO 11-29 06:29:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:29:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.616714213400046

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.8_0.3_3_1/generated_contents/2
INFO 11-29 06:29:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:29:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.620556437364088

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.8_0.3_3_1/checkpoint-33
searching parameters: task1562_20_15_50_0.7_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:30:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:30:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:31:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:32:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 76
expected_example_num: 300
selection_ratio: 0.25333333333333335
finetune_vicuna!
{'loss': 1.0784, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.5759, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.5399, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.3273, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.1473, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1898, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.103, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0658, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0583, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 207.9531, 'train_samples_per_second': 1.096, 'train_steps_per_second': 0.188, 'train_loss': 0.3194658983594332, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:37:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:37:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.5633242531330663

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.4_3_1/generated_contents/1
INFO 11-29 06:37:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:37:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.635729064920349

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.4_3_1/generated_contents/2
INFO 11-29 06:37:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:38:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.6429784605184676

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.4_3_1/checkpoint-39
searching parameters: task1562_10_10_40_0.5_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.5_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.5_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:38:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:38:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:39:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:39:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 15
expected_example_num: 100
selection_ratio: 0.15
finetune_vicuna!
{'loss': 1.0798, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.4526, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 174.4593, 'train_samples_per_second': 0.258, 'train_steps_per_second': 0.052, 'train_loss': 0.6901553504996829, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:43:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:43:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_10_40_0.5_0.3_3_1 epoch 1

------------------------------------------------

0.5075021188103059

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.5_0.3_3_1/generated_contents/1
INFO 11-29 06:43:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:44:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_10_40_0.5_0.3_3_1 epoch 2

------------------------------------------------

0.5360830800043462

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.5_0.3_3_1/generated_contents/2
INFO 11-29 06:44:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:44:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_10_40_0.5_0.3_3_1 epoch 3

------------------------------------------------

0.557327422334248

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.5_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.5_0.3_3_1/checkpoint-9
searching parameters: task1562_10_10_40_0.8_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.8_0.4_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.8_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:44:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:45:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:46:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:46:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 26
expected_example_num: 100
selection_ratio: 0.26
finetune_vicuna!
{'loss': 1.3307, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 2.8516, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.4437, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 176.9905, 'train_samples_per_second': 0.441, 'train_steps_per_second': 0.085, 'train_loss': 1.270008353392283, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:50:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:50:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_10_40_0.8_0.4_3_1 epoch 1

------------------------------------------------

0.49999027784720823

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.8_0.4_3_1/generated_contents/1
INFO 11-29 06:50:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:51:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_10_40_0.8_0.4_3_1 epoch 2

------------------------------------------------

0.5838674256977389

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.8_0.4_3_1/generated_contents/2
INFO 11-29 06:51:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:51:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_10_40_0.8_0.4_3_1 epoch 3

------------------------------------------------

0.6022136190047721

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_10_40_0.8_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_10_40_0.8_0.4_3_1/checkpoint-15
searching parameters: task1562_30_15_40_0.4_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_30_15_40_0.4_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_30_15_40_0.4_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:51:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:52:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:53:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:53:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 63
expected_example_num: 450
selection_ratio: 0.14
finetune_vicuna!
{'loss': 1.3367, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.7106, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4718, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.1786, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.2161, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.0766, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0769, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0504, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 202.7414, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.163, 'train_loss': 0.3805618274844054, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:58:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:58:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_15_40_0.4_0.35_3_1 epoch 1

------------------------------------------------

0.5697964184954301

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_15_40_0.4_0.35_3_1/generated_contents/1
INFO 11-29 06:58:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:59:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_15_40_0.4_0.35_3_1 epoch 2

------------------------------------------------

0.5748342490304182

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_15_40_0.4_0.35_3_1/generated_contents/2
INFO 11-29 06:59:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:59:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_15_40_0.4_0.35_3_1 epoch 3

------------------------------------------------

0.5619724924823304

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_15_40_0.4_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_40_0.4_0.35_3_1/checkpoint-33
searching parameters: task1562_10_20_45_0.4_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_20_45_0.4_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_10_20_45_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:59:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:59:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:00:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:00:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 33
expected_example_num: 200
selection_ratio: 0.165
finetune_vicuna!
{'loss': 0.9428, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.7562, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2499, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0635, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 186.1093, 'train_samples_per_second': 0.532, 'train_steps_per_second': 0.097, 'train_loss': 0.45755942745341194, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:05:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:05:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_45_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.5799555239892534

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_45_0.4_0.3_3_1/generated_contents/1
INFO 11-29 07:05:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:05:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_45_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.6078986381455654

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_45_0.4_0.3_3_1/generated_contents/2
INFO 11-29 07:06:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:06:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_10_20_45_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.5945107844008961

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_10_20_45_0.4_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_10_20_45_0.4_0.3_3_1/checkpoint-18
searching parameters: task1562_20_15_50_0.7_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:06:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:06:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:08:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:08:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 64
expected_example_num: 300
selection_ratio: 0.21333333333333335
finetune_vicuna!
{'loss': 1.0907, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.7467, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4928, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.1483, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1664, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.135, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0673, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0452, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 194.5345, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.17, 'train_loss': 0.35122298009016295, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:13:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:13:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.6765303044315744

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.3_3_1/generated_contents/1
INFO 11-29 07:13:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:13:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.700062826329969

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.3_3_1/generated_contents/2
INFO 11-29 07:14:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:14:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.6896363614202642

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1562_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-22 /data2/cyzhao/best_ckpt/NI_task1562_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.3_3_1/checkpoint-33
searching parameters: task1562_20_15_50_0.7_0.3_3_1
searching parameters: task1562_20_15_50_0.7_0.3_3_1
searching parameters: task1562_20_15_50_0.7_0.3_3_1
searching parameters: task1562_20_15_50_0.7_0.3_3_1
searching parameters: task1562_30_15_45_0.6_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_30_15_45_0.6_0.3_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_30_15_45_0.6_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:14:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:14:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:16:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:16:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 69
expected_example_num: 450
selection_ratio: 0.15333333333333332
finetune_vicuna!
{'loss': 1.1831, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5024, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.666, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1156, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.198, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1075, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0419, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0527, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0367, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 194.8982, 'train_samples_per_second': 1.062, 'train_steps_per_second': 0.185, 'train_loss': 0.3226523506972525, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:21:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:21:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_15_45_0.6_0.3_3_1 epoch 1

------------------------------------------------

0.6000878578230538

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_15_45_0.6_0.3_3_1/generated_contents/1
INFO 11-29 07:21:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:21:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_15_45_0.6_0.3_3_1 epoch 2

------------------------------------------------

0.6218189892623674

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_15_45_0.6_0.3_3_1/generated_contents/2
INFO 11-29 07:22:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:22:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_30_15_45_0.6_0.3_3_1 epoch 3

------------------------------------------------

0.6267218157989057

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_30_15_45_0.6_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_30_15_45_0.6_0.3_3_1/checkpoint-36
searching parameters: task1562_20_15_50_0.7_0.3_3_1
searching parameters: task1562_20_15_50_0.7_0.3_3_1
searching parameters: task1562_20_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:22:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:22:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:24:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:24:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 76
expected_example_num: 300
selection_ratio: 0.25333333333333335
finetune_vicuna!
{'loss': 1.0453, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.5641, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.5372, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.3119, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.1395, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1494, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.0828, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0514, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0357, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 213.2034, 'train_samples_per_second': 1.069, 'train_steps_per_second': 0.183, 'train_loss': 0.3014592587565764, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:29:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:30:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.35_3_1 epoch 1

------------------------------------------------

0.5852084066767115

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.35_3_1/generated_contents/1
INFO 11-29 07:30:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:30:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.35_3_1 epoch 2

------------------------------------------------

0.6072627990921229

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.35_3_1/generated_contents/2
INFO 11-29 07:30:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:31:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1562_20_15_50_0.7_0.35_3_1 epoch 3

------------------------------------------------

0.6332047076736661

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/task1562_20_15_50_0.7_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1562_20_15_50_0.7_0.35_3_1/checkpoint-39
{'generation_epochs': 20, 'generation_batch_size': 15, 'generation_top_k': 50, 'generation_temperature': 0.7, 'min_frequency': 0.3, 'training_epochs': 3}
test best ckpt.
INFO 11-29 07:31:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task1562_exp_1', tokenizer='/data2/cyzhao/best_ckpt/NI_task1562_exp_1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:31:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task1562_exp_1

------------------------------------------------

0.6232766519635013

------------------------------------------------


The best ckpt on test set gain 0.6232766519635013
Genrated contents are stored in /home/cyzhao/NI_task1562_exp_1/best_ckpt_generated_content
