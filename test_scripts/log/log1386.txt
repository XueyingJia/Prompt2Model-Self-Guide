[2023-11-23 22:08:58,291] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
searching parameters: NI_task1386_10_40_50_1.0_0.5_130_5
/home/cyzhao/NI_task1386_exp_1/NI_task1386_10_40_50_1.0_0.5_130_5
/home/cyzhao/NI_task1386_exp_1/NI_task1386_10_40_50_1.0_0.5_130_5/config.json
generate_and_write_inputs!
INFO 11-23 22:09:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:09:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 22:13:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:13:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 248
expected_example_num: 400
selection_ratio: 0.62
finetune_deepseek!
{'loss': 0.8343, 'learning_rate': 4.870967741935484e-05, 'epoch': 0.13}
{'loss': 0.4652, 'learning_rate': 4.741935483870968e-05, 'epoch': 0.26}
{'loss': 0.3035, 'learning_rate': 4.612903225806452e-05, 'epoch': 0.39}
{'loss': 0.5666, 'learning_rate': 4.4838709677419356e-05, 'epoch': 0.52}
{'loss': 0.2308, 'learning_rate': 4.3548387096774194e-05, 'epoch': 0.65}
{'loss': 0.3864, 'learning_rate': 4.225806451612904e-05, 'epoch': 0.77}
{'loss': 0.28, 'learning_rate': 4.096774193548387e-05, 'epoch': 0.9}
{'loss': 0.1317, 'learning_rate': 3.9677419354838716e-05, 'epoch': 1.03}
{'loss': 0.1602, 'learning_rate': 3.838709677419355e-05, 'epoch': 1.16}
{'loss': 0.0675, 'learning_rate': 3.7096774193548386e-05, 'epoch': 1.29}
{'loss': 0.1123, 'learning_rate': 3.580645161290323e-05, 'epoch': 1.42}
{'loss': 0.0269, 'learning_rate': 3.451612903225806e-05, 'epoch': 1.55}
{'loss': 0.0761, 'learning_rate': 3.322580645161291e-05, 'epoch': 1.68}
{'loss': 0.149, 'learning_rate': 3.193548387096774e-05, 'epoch': 1.81}
{'loss': 0.1711, 'learning_rate': 3.0645161290322585e-05, 'epoch': 1.94}
{'loss': 0.0914, 'learning_rate': 2.9354838709677417e-05, 'epoch': 2.06}
{'loss': 0.1348, 'learning_rate': 2.806451612903226e-05, 'epoch': 2.19}
{'loss': 0.092, 'learning_rate': 2.67741935483871e-05, 'epoch': 2.32}
{'loss': 0.0195, 'learning_rate': 2.5483870967741935e-05, 'epoch': 2.45}
{'loss': 0.0351, 'learning_rate': 2.4193548387096777e-05, 'epoch': 2.58}
{'loss': 0.037, 'learning_rate': 2.2903225806451615e-05, 'epoch': 2.71}
{'loss': 0.0586, 'learning_rate': 2.1612903225806454e-05, 'epoch': 2.84}
{'loss': 0.1101, 'learning_rate': 2.0322580645161292e-05, 'epoch': 2.97}
{'loss': 0.0538, 'learning_rate': 1.9032258064516127e-05, 'epoch': 3.1}
{'loss': 0.0397, 'learning_rate': 1.774193548387097e-05, 'epoch': 3.23}
{'loss': 0.0101, 'learning_rate': 1.6451612903225807e-05, 'epoch': 3.35}
{'loss': 0.0563, 'learning_rate': 1.5161290322580646e-05, 'epoch': 3.48}
{'loss': 0.0347, 'learning_rate': 1.3870967741935484e-05, 'epoch': 3.61}
{'loss': 0.044, 'learning_rate': 1.2580645161290322e-05, 'epoch': 3.74}
{'loss': 0.0065, 'learning_rate': 1.129032258064516e-05, 'epoch': 3.87}
{'loss': 0.0457, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0257, 'learning_rate': 8.70967741935484e-06, 'epoch': 4.13}
{'loss': 0.0123, 'learning_rate': 7.419354838709678e-06, 'epoch': 4.26}
{'loss': 0.0226, 'learning_rate': 6.129032258064516e-06, 'epoch': 4.39}
{'loss': 0.0308, 'learning_rate': 4.838709677419355e-06, 'epoch': 4.52}
{'loss': 0.0067, 'learning_rate': 3.5483870967741936e-06, 'epoch': 4.65}
{'loss': 0.0077, 'learning_rate': 2.2580645161290324e-06, 'epoch': 4.77}
{'loss': 0.0049, 'learning_rate': 9.67741935483871e-07, 'epoch': 4.9}
{'train_runtime': 588.7822, 'train_samples_per_second': 2.106, 'train_steps_per_second': 0.263, 'train_loss': 0.12801175062214173, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-155/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-124/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-31/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-62/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-93/optimizer.pt
validate!
last validate 0.
INFO 11-23 22:26:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-31', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-31', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:26:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_10_40_50_1.0_0.5_130_5 epoch 1

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_10_40_50_1.0_0.5_130_5/generated_contents/1
INFO 11-23 22:27:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-62', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-62', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:27:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_10_40_50_1.0_0.5_130_5 epoch 2

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_10_40_50_1.0_0.5_130_5/generated_contents/2
INFO 11-23 22:27:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-93', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-93', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:28:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_10_40_50_1.0_0.5_130_5 epoch 3

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_10_40_50_1.0_0.5_130_5/generated_contents/3
INFO 11-23 22:28:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-124', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-124', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:28:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_10_40_50_1.0_0.5_130_5 epoch 4

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_10_40_50_1.0_0.5_130_5/generated_contents/4
INFO 11-23 22:29:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-155', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-155', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:29:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_10_40_50_1.0_0.5_130_5 epoch 5

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_10_40_50_1.0_0.5_130_5/generated_contents/5
mv /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-31 /data2/cyzhao/best_ckpt/NI_task1386_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-62
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-93
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-124
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1386_10_40_50_1.0_0.5_130_5/checkpoint-155
searching parameters: NI_task1386_20_30_40_0.8_0.5_130_5
/home/cyzhao/NI_task1386_exp_1/NI_task1386_20_30_40_0.8_0.5_130_5
/home/cyzhao/NI_task1386_exp_1/NI_task1386_20_30_40_0.8_0.5_130_5/config.json
generate_and_write_inputs!
INFO 11-23 22:30:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:30:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 22:38:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:38:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 323
expected_example_num: 600
selection_ratio: 0.5383333333333333
finetune_deepseek!
{'loss': 1.1885, 'learning_rate': 4.902439024390244e-05, 'epoch': 0.1}
{'loss': 0.2916, 'learning_rate': 4.804878048780488e-05, 'epoch': 0.2}
{'loss': 0.122, 'learning_rate': 4.707317073170732e-05, 'epoch': 0.29}
{'loss': 0.0441, 'learning_rate': 4.609756097560976e-05, 'epoch': 0.39}
{'loss': 0.1955, 'learning_rate': 4.51219512195122e-05, 'epoch': 0.49}
{'loss': 0.2039, 'learning_rate': 4.414634146341464e-05, 'epoch': 0.59}
{'loss': 0.3625, 'learning_rate': 4.317073170731707e-05, 'epoch': 0.68}
{'loss': 0.1896, 'learning_rate': 4.2195121951219514e-05, 'epoch': 0.78}
{'loss': 0.307, 'learning_rate': 4.1219512195121954e-05, 'epoch': 0.88}
{'loss': 0.2639, 'learning_rate': 4.0243902439024395e-05, 'epoch': 0.98}
{'loss': 0.1263, 'learning_rate': 3.9268292682926835e-05, 'epoch': 1.07}
{'loss': 0.0523, 'learning_rate': 3.829268292682927e-05, 'epoch': 1.17}
{'loss': 0.2014, 'learning_rate': 3.731707317073171e-05, 'epoch': 1.27}
{'loss': 0.1561, 'learning_rate': 3.634146341463415e-05, 'epoch': 1.37}
{'loss': 0.0799, 'learning_rate': 3.5365853658536584e-05, 'epoch': 1.46}
{'loss': 0.0713, 'learning_rate': 3.4390243902439025e-05, 'epoch': 1.56}
{'loss': 0.0851, 'learning_rate': 3.3414634146341465e-05, 'epoch': 1.66}
{'loss': 0.1221, 'learning_rate': 3.2439024390243906e-05, 'epoch': 1.76}
{'loss': 0.0801, 'learning_rate': 3.146341463414634e-05, 'epoch': 1.85}
{'loss': 0.0339, 'learning_rate': 3.048780487804878e-05, 'epoch': 1.95}
{'loss': 0.0885, 'learning_rate': 2.951219512195122e-05, 'epoch': 2.05}
{'loss': 0.0321, 'learning_rate': 2.8536585365853658e-05, 'epoch': 2.15}
{'loss': 0.0272, 'learning_rate': 2.7560975609756102e-05, 'epoch': 2.24}
{'loss': 0.0353, 'learning_rate': 2.658536585365854e-05, 'epoch': 2.34}
{'loss': 0.0642, 'learning_rate': 2.5609756097560977e-05, 'epoch': 2.44}
{'loss': 0.0446, 'learning_rate': 2.4634146341463414e-05, 'epoch': 2.54}
{'loss': 0.1166, 'learning_rate': 2.3658536585365854e-05, 'epoch': 2.63}
{'loss': 0.0881, 'learning_rate': 2.2682926829268295e-05, 'epoch': 2.73}
{'loss': 0.0501, 'learning_rate': 2.1707317073170732e-05, 'epoch': 2.83}
{'loss': 0.0376, 'learning_rate': 2.073170731707317e-05, 'epoch': 2.93}
{'loss': 0.0106, 'learning_rate': 1.975609756097561e-05, 'epoch': 3.02}
{'loss': 0.1016, 'learning_rate': 1.878048780487805e-05, 'epoch': 3.12}
{'loss': 0.0477, 'learning_rate': 1.7804878048780488e-05, 'epoch': 3.22}
{'loss': 0.0184, 'learning_rate': 1.682926829268293e-05, 'epoch': 3.32}
{'loss': 0.0207, 'learning_rate': 1.5853658536585366e-05, 'epoch': 3.41}
{'loss': 0.0907, 'learning_rate': 1.4878048780487805e-05, 'epoch': 3.51}
{'loss': 0.013, 'learning_rate': 1.3902439024390245e-05, 'epoch': 3.61}
{'loss': 0.0416, 'learning_rate': 1.2926829268292684e-05, 'epoch': 3.71}
{'loss': 0.0341, 'learning_rate': 1.1951219512195121e-05, 'epoch': 3.8}
{'loss': 0.0256, 'learning_rate': 1.0975609756097562e-05, 'epoch': 3.9}
{'loss': 0.0446, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0075, 'learning_rate': 9.02439024390244e-06, 'epoch': 4.1}
{'loss': 0.0511, 'learning_rate': 8.048780487804879e-06, 'epoch': 4.2}
{'loss': 0.0099, 'learning_rate': 7.073170731707317e-06, 'epoch': 4.29}
{'loss': 0.0075, 'learning_rate': 6.0975609756097564e-06, 'epoch': 4.39}
{'loss': 0.0135, 'learning_rate': 5.121951219512195e-06, 'epoch': 4.49}
{'loss': 0.0102, 'learning_rate': 4.146341463414634e-06, 'epoch': 4.59}
{'loss': 0.0379, 'learning_rate': 3.1707317073170736e-06, 'epoch': 4.68}
{'loss': 0.0265, 'learning_rate': 2.195121951219512e-06, 'epoch': 4.78}
{'loss': 0.0095, 'learning_rate': 1.2195121951219514e-06, 'epoch': 4.88}
{'loss': 0.0044, 'learning_rate': 2.439024390243903e-07, 'epoch': 4.98}
{'train_runtime': 761.5573, 'train_samples_per_second': 2.121, 'train_steps_per_second': 0.269, 'train_loss': 0.10514695912531417, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-205/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-41/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-82/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-123/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-164/optimizer.pt
validate!
last validate 0.
INFO 11-23 22:55:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-41', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-41', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:56:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_20_30_40_0.8_0.5_130_5 epoch 1

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_20_30_40_0.8_0.5_130_5/generated_contents/1
INFO 11-23 22:56:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-82', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-82', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:56:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_20_30_40_0.8_0.5_130_5 epoch 2

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_20_30_40_0.8_0.5_130_5/generated_contents/2
INFO 11-23 22:57:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-123', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-123', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:57:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_20_30_40_0.8_0.5_130_5 epoch 3

------------------------------------------------

0.35714285714285715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1386_exp_1/NI_task1386_20_30_40_0.8_0.5_130_5/generated_contents/3
INFO 11-23 22:58:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-164', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1386_20_30_40_0.8_0.5_130_5/checkpoint-164', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:58:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task1386_20_30_40_0.8_0.5_130_5 epoch 4

------------------------------------------------

0.35714285714285715

------------------------------------------------


