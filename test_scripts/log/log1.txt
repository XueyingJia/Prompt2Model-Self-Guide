[2023-11-23 19:34:51,063] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
searching parameters: NI_task937_20_15_50_1.0_0.5_120_4
/home/cyzhao/NI_task937_exp_1/NI_task937_20_15_50_1.0_0.5_120_4
/home/cyzhao/NI_task937_exp_1/NI_task937_20_15_50_1.0_0.5_120_4/config.json
generate_and_write_inputs!
INFO 11-23 19:34:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:35:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 19:36:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:37:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 219
expected_example_num: 300
selection_ratio: 0.73
finetune_deepseek!
{'loss': 1.8482, 'learning_rate': 4.8214285714285716e-05, 'epoch': 0.14}
{'loss': 2.358, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.29}
{'loss': 0.559, 'learning_rate': 4.464285714285715e-05, 'epoch': 0.43}
{'loss': 0.5208, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.57}
{'loss': 0.2572, 'learning_rate': 4.107142857142857e-05, 'epoch': 0.71}
{'loss': 0.2373, 'learning_rate': 3.928571428571429e-05, 'epoch': 0.86}
{'loss': 0.1407, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.0}
{'loss': 0.363, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.14}
{'loss': 0.1935, 'learning_rate': 3.392857142857143e-05, 'epoch': 1.29}
{'loss': 0.1384, 'learning_rate': 3.2142857142857144e-05, 'epoch': 1.43}
{'loss': 0.1374, 'learning_rate': 3.0357142857142857e-05, 'epoch': 1.57}
{'loss': 0.1082, 'learning_rate': 2.857142857142857e-05, 'epoch': 1.71}
{'loss': 0.1101, 'learning_rate': 2.6785714285714288e-05, 'epoch': 1.86}
{'loss': 0.1489, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.0544, 'learning_rate': 2.3214285714285715e-05, 'epoch': 2.14}
{'loss': 0.1841, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.29}
{'loss': 0.0373, 'learning_rate': 1.9642857142857145e-05, 'epoch': 2.43}
{'loss': 0.173, 'learning_rate': 1.785714285714286e-05, 'epoch': 2.57}
{'loss': 0.0507, 'learning_rate': 1.6071428571428572e-05, 'epoch': 2.71}
{'loss': 0.0573, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.86}
{'loss': 0.1229, 'learning_rate': 1.25e-05, 'epoch': 3.0}
{'loss': 0.0186, 'learning_rate': 1.0714285714285714e-05, 'epoch': 3.14}
{'loss': 0.0148, 'learning_rate': 8.92857142857143e-06, 'epoch': 3.29}
{'loss': 0.1208, 'learning_rate': 7.142857142857143e-06, 'epoch': 3.43}
{'loss': 0.0155, 'learning_rate': 5.357142857142857e-06, 'epoch': 3.57}
{'loss': 0.0244, 'learning_rate': 3.5714285714285714e-06, 'epoch': 3.71}
{'loss': 0.0224, 'learning_rate': 1.7857142857142857e-06, 'epoch': 3.86}
{'loss': 0.006, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 294.5408, 'train_samples_per_second': 2.974, 'train_steps_per_second': 0.38, 'train_loss': 0.28652609427393017, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-56/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-112/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-84/optimizer.pt
validate!
last validate 0.
INFO 11-23 19:43:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:43:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_20_15_50_1.0_0.5_120_4 epoch 1

------------------------------------------------

0.5263333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_20_15_50_1.0_0.5_120_4/generated_contents/1
INFO 11-23 19:44:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-56', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-56', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:45:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_20_15_50_1.0_0.5_120_4 epoch 2

------------------------------------------------

0.4736666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_20_15_50_1.0_0.5_120_4/generated_contents/2
INFO 11-23 19:46:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-84', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-84', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:46:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_20_15_50_1.0_0.5_120_4 epoch 3

------------------------------------------------

0.499

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_20_15_50_1.0_0.5_120_4/generated_contents/3
INFO 11-23 19:48:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-112', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-112', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:48:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_20_15_50_1.0_0.5_120_4 epoch 4

------------------------------------------------

0.5016666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_20_15_50_1.0_0.5_120_4/generated_contents/4
mv /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-28 /data2/cyzhao/best_ckpt/NI_task937_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-56
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-84
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_20_15_50_1.0_0.5_120_4/checkpoint-112
searching parameters: NI_task937_10_20_50_0.8_0.4_115_5
/home/cyzhao/NI_task937_exp_1/NI_task937_10_20_50_0.8_0.4_115_5
/home/cyzhao/NI_task937_exp_1/NI_task937_10_20_50_0.8_0.4_115_5/config.json
generate_and_write_inputs!
INFO 11-23 19:50:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:50:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 19:51:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:51:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 150
expected_example_num: 200
selection_ratio: 0.75
finetune_deepseek!
{'loss': 1.6121, 'learning_rate': 4.789473684210526e-05, 'epoch': 0.21}
{'loss': 1.1198, 'learning_rate': 4.5789473684210527e-05, 'epoch': 0.42}
{'loss': 0.8362, 'learning_rate': 4.368421052631579e-05, 'epoch': 0.63}
{'loss': 0.2255, 'learning_rate': 4.157894736842106e-05, 'epoch': 0.84}
{'loss': 0.949, 'learning_rate': 3.9473684210526316e-05, 'epoch': 1.05}
{'loss': 0.1981, 'learning_rate': 3.736842105263158e-05, 'epoch': 1.26}
{'loss': 0.153, 'learning_rate': 3.526315789473684e-05, 'epoch': 1.47}
{'loss': 0.1685, 'learning_rate': 3.3157894736842106e-05, 'epoch': 1.68}
{'loss': 0.1148, 'learning_rate': 3.105263157894737e-05, 'epoch': 1.89}
{'loss': 0.4989, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.11}
{'loss': 0.0934, 'learning_rate': 2.6842105263157896e-05, 'epoch': 2.32}
{'loss': 0.1106, 'learning_rate': 2.4736842105263158e-05, 'epoch': 2.53}
{'loss': 0.0665, 'learning_rate': 2.2631578947368423e-05, 'epoch': 2.74}
{'loss': 0.1917, 'learning_rate': 2.0526315789473685e-05, 'epoch': 2.95}
{'loss': 0.181, 'learning_rate': 1.8421052631578947e-05, 'epoch': 3.16}
{'loss': 0.1985, 'learning_rate': 1.6315789473684213e-05, 'epoch': 3.37}
{'loss': 0.1081, 'learning_rate': 1.4210526315789475e-05, 'epoch': 3.58}
{'loss': 0.1615, 'learning_rate': 1.2105263157894737e-05, 'epoch': 3.79}
{'loss': 0.0733, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0834, 'learning_rate': 7.894736842105263e-06, 'epoch': 4.21}
{'loss': 0.0864, 'learning_rate': 5.789473684210527e-06, 'epoch': 4.42}
{'loss': 0.0765, 'learning_rate': 3.6842105263157892e-06, 'epoch': 4.63}
{'loss': 0.0875, 'learning_rate': 1.5789473684210528e-06, 'epoch': 4.84}
{'train_runtime': 341.2292, 'train_samples_per_second': 2.198, 'train_steps_per_second': 0.278, 'train_loss': 0.3143970084817786, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-95/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-76/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-23 19:58:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 19:58:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_50_0.8_0.4_115_5 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_50_0.8_0.4_115_5/generated_contents/1
INFO 11-23 19:59:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:00:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_50_0.8_0.4_115_5 epoch 2

------------------------------------------------

0.4736666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_50_0.8_0.4_115_5/generated_contents/2
INFO 11-23 20:01:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:01:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_50_0.8_0.4_115_5 epoch 3

------------------------------------------------

0.5253333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_50_0.8_0.4_115_5/generated_contents/3
INFO 11-23 20:03:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-76', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-76', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:03:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_50_0.8_0.4_115_5 epoch 4

------------------------------------------------

0.4736666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_50_0.8_0.4_115_5/generated_contents/4
INFO 11-23 20:04:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-95', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-95', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:05:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_50_0.8_0.4_115_5 epoch 5

------------------------------------------------

0.4736666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_50_0.8_0.4_115_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-57
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-76
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_50_0.8_0.4_115_5/checkpoint-95
searching parameters: NI_task937_30_20_45_0.9_0.5_130_3
/home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.5_130_3
/home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.5_130_3/config.json
generate_and_write_inputs!
INFO 11-23 20:06:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:06:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 20:10:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:10:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 436
expected_example_num: 600
selection_ratio: 0.7266666666666667
finetune_deepseek!
{'loss': 2.7822, 'learning_rate': 4.878787878787879e-05, 'epoch': 0.07}
{'loss': 1.2932, 'learning_rate': 4.7575757575757576e-05, 'epoch': 0.15}
{'loss': 0.3141, 'learning_rate': 4.636363636363636e-05, 'epoch': 0.22}
{'loss': 0.2576, 'learning_rate': 4.515151515151516e-05, 'epoch': 0.29}
{'loss': 0.3532, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.2891, 'learning_rate': 4.2727272727272724e-05, 'epoch': 0.44}
{'loss': 0.1202, 'learning_rate': 4.151515151515152e-05, 'epoch': 0.51}
{'loss': 0.3126, 'learning_rate': 4.0303030303030305e-05, 'epoch': 0.58}
{'loss': 0.2717, 'learning_rate': 3.909090909090909e-05, 'epoch': 0.65}
{'loss': 0.1833, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.167, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3448, 'learning_rate': 3.545454545454546e-05, 'epoch': 0.87}
{'loss': 0.1181, 'learning_rate': 3.424242424242424e-05, 'epoch': 0.95}
{'loss': 0.0405, 'learning_rate': 3.303030303030303e-05, 'epoch': 1.02}
{'loss': 0.2282, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.279, 'learning_rate': 3.060606060606061e-05, 'epoch': 1.16}
{'loss': 0.0354, 'learning_rate': 2.9393939393939394e-05, 'epoch': 1.24}
{'loss': 0.0335, 'learning_rate': 2.818181818181818e-05, 'epoch': 1.31}
{'loss': 0.026, 'learning_rate': 2.696969696969697e-05, 'epoch': 1.38}
{'loss': 0.1067, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.2515, 'learning_rate': 2.4545454545454545e-05, 'epoch': 1.53}
{'loss': 0.0195, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1313, 'learning_rate': 2.2121212121212123e-05, 'epoch': 1.67}
{'loss': 0.1466, 'learning_rate': 2.090909090909091e-05, 'epoch': 1.75}
{'loss': 0.1797, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.1424, 'learning_rate': 1.8484848484848487e-05, 'epoch': 1.89}
{'loss': 0.1254, 'learning_rate': 1.7272727272727274e-05, 'epoch': 1.96}
{'loss': 0.0414, 'learning_rate': 1.606060606060606e-05, 'epoch': 2.04}
{'loss': 0.0221, 'learning_rate': 1.484848484848485e-05, 'epoch': 2.11}
{'loss': 0.024, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0094, 'learning_rate': 1.2424242424242424e-05, 'epoch': 2.25}
{'loss': 0.0166, 'learning_rate': 1.1212121212121212e-05, 'epoch': 2.33}
{'loss': 0.0732, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0029, 'learning_rate': 8.787878787878788e-06, 'epoch': 2.47}
{'loss': 0.0176, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.027, 'learning_rate': 6.363636363636363e-06, 'epoch': 2.62}
{'loss': 0.0227, 'learning_rate': 5.151515151515152e-06, 'epoch': 2.69}
{'loss': 0.0025, 'learning_rate': 3.939393939393939e-06, 'epoch': 2.76}
{'loss': 0.0226, 'learning_rate': 2.7272727272727272e-06, 'epoch': 2.84}
{'loss': 0.026, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'loss': 0.001, 'learning_rate': 3.0303030303030305e-07, 'epoch': 2.98}
{'train_runtime': 309.2122, 'train_samples_per_second': 4.23, 'train_steps_per_second': 0.534, 'train_loss': 0.21483250735821485, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-55/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-110/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-165/optimizer.pt
validate!
last validate 0.
INFO 11-23 20:17:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-55', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-55', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:17:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.5_130_3 epoch 1

------------------------------------------------

0.5666666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.5_130_3/generated_contents/1
INFO 11-23 20:18:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-110', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-110', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:19:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.5_130_3 epoch 2

------------------------------------------------

0.6126666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.5_130_3/generated_contents/2
INFO 11-23 20:20:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-165', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-165', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:20:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.5_130_3 epoch 3

------------------------------------------------

0.591

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.5_130_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task937_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-110 /data2/cyzhao/best_ckpt/NI_task937_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-55
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.5_130_3/checkpoint-165
searching parameters: NI_task937_40_10_40_0.8_0.4_125_5
/home/cyzhao/NI_task937_exp_1/NI_task937_40_10_40_0.8_0.4_125_5
/home/cyzhao/NI_task937_exp_1/NI_task937_40_10_40_0.8_0.4_125_5/config.json
generate_and_write_inputs!
INFO 11-23 20:22:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:22:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 20:25:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:25:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 336
expected_example_num: 400
selection_ratio: 0.84
finetune_deepseek!
{'loss': 2.6453, 'learning_rate': 4.904761904761905e-05, 'epoch': 0.1}
{'loss': 1.2545, 'learning_rate': 4.80952380952381e-05, 'epoch': 0.19}
{'loss': 0.5105, 'learning_rate': 4.714285714285714e-05, 'epoch': 0.29}
{'loss': 0.7597, 'learning_rate': 4.6190476190476194e-05, 'epoch': 0.38}
{'loss': 0.3708, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.48}
{'loss': 0.501, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.57}
{'loss': 0.1202, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.67}
{'loss': 0.2511, 'learning_rate': 4.2380952380952385e-05, 'epoch': 0.76}
{'loss': 0.1635, 'learning_rate': 4.1428571428571437e-05, 'epoch': 0.86}
{'loss': 0.2602, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.95}
{'loss': 0.227, 'learning_rate': 3.9523809523809526e-05, 'epoch': 1.05}
{'loss': 0.1668, 'learning_rate': 3.857142857142858e-05, 'epoch': 1.14}
{'loss': 0.148, 'learning_rate': 3.761904761904762e-05, 'epoch': 1.24}
{'loss': 0.185, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}
{'loss': 0.1613, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.43}
{'loss': 0.145, 'learning_rate': 3.476190476190476e-05, 'epoch': 1.52}
{'loss': 0.1315, 'learning_rate': 3.380952380952381e-05, 'epoch': 1.62}
{'loss': 0.1345, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.71}
{'loss': 0.0787, 'learning_rate': 3.19047619047619e-05, 'epoch': 1.81}
{'loss': 0.1269, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.9}
{'loss': 0.1351, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.0747, 'learning_rate': 2.9047619047619052e-05, 'epoch': 2.1}
{'loss': 0.0908, 'learning_rate': 2.8095238095238096e-05, 'epoch': 2.19}
{'loss': 0.0879, 'learning_rate': 2.714285714285714e-05, 'epoch': 2.29}
{'loss': 0.2426, 'learning_rate': 2.6190476190476192e-05, 'epoch': 2.38}
{'loss': 0.0361, 'learning_rate': 2.523809523809524e-05, 'epoch': 2.48}
{'loss': 0.075, 'learning_rate': 2.4285714285714288e-05, 'epoch': 2.57}
{'loss': 0.1364, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}
{'loss': 0.1083, 'learning_rate': 2.2380952380952384e-05, 'epoch': 2.76}
{'loss': 0.1504, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.86}
{'loss': 0.0617, 'learning_rate': 2.0476190476190476e-05, 'epoch': 2.95}
{'loss': 0.0624, 'learning_rate': 1.9523809523809524e-05, 'epoch': 3.05}
{'loss': 0.0116, 'learning_rate': 1.8571428571428572e-05, 'epoch': 3.14}
{'loss': 0.0269, 'learning_rate': 1.761904761904762e-05, 'epoch': 3.24}
{'loss': 0.0307, 'learning_rate': 1.6666666666666667e-05, 'epoch': 3.33}
{'loss': 0.0908, 'learning_rate': 1.5714285714285715e-05, 'epoch': 3.43}
{'loss': 0.067, 'learning_rate': 1.4761904761904763e-05, 'epoch': 3.52}
{'loss': 0.0278, 'learning_rate': 1.3809523809523811e-05, 'epoch': 3.62}
{'loss': 0.0827, 'learning_rate': 1.2857142857142857e-05, 'epoch': 3.71}
{'loss': 0.0027, 'learning_rate': 1.1904761904761905e-05, 'epoch': 3.81}
{'loss': 0.0015, 'learning_rate': 1.0952380952380953e-05, 'epoch': 3.9}
{'loss': 0.0097, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0017, 'learning_rate': 9.047619047619047e-06, 'epoch': 4.1}
{'loss': 0.0116, 'learning_rate': 8.095238095238097e-06, 'epoch': 4.19}
{'loss': 0.0033, 'learning_rate': 7.142857142857143e-06, 'epoch': 4.29}
{'loss': 0.0009, 'learning_rate': 6.190476190476191e-06, 'epoch': 4.38}
{'loss': 0.0011, 'learning_rate': 5.2380952380952384e-06, 'epoch': 4.48}
{'loss': 0.0067, 'learning_rate': 4.285714285714286e-06, 'epoch': 4.57}
{'loss': 0.0005, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.67}
{'loss': 0.0014, 'learning_rate': 2.3809523809523808e-06, 'epoch': 4.76}
{'loss': 0.0247, 'learning_rate': 1.4285714285714286e-06, 'epoch': 4.86}
{'loss': 0.0234, 'learning_rate': 4.761904761904763e-07, 'epoch': 4.95}
{'train_runtime': 448.6315, 'train_samples_per_second': 3.745, 'train_steps_per_second': 0.468, 'train_loss': 0.1910581527615986, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-168/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-210/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-126/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-84/optimizer.pt
validate!
last validate 0.
INFO 11-23 20:34:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:34:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_10_40_0.8_0.4_125_5 epoch 1

------------------------------------------------

0.4736666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_10_40_0.8_0.4_125_5/generated_contents/1
INFO 11-23 20:36:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-84', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-84', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:36:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_10_40_0.8_0.4_125_5 epoch 2

------------------------------------------------

0.5263333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_10_40_0.8_0.4_125_5/generated_contents/2
INFO 11-23 20:37:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-126', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-126', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:38:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_10_40_0.8_0.4_125_5 epoch 3

------------------------------------------------

0.5693333333333334

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_10_40_0.8_0.4_125_5/generated_contents/3
INFO 11-23 20:39:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-168', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-168', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:39:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_10_40_0.8_0.4_125_5 epoch 4

------------------------------------------------

0.5603333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_10_40_0.8_0.4_125_5/generated_contents/4
INFO 11-23 20:41:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-210', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-210', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:41:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_10_40_0.8_0.4_125_5 epoch 5

------------------------------------------------

0.5793333333333334

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_10_40_0.8_0.4_125_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-84
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-126
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-168
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_10_40_0.8_0.4_125_5/checkpoint-210
searching parameters: NI_task937_40_30_45_0.9_0.4_130_5
/home/cyzhao/NI_task937_exp_1/NI_task937_40_30_45_0.9_0.4_130_5
/home/cyzhao/NI_task937_exp_1/NI_task937_40_30_45_0.9_0.4_130_5/config.json
generate_and_write_inputs!
INFO 11-23 20:42:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:43:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 20:48:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 20:48:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 857
expected_example_num: 1200
selection_ratio: 0.7141666666666666
finetune_deepseek!
{'loss': 2.5757, 'learning_rate': 4.962962962962963e-05, 'epoch': 0.04}
{'loss': 1.0524, 'learning_rate': 4.925925925925926e-05, 'epoch': 0.07}
{'loss': 0.3818, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.11}
{'loss': 0.1431, 'learning_rate': 4.851851851851852e-05, 'epoch': 0.15}
{'loss': 0.8964, 'learning_rate': 4.814814814814815e-05, 'epoch': 0.19}
{'loss': 0.3005, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.22}
{'loss': 0.117, 'learning_rate': 4.740740740740741e-05, 'epoch': 0.26}
{'loss': 0.1635, 'learning_rate': 4.703703703703704e-05, 'epoch': 0.3}
{'loss': 0.1413, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.33}
{'loss': 0.1536, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.37}
{'loss': 0.2481, 'learning_rate': 4.592592592592593e-05, 'epoch': 0.41}
{'loss': 0.1913, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.44}
{'loss': 0.1109, 'learning_rate': 4.518518518518519e-05, 'epoch': 0.48}
{'loss': 0.0763, 'learning_rate': 4.481481481481482e-05, 'epoch': 0.52}
{'loss': 0.1825, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.56}
{'loss': 0.1342, 'learning_rate': 4.4074074074074076e-05, 'epoch': 0.59}
{'loss': 0.3162, 'learning_rate': 4.3703703703703705e-05, 'epoch': 0.63}
{'loss': 0.0765, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.67}
{'loss': 0.0849, 'learning_rate': 4.296296296296296e-05, 'epoch': 0.7}
{'loss': 0.1085, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.74}
{'loss': 0.1634, 'learning_rate': 4.222222222222222e-05, 'epoch': 0.78}
{'loss': 0.0845, 'learning_rate': 4.185185185185185e-05, 'epoch': 0.81}
{'loss': 0.1867, 'learning_rate': 4.148148148148148e-05, 'epoch': 0.85}
{'loss': 0.1094, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.89}
{'loss': 0.0644, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.93}
{'loss': 0.0799, 'learning_rate': 4.0370370370370374e-05, 'epoch': 0.96}
{'loss': 0.04, 'learning_rate': 4e-05, 'epoch': 1.0}
{'loss': 0.1203, 'learning_rate': 3.962962962962963e-05, 'epoch': 1.04}
{'loss': 0.0906, 'learning_rate': 3.925925925925926e-05, 'epoch': 1.07}
{'loss': 0.1022, 'learning_rate': 3.888888888888889e-05, 'epoch': 1.11}
{'loss': 0.0298, 'learning_rate': 3.851851851851852e-05, 'epoch': 1.15}
{'loss': 0.0193, 'learning_rate': 3.814814814814815e-05, 'epoch': 1.19}
{'loss': 0.0478, 'learning_rate': 3.777777777777778e-05, 'epoch': 1.22}
{'loss': 0.0938, 'learning_rate': 3.740740740740741e-05, 'epoch': 1.26}
{'loss': 0.0988, 'learning_rate': 3.7037037037037037e-05, 'epoch': 1.3}
{'loss': 0.013, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}
{'loss': 0.0744, 'learning_rate': 3.62962962962963e-05, 'epoch': 1.37}
{'loss': 0.0459, 'learning_rate': 3.592592592592593e-05, 'epoch': 1.41}
{'loss': 0.0169, 'learning_rate': 3.555555555555556e-05, 'epoch': 1.44}
{'loss': 0.0133, 'learning_rate': 3.518518518518519e-05, 'epoch': 1.48}
{'loss': 0.1332, 'learning_rate': 3.481481481481482e-05, 'epoch': 1.52}
{'loss': 0.0967, 'learning_rate': 3.444444444444445e-05, 'epoch': 1.56}
{'loss': 0.0612, 'learning_rate': 3.4074074074074077e-05, 'epoch': 1.59}
{'loss': 0.0735, 'learning_rate': 3.3703703703703706e-05, 'epoch': 1.63}
{'loss': 0.0741, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.67}
{'loss': 0.0209, 'learning_rate': 3.2962962962962964e-05, 'epoch': 1.7}
{'loss': 0.0217, 'learning_rate': 3.25925925925926e-05, 'epoch': 1.74}
{'loss': 0.0858, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.78}
{'loss': 0.1747, 'learning_rate': 3.185185185185185e-05, 'epoch': 1.81}
{'loss': 0.0788, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.85}
{'loss': 0.08, 'learning_rate': 3.111111111111111e-05, 'epoch': 1.89}
{'loss': 0.0242, 'learning_rate': 3.074074074074074e-05, 'epoch': 1.93}
{'loss': 0.0541, 'learning_rate': 3.037037037037037e-05, 'epoch': 1.96}
{'loss': 0.018, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.0057, 'learning_rate': 2.962962962962963e-05, 'epoch': 2.04}
{'loss': 0.0287, 'learning_rate': 2.925925925925926e-05, 'epoch': 2.07}
{'loss': 0.0193, 'learning_rate': 2.8888888888888888e-05, 'epoch': 2.11}
{'loss': 0.0006, 'learning_rate': 2.851851851851852e-05, 'epoch': 2.15}
{'loss': 0.0015, 'learning_rate': 2.814814814814815e-05, 'epoch': 2.19}
{'loss': 0.0473, 'learning_rate': 2.777777777777778e-05, 'epoch': 2.22}
{'loss': 0.034, 'learning_rate': 2.7407407407407408e-05, 'epoch': 2.26}
{'loss': 0.0093, 'learning_rate': 2.7037037037037037e-05, 'epoch': 2.3}
{'loss': 0.0308, 'learning_rate': 2.6666666666666667e-05, 'epoch': 2.33}
{'loss': 0.0764, 'learning_rate': 2.6296296296296296e-05, 'epoch': 2.37}
{'loss': 0.0036, 'learning_rate': 2.5925925925925925e-05, 'epoch': 2.41}
{'loss': 0.0078, 'learning_rate': 2.5555555555555554e-05, 'epoch': 2.44}
{'loss': 0.0032, 'learning_rate': 2.5185185185185183e-05, 'epoch': 2.48}
{'loss': 0.053, 'learning_rate': 2.4814814814814816e-05, 'epoch': 2.52}
{'loss': 0.0029, 'learning_rate': 2.4444444444444445e-05, 'epoch': 2.56}
{'loss': 0.0004, 'learning_rate': 2.4074074074074074e-05, 'epoch': 2.59}
{'loss': 0.0002, 'learning_rate': 2.3703703703703707e-05, 'epoch': 2.63}
{'loss': 0.0569, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}
{'loss': 0.0029, 'learning_rate': 2.2962962962962965e-05, 'epoch': 2.7}
{'loss': 0.0207, 'learning_rate': 2.2592592592592594e-05, 'epoch': 2.74}
{'loss': 0.0004, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.78}
{'loss': 0.0508, 'learning_rate': 2.1851851851851852e-05, 'epoch': 2.81}
{'loss': 0.0008, 'learning_rate': 2.148148148148148e-05, 'epoch': 2.85}
{'loss': 0.0012, 'learning_rate': 2.111111111111111e-05, 'epoch': 2.89}
{'loss': 0.026, 'learning_rate': 2.074074074074074e-05, 'epoch': 2.93}
{'loss': 0.0018, 'learning_rate': 2.037037037037037e-05, 'epoch': 2.96}
{'loss': 0.0611, 'learning_rate': 2e-05, 'epoch': 3.0}
{'loss': 0.0003, 'learning_rate': 1.962962962962963e-05, 'epoch': 3.04}
{'loss': 0.0156, 'learning_rate': 1.925925925925926e-05, 'epoch': 3.07}
{'loss': 0.0006, 'learning_rate': 1.888888888888889e-05, 'epoch': 3.11}
{'loss': 0.0048, 'learning_rate': 1.8518518518518518e-05, 'epoch': 3.15}
{'loss': 0.0005, 'learning_rate': 1.814814814814815e-05, 'epoch': 3.19}
{'loss': 0.0344, 'learning_rate': 1.777777777777778e-05, 'epoch': 3.22}
{'loss': 0.0005, 'learning_rate': 1.740740740740741e-05, 'epoch': 3.26}
{'loss': 0.0221, 'learning_rate': 1.7037037037037038e-05, 'epoch': 3.3}
{'loss': 0.0007, 'learning_rate': 1.6666666666666667e-05, 'epoch': 3.33}
{'loss': 0.0008, 'learning_rate': 1.62962962962963e-05, 'epoch': 3.37}
{'loss': 0.0297, 'learning_rate': 1.5925925925925926e-05, 'epoch': 3.41}
{'loss': 0.0005, 'learning_rate': 1.5555555555555555e-05, 'epoch': 3.44}
{'loss': 0.0427, 'learning_rate': 1.5185185185185186e-05, 'epoch': 3.48}
{'loss': 0.0053, 'learning_rate': 1.4814814814814815e-05, 'epoch': 3.52}
{'loss': 0.0003, 'learning_rate': 1.4444444444444444e-05, 'epoch': 3.56}
{'loss': 0.0003, 'learning_rate': 1.4074074074074075e-05, 'epoch': 3.59}
{'loss': 0.0164, 'learning_rate': 1.3703703703703704e-05, 'epoch': 3.63}
{'loss': 0.0236, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.67}
{'loss': 0.0004, 'learning_rate': 1.2962962962962962e-05, 'epoch': 3.7}
{'loss': 0.0221, 'learning_rate': 1.2592592592592592e-05, 'epoch': 3.74}
{'loss': 0.0003, 'learning_rate': 1.2222222222222222e-05, 'epoch': 3.78}
{'loss': 0.0003, 'learning_rate': 1.1851851851851853e-05, 'epoch': 3.81}
{'loss': 0.0003, 'learning_rate': 1.1481481481481482e-05, 'epoch': 3.85}
{'loss': 0.0004, 'learning_rate': 1.1111111111111112e-05, 'epoch': 3.89}
{'loss': 0.0003, 'learning_rate': 1.074074074074074e-05, 'epoch': 3.93}
{'loss': 0.0005, 'learning_rate': 1.037037037037037e-05, 'epoch': 3.96}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0136, 'learning_rate': 9.62962962962963e-06, 'epoch': 4.04}
{'loss': 0.0003, 'learning_rate': 9.259259259259259e-06, 'epoch': 4.07}
{'loss': 0.0004, 'learning_rate': 8.88888888888889e-06, 'epoch': 4.11}
{'loss': 0.0003, 'learning_rate': 8.518518518518519e-06, 'epoch': 4.15}
{'loss': 0.0011, 'learning_rate': 8.14814814814815e-06, 'epoch': 4.19}
{'loss': 0.0003, 'learning_rate': 7.777777777777777e-06, 'epoch': 4.22}
{'loss': 0.0003, 'learning_rate': 7.4074074074074075e-06, 'epoch': 4.26}
{'loss': 0.0003, 'learning_rate': 7.0370370370370375e-06, 'epoch': 4.3}
{'loss': 0.0005, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.33}
{'loss': 0.0013, 'learning_rate': 6.296296296296296e-06, 'epoch': 4.37}
{'loss': 0.001, 'learning_rate': 5.925925925925927e-06, 'epoch': 4.41}
{'loss': 0.0002, 'learning_rate': 5.555555555555556e-06, 'epoch': 4.44}
{'loss': 0.0012, 'learning_rate': 5.185185185185185e-06, 'epoch': 4.48}
{'loss': 0.0395, 'learning_rate': 4.814814814814815e-06, 'epoch': 4.52}
{'loss': 0.0003, 'learning_rate': 4.444444444444445e-06, 'epoch': 4.56}
{'loss': 0.0004, 'learning_rate': 4.074074074074075e-06, 'epoch': 4.59}
{'loss': 0.0004, 'learning_rate': 3.7037037037037037e-06, 'epoch': 4.63}
{'loss': 0.0004, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.67}
{'loss': 0.0003, 'learning_rate': 2.9629629629629633e-06, 'epoch': 4.7}
{'loss': 0.0093, 'learning_rate': 2.5925925925925925e-06, 'epoch': 4.74}
{'loss': 0.0003, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.78}
{'loss': 0.0004, 'learning_rate': 1.8518518518518519e-06, 'epoch': 4.81}
{'loss': 0.0002, 'learning_rate': 1.4814814814814817e-06, 'epoch': 4.85}
{'loss': 0.0004, 'learning_rate': 1.1111111111111112e-06, 'epoch': 4.89}
{'loss': 0.0003, 'learning_rate': 7.407407407407408e-07, 'epoch': 4.93}
{'loss': 0.0002, 'learning_rate': 3.703703703703704e-07, 'epoch': 4.96}
{'loss': 0.0139, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 727.6674, 'train_samples_per_second': 5.889, 'train_steps_per_second': 0.742, 'train_loss': 0.08003131557725957, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-432/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-540/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-216/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-108/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-324/optimizer.pt
validate!
last validate 0.
INFO 11-23 21:03:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-108', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-108', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:03:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_30_45_0.9_0.4_130_5 epoch 1

------------------------------------------------

0.6073333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_30_45_0.9_0.4_130_5/generated_contents/1
INFO 11-23 21:05:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-216', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-216', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:05:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_30_45_0.9_0.4_130_5 epoch 2

------------------------------------------------

0.492

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_30_45_0.9_0.4_130_5/generated_contents/2
INFO 11-23 21:06:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-324', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-324', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:07:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_30_45_0.9_0.4_130_5 epoch 3

------------------------------------------------

0.5726666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_30_45_0.9_0.4_130_5/generated_contents/3
INFO 11-23 21:08:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-432', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-432', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:08:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_30_45_0.9_0.4_130_5 epoch 4

------------------------------------------------

0.5763333333333334

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_30_45_0.9_0.4_130_5/generated_contents/4
INFO 11-23 21:10:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-540', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-540', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:10:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_40_30_45_0.9_0.4_130_5 epoch 5

------------------------------------------------

0.578

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_40_30_45_0.9_0.4_130_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-108
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-216
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-324
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-432
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_40_30_45_0.9_0.4_130_5/checkpoint-540
searching parameters: NI_task937_50_20_50_0.8_0.3_120_4
/home/cyzhao/NI_task937_exp_1/NI_task937_50_20_50_0.8_0.3_120_4
/home/cyzhao/NI_task937_exp_1/NI_task937_50_20_50_0.8_0.3_120_4/config.json
generate_and_write_inputs!
INFO 11-23 21:12:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:12:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 21:16:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:17:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 785
expected_example_num: 1000
selection_ratio: 0.785
finetune_deepseek!
{'loss': 3.1112, 'learning_rate': 4.94949494949495e-05, 'epoch': 0.04}
{'loss': 1.6212, 'learning_rate': 4.898989898989899e-05, 'epoch': 0.08}
{'loss': 0.6283, 'learning_rate': 4.848484848484849e-05, 'epoch': 0.12}
{'loss': 0.2572, 'learning_rate': 4.797979797979798e-05, 'epoch': 0.16}
{'loss': 0.2115, 'learning_rate': 4.7474747474747476e-05, 'epoch': 0.2}
{'loss': 0.2128, 'learning_rate': 4.696969696969697e-05, 'epoch': 0.24}
{'loss': 0.1768, 'learning_rate': 4.6464646464646464e-05, 'epoch': 0.28}
{'loss': 0.085, 'learning_rate': 4.595959595959596e-05, 'epoch': 0.32}
{'loss': 0.1497, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.36}
{'loss': 0.1021, 'learning_rate': 4.494949494949495e-05, 'epoch': 0.4}
{'loss': 0.1237, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.44}
{'loss': 0.4521, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.48}
{'loss': 0.0856, 'learning_rate': 4.343434343434344e-05, 'epoch': 0.53}
{'loss': 0.111, 'learning_rate': 4.292929292929293e-05, 'epoch': 0.57}
{'loss': 0.0795, 'learning_rate': 4.242424242424243e-05, 'epoch': 0.61}
{'loss': 0.096, 'learning_rate': 4.191919191919192e-05, 'epoch': 0.65}
{'loss': 0.1348, 'learning_rate': 4.141414141414142e-05, 'epoch': 0.69}
{'loss': 0.0325, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.73}
{'loss': 0.1242, 'learning_rate': 4.0404040404040405e-05, 'epoch': 0.77}
{'loss': 0.0145, 'learning_rate': 3.98989898989899e-05, 'epoch': 0.81}
{'loss': 0.2757, 'learning_rate': 3.939393939393939e-05, 'epoch': 0.85}
{'loss': 0.0527, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.89}
{'loss': 0.0721, 'learning_rate': 3.838383838383838e-05, 'epoch': 0.93}
{'loss': 0.2625, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.97}
{'loss': 0.0821, 'learning_rate': 3.7373737373737376e-05, 'epoch': 1.01}
{'loss': 0.1443, 'learning_rate': 3.686868686868687e-05, 'epoch': 1.05}
{'loss': 0.0659, 'learning_rate': 3.6363636363636364e-05, 'epoch': 1.09}
{'loss': 0.0425, 'learning_rate': 3.5858585858585855e-05, 'epoch': 1.13}
{'loss': 0.2942, 'learning_rate': 3.535353535353535e-05, 'epoch': 1.17}
{'loss': 0.1108, 'learning_rate': 3.484848484848485e-05, 'epoch': 1.21}
{'loss': 0.0203, 'learning_rate': 3.434343434343435e-05, 'epoch': 1.25}
{'loss': 0.0548, 'learning_rate': 3.3838383838383844e-05, 'epoch': 1.29}
{'loss': 0.1306, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.33}
{'loss': 0.0787, 'learning_rate': 3.282828282828283e-05, 'epoch': 1.37}
{'loss': 0.0622, 'learning_rate': 3.232323232323233e-05, 'epoch': 1.41}
{'loss': 0.0187, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.45}
{'loss': 0.0583, 'learning_rate': 3.131313131313132e-05, 'epoch': 1.49}
{'loss': 0.2451, 'learning_rate': 3.080808080808081e-05, 'epoch': 1.54}
{'loss': 0.044, 'learning_rate': 3.0303030303030306e-05, 'epoch': 1.58}
{'loss': 0.1573, 'learning_rate': 2.9797979797979796e-05, 'epoch': 1.62}
{'loss': 0.0901, 'learning_rate': 2.9292929292929294e-05, 'epoch': 1.66}
{'loss': 0.0129, 'learning_rate': 2.878787878787879e-05, 'epoch': 1.7}
{'loss': 0.0019, 'learning_rate': 2.8282828282828282e-05, 'epoch': 1.74}
{'loss': 0.0872, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.78}
{'loss': 0.1267, 'learning_rate': 2.7272727272727273e-05, 'epoch': 1.82}
{'loss': 0.0675, 'learning_rate': 2.676767676767677e-05, 'epoch': 1.86}
{'loss': 0.1041, 'learning_rate': 2.6262626262626268e-05, 'epoch': 1.9}
{'loss': 0.0348, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.94}
{'loss': 0.0581, 'learning_rate': 2.5252525252525256e-05, 'epoch': 1.98}
{'loss': 0.0328, 'learning_rate': 2.474747474747475e-05, 'epoch': 2.02}
{'loss': 0.0054, 'learning_rate': 2.4242424242424244e-05, 'epoch': 2.06}
{'loss': 0.0099, 'learning_rate': 2.3737373737373738e-05, 'epoch': 2.1}
{'loss': 0.1346, 'learning_rate': 2.3232323232323232e-05, 'epoch': 2.14}
{'loss': 0.0215, 'learning_rate': 2.272727272727273e-05, 'epoch': 2.18}
{'loss': 0.028, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.22}
{'loss': 0.0062, 'learning_rate': 2.171717171717172e-05, 'epoch': 2.26}
{'loss': 0.0023, 'learning_rate': 2.1212121212121215e-05, 'epoch': 2.3}
{'loss': 0.0233, 'learning_rate': 2.070707070707071e-05, 'epoch': 2.34}
{'loss': 0.0019, 'learning_rate': 2.0202020202020203e-05, 'epoch': 2.38}
{'loss': 0.0614, 'learning_rate': 1.9696969696969697e-05, 'epoch': 2.42}
{'loss': 0.0147, 'learning_rate': 1.919191919191919e-05, 'epoch': 2.46}
{'loss': 0.0574, 'learning_rate': 1.8686868686868688e-05, 'epoch': 2.51}
{'loss': 0.0012, 'learning_rate': 1.8181818181818182e-05, 'epoch': 2.55}
{'loss': 0.0612, 'learning_rate': 1.7676767676767676e-05, 'epoch': 2.59}
{'loss': 0.0011, 'learning_rate': 1.7171717171717173e-05, 'epoch': 2.63}
{'loss': 0.033, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.67}
{'loss': 0.0156, 'learning_rate': 1.6161616161616165e-05, 'epoch': 2.71}
{'loss': 0.0225, 'learning_rate': 1.565656565656566e-05, 'epoch': 2.75}
{'loss': 0.027, 'learning_rate': 1.5151515151515153e-05, 'epoch': 2.79}
{'loss': 0.0108, 'learning_rate': 1.4646464646464647e-05, 'epoch': 2.83}
{'loss': 0.0144, 'learning_rate': 1.4141414141414141e-05, 'epoch': 2.87}
{'loss': 0.0141, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.91}
{'loss': 0.003, 'learning_rate': 1.3131313131313134e-05, 'epoch': 2.95}
{'loss': 0.0476, 'learning_rate': 1.2626262626262628e-05, 'epoch': 2.99}
{'loss': 0.0004, 'learning_rate': 1.2121212121212122e-05, 'epoch': 3.03}
{'loss': 0.0013, 'learning_rate': 1.1616161616161616e-05, 'epoch': 3.07}
{'loss': 0.0004, 'learning_rate': 1.1111111111111112e-05, 'epoch': 3.11}
{'loss': 0.0029, 'learning_rate': 1.0606060606060607e-05, 'epoch': 3.15}
{'loss': 0.0009, 'learning_rate': 1.0101010101010101e-05, 'epoch': 3.19}
{'loss': 0.0006, 'learning_rate': 9.595959595959595e-06, 'epoch': 3.23}
{'loss': 0.0004, 'learning_rate': 9.090909090909091e-06, 'epoch': 3.27}
{'loss': 0.0008, 'learning_rate': 8.585858585858587e-06, 'epoch': 3.31}
{'loss': 0.0004, 'learning_rate': 8.080808080808082e-06, 'epoch': 3.35}
{'loss': 0.0005, 'learning_rate': 7.5757575757575764e-06, 'epoch': 3.39}
{'loss': 0.0003, 'learning_rate': 7.0707070707070704e-06, 'epoch': 3.43}
{'loss': 0.0352, 'learning_rate': 6.565656565656567e-06, 'epoch': 3.47}
{'loss': 0.0508, 'learning_rate': 6.060606060606061e-06, 'epoch': 3.52}
{'loss': 0.0003, 'learning_rate': 5.555555555555556e-06, 'epoch': 3.56}
{'loss': 0.0024, 'learning_rate': 5.050505050505051e-06, 'epoch': 3.6}
{'loss': 0.0067, 'learning_rate': 4.5454545454545455e-06, 'epoch': 3.64}
{'loss': 0.0005, 'learning_rate': 4.040404040404041e-06, 'epoch': 3.68}
{'loss': 0.0009, 'learning_rate': 3.5353535353535352e-06, 'epoch': 3.72}
{'loss': 0.0105, 'learning_rate': 3.0303030303030305e-06, 'epoch': 3.76}
{'loss': 0.0005, 'learning_rate': 2.5252525252525253e-06, 'epoch': 3.8}
{'loss': 0.0143, 'learning_rate': 2.0202020202020206e-06, 'epoch': 3.84}
{'loss': 0.0008, 'learning_rate': 1.5151515151515152e-06, 'epoch': 3.88}
{'loss': 0.0005, 'learning_rate': 1.0101010101010103e-06, 'epoch': 3.92}
{'loss': 0.0004, 'learning_rate': 5.050505050505052e-07, 'epoch': 3.96}
{'loss': 0.0011, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 545.9687, 'train_samples_per_second': 5.751, 'train_steps_per_second': 0.725, 'train_loss': 0.11565865357018386, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-99/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-297/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-198/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-396/optimizer.pt
validate!
last validate 0.
INFO 11-23 21:28:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-99', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-99', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:28:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_20_50_0.8_0.3_120_4 epoch 1

------------------------------------------------

0.566

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_20_50_0.8_0.3_120_4/generated_contents/1
INFO 11-23 21:30:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-198', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-198', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:30:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_20_50_0.8_0.3_120_4 epoch 2

------------------------------------------------

0.6056666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_20_50_0.8_0.3_120_4/generated_contents/2
INFO 11-23 21:31:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-297', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-297', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:32:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_20_50_0.8_0.3_120_4 epoch 3

------------------------------------------------

0.6116666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_20_50_0.8_0.3_120_4/generated_contents/3
INFO 11-23 21:33:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-396', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-396', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:33:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_20_50_0.8_0.3_120_4 epoch 4

------------------------------------------------

0.6116666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_20_50_0.8_0.3_120_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-99
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-198
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-297
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_20_50_0.8_0.3_120_4/checkpoint-396
searching parameters: NI_task937_50_40_50_0.8_0.4_130_4
/home/cyzhao/NI_task937_exp_1/NI_task937_50_40_50_0.8_0.4_130_4
/home/cyzhao/NI_task937_exp_1/NI_task937_50_40_50_0.8_0.4_130_4/config.json
generate_and_write_inputs!
INFO 11-23 21:35:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:35:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 21:42:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 21:43:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 1477
expected_example_num: 2000
selection_ratio: 0.7385
finetune_deepseek!
{'loss': 3.7708, 'learning_rate': 4.972972972972974e-05, 'epoch': 0.02}
{'loss': 2.3662, 'learning_rate': 4.945945945945946e-05, 'epoch': 0.04}
{'loss': 0.9803, 'learning_rate': 4.9189189189189196e-05, 'epoch': 0.06}
{'loss': 0.263, 'learning_rate': 4.891891891891892e-05, 'epoch': 0.09}
{'loss': 0.2475, 'learning_rate': 4.8648648648648654e-05, 'epoch': 0.11}
{'loss': 0.382, 'learning_rate': 4.837837837837838e-05, 'epoch': 0.13}
{'loss': 0.3987, 'learning_rate': 4.810810810810811e-05, 'epoch': 0.15}
{'loss': 0.1898, 'learning_rate': 4.783783783783784e-05, 'epoch': 0.17}
{'loss': 0.1371, 'learning_rate': 4.756756756756757e-05, 'epoch': 0.19}
{'loss': 0.1202, 'learning_rate': 4.72972972972973e-05, 'epoch': 0.22}
{'loss': 0.106, 'learning_rate': 4.7027027027027035e-05, 'epoch': 0.24}
{'loss': 0.0778, 'learning_rate': 4.675675675675676e-05, 'epoch': 0.26}
{'loss': 0.18, 'learning_rate': 4.648648648648649e-05, 'epoch': 0.28}
{'loss': 0.2395, 'learning_rate': 4.6216216216216215e-05, 'epoch': 0.3}
{'loss': 0.2301, 'learning_rate': 4.594594594594595e-05, 'epoch': 0.32}
{'loss': 0.1689, 'learning_rate': 4.567567567567568e-05, 'epoch': 0.35}
{'loss': 0.8481, 'learning_rate': 4.540540540540541e-05, 'epoch': 0.37}
{'loss': 0.3015, 'learning_rate': 4.513513513513514e-05, 'epoch': 0.39}
{'loss': 0.1739, 'learning_rate': 4.486486486486487e-05, 'epoch': 0.41}
{'loss': 0.0921, 'learning_rate': 4.4594594594594596e-05, 'epoch': 0.43}
{'loss': 0.0757, 'learning_rate': 4.432432432432433e-05, 'epoch': 0.45}
{'loss': 0.0966, 'learning_rate': 4.4054054054054054e-05, 'epoch': 0.48}
{'loss': 0.2339, 'learning_rate': 4.378378378378379e-05, 'epoch': 0.5}
{'loss': 0.1131, 'learning_rate': 4.351351351351351e-05, 'epoch': 0.52}
{'loss': 0.0993, 'learning_rate': 4.324324324324325e-05, 'epoch': 0.54}
{'loss': 0.0714, 'learning_rate': 4.297297297297298e-05, 'epoch': 0.56}
{'loss': 0.12, 'learning_rate': 4.2702702702702706e-05, 'epoch': 0.58}
{'loss': 0.0589, 'learning_rate': 4.2432432432432435e-05, 'epoch': 0.61}
{'loss': 0.127, 'learning_rate': 4.2162162162162164e-05, 'epoch': 0.63}
{'loss': 0.0808, 'learning_rate': 4.189189189189189e-05, 'epoch': 0.65}
{'loss': 0.0478, 'learning_rate': 4.162162162162163e-05, 'epoch': 0.67}
{'loss': 0.0157, 'learning_rate': 4.135135135135135e-05, 'epoch': 0.69}
{'loss': 0.2915, 'learning_rate': 4.108108108108109e-05, 'epoch': 0.71}
{'loss': 0.0734, 'learning_rate': 4.081081081081081e-05, 'epoch': 0.74}
{'loss': 0.1503, 'learning_rate': 4.0540540540540545e-05, 'epoch': 0.76}
{'loss': 0.1588, 'learning_rate': 4.0270270270270274e-05, 'epoch': 0.78}
{'loss': 0.1259, 'learning_rate': 4e-05, 'epoch': 0.8}
{'loss': 0.4332, 'learning_rate': 3.972972972972973e-05, 'epoch': 0.82}
{'loss': 0.079, 'learning_rate': 3.945945945945946e-05, 'epoch': 0.84}
{'loss': 0.0969, 'learning_rate': 3.918918918918919e-05, 'epoch': 0.86}
{'loss': 0.1588, 'learning_rate': 3.8918918918918926e-05, 'epoch': 0.89}
{'loss': 0.173, 'learning_rate': 3.864864864864865e-05, 'epoch': 0.91}
{'loss': 0.0698, 'learning_rate': 3.8378378378378384e-05, 'epoch': 0.93}
{'loss': 0.0898, 'learning_rate': 3.8108108108108106e-05, 'epoch': 0.95}
{'loss': 0.0959, 'learning_rate': 3.783783783783784e-05, 'epoch': 0.97}
{'loss': 0.0634, 'learning_rate': 3.756756756756757e-05, 'epoch': 0.99}
{'loss': 0.1998, 'learning_rate': 3.72972972972973e-05, 'epoch': 1.02}
{'loss': 0.0499, 'learning_rate': 3.702702702702703e-05, 'epoch': 1.04}
{'loss': 0.0656, 'learning_rate': 3.675675675675676e-05, 'epoch': 1.06}
{'loss': 0.0357, 'learning_rate': 3.648648648648649e-05, 'epoch': 1.08}
{'loss': 0.1254, 'learning_rate': 3.621621621621622e-05, 'epoch': 1.1}
{'loss': 0.121, 'learning_rate': 3.5945945945945945e-05, 'epoch': 1.12}
{'loss': 0.0593, 'learning_rate': 3.567567567567568e-05, 'epoch': 1.15}
{'loss': 0.0337, 'learning_rate': 3.5405405405405403e-05, 'epoch': 1.17}
{'loss': 0.0419, 'learning_rate': 3.513513513513514e-05, 'epoch': 1.19}
{'loss': 0.0804, 'learning_rate': 3.486486486486487e-05, 'epoch': 1.21}
{'loss': 0.0307, 'learning_rate': 3.45945945945946e-05, 'epoch': 1.23}
{'loss': 0.0739, 'learning_rate': 3.4324324324324326e-05, 'epoch': 1.25}
{'loss': 0.0695, 'learning_rate': 3.4054054054054055e-05, 'epoch': 1.28}
{'loss': 0.1267, 'learning_rate': 3.3783783783783784e-05, 'epoch': 1.3}
{'loss': 0.0274, 'learning_rate': 3.351351351351352e-05, 'epoch': 1.32}
{'loss': 0.0145, 'learning_rate': 3.324324324324324e-05, 'epoch': 1.34}
{'loss': 0.0236, 'learning_rate': 3.297297297297298e-05, 'epoch': 1.36}
{'loss': 0.0749, 'learning_rate': 3.27027027027027e-05, 'epoch': 1.38}
{'loss': 0.0541, 'learning_rate': 3.2432432432432436e-05, 'epoch': 1.41}
{'loss': 0.0264, 'learning_rate': 3.2162162162162165e-05, 'epoch': 1.43}
{'loss': 0.0578, 'learning_rate': 3.1891891891891894e-05, 'epoch': 1.45}
{'loss': 0.0686, 'learning_rate': 3.162162162162162e-05, 'epoch': 1.47}
{'loss': 0.0482, 'learning_rate': 3.135135135135135e-05, 'epoch': 1.49}
{'loss': 0.039, 'learning_rate': 3.108108108108108e-05, 'epoch': 1.51}
{'loss': 0.0341, 'learning_rate': 3.081081081081082e-05, 'epoch': 1.54}
{'loss': 0.0628, 'learning_rate': 3.054054054054054e-05, 'epoch': 1.56}
{'loss': 0.1147, 'learning_rate': 3.0270270270270272e-05, 'epoch': 1.58}
{'loss': 0.0845, 'learning_rate': 3e-05, 'epoch': 1.6}
{'loss': 0.0863, 'learning_rate': 2.9729729729729733e-05, 'epoch': 1.62}
{'loss': 0.1063, 'learning_rate': 2.945945945945946e-05, 'epoch': 1.64}
{'loss': 0.0989, 'learning_rate': 2.918918918918919e-05, 'epoch': 1.66}
{'loss': 0.0674, 'learning_rate': 2.891891891891892e-05, 'epoch': 1.69}
{'loss': 0.1714, 'learning_rate': 2.8648648648648653e-05, 'epoch': 1.71}
{'loss': 0.0309, 'learning_rate': 2.8378378378378378e-05, 'epoch': 1.73}
{'loss': 0.0138, 'learning_rate': 2.810810810810811e-05, 'epoch': 1.75}
{'loss': 0.1414, 'learning_rate': 2.7837837837837836e-05, 'epoch': 1.77}
{'loss': 0.0496, 'learning_rate': 2.7567567567567572e-05, 'epoch': 1.79}
{'loss': 0.0531, 'learning_rate': 2.7297297297297298e-05, 'epoch': 1.82}
{'loss': 0.0851, 'learning_rate': 2.702702702702703e-05, 'epoch': 1.84}
{'loss': 0.0354, 'learning_rate': 2.6756756756756756e-05, 'epoch': 1.86}
{'loss': 0.0443, 'learning_rate': 2.6486486486486488e-05, 'epoch': 1.88}
{'loss': 0.1172, 'learning_rate': 2.6216216216216217e-05, 'epoch': 1.9}
{'loss': 0.0761, 'learning_rate': 2.594594594594595e-05, 'epoch': 1.92}
{'loss': 0.0863, 'learning_rate': 2.5675675675675675e-05, 'epoch': 1.95}
{'loss': 0.0333, 'learning_rate': 2.5405405405405408e-05, 'epoch': 1.97}
{'loss': 0.0685, 'learning_rate': 2.5135135135135133e-05, 'epoch': 1.99}
{'loss': 0.0058, 'learning_rate': 2.486486486486487e-05, 'epoch': 2.01}
{'loss': 0.0184, 'learning_rate': 2.4594594594594598e-05, 'epoch': 2.03}
{'loss': 0.0387, 'learning_rate': 2.4324324324324327e-05, 'epoch': 2.05}
{'loss': 0.0111, 'learning_rate': 2.4054054054054056e-05, 'epoch': 2.08}
{'loss': 0.0048, 'learning_rate': 2.3783783783783785e-05, 'epoch': 2.1}
{'loss': 0.0028, 'learning_rate': 2.3513513513513518e-05, 'epoch': 2.12}
{'loss': 0.0023, 'learning_rate': 2.3243243243243247e-05, 'epoch': 2.14}
{'loss': 0.0362, 'learning_rate': 2.2972972972972976e-05, 'epoch': 2.16}
{'loss': 0.0123, 'learning_rate': 2.2702702702702705e-05, 'epoch': 2.18}
{'loss': 0.0035, 'learning_rate': 2.2432432432432434e-05, 'epoch': 2.21}
{'loss': 0.0137, 'learning_rate': 2.2162162162162166e-05, 'epoch': 2.23}
{'loss': 0.0112, 'learning_rate': 2.1891891891891895e-05, 'epoch': 2.25}
{'loss': 0.0088, 'learning_rate': 2.1621621621621624e-05, 'epoch': 2.27}
{'loss': 0.079, 'learning_rate': 2.1351351351351353e-05, 'epoch': 2.29}
{'loss': 0.0057, 'learning_rate': 2.1081081081081082e-05, 'epoch': 2.31}
{'loss': 0.0226, 'learning_rate': 2.0810810810810815e-05, 'epoch': 2.34}
{'loss': 0.0206, 'learning_rate': 2.0540540540540544e-05, 'epoch': 2.36}
{'loss': 0.0028, 'learning_rate': 2.0270270270270273e-05, 'epoch': 2.38}
{'loss': 0.0014, 'learning_rate': 2e-05, 'epoch': 2.4}
{'loss': 0.007, 'learning_rate': 1.972972972972973e-05, 'epoch': 2.42}
{'loss': 0.0305, 'learning_rate': 1.9459459459459463e-05, 'epoch': 2.44}
{'loss': 0.0506, 'learning_rate': 1.9189189189189192e-05, 'epoch': 2.46}
{'loss': 0.0232, 'learning_rate': 1.891891891891892e-05, 'epoch': 2.49}
{'loss': 0.0043, 'learning_rate': 1.864864864864865e-05, 'epoch': 2.51}
{'loss': 0.0941, 'learning_rate': 1.837837837837838e-05, 'epoch': 2.53}
{'loss': 0.0006, 'learning_rate': 1.810810810810811e-05, 'epoch': 2.55}
{'loss': 0.022, 'learning_rate': 1.783783783783784e-05, 'epoch': 2.57}
{'loss': 0.0008, 'learning_rate': 1.756756756756757e-05, 'epoch': 2.59}
{'loss': 0.0437, 'learning_rate': 1.72972972972973e-05, 'epoch': 2.62}
{'loss': 0.0242, 'learning_rate': 1.7027027027027028e-05, 'epoch': 2.64}
{'loss': 0.0567, 'learning_rate': 1.675675675675676e-05, 'epoch': 2.66}
{'loss': 0.001, 'learning_rate': 1.648648648648649e-05, 'epoch': 2.68}
{'loss': 0.0009, 'learning_rate': 1.6216216216216218e-05, 'epoch': 2.7}
{'loss': 0.002, 'learning_rate': 1.5945945945945947e-05, 'epoch': 2.72}
{'loss': 0.002, 'learning_rate': 1.5675675675675676e-05, 'epoch': 2.75}
{'loss': 0.0787, 'learning_rate': 1.540540540540541e-05, 'epoch': 2.77}
{'loss': 0.005, 'learning_rate': 1.5135135135135136e-05, 'epoch': 2.79}
{'loss': 0.0003, 'learning_rate': 1.4864864864864867e-05, 'epoch': 2.81}
{'loss': 0.0481, 'learning_rate': 1.4594594594594596e-05, 'epoch': 2.83}
{'loss': 0.0015, 'learning_rate': 1.4324324324324326e-05, 'epoch': 2.85}
{'loss': 0.0003, 'learning_rate': 1.4054054054054055e-05, 'epoch': 2.88}
{'loss': 0.0287, 'learning_rate': 1.3783783783783786e-05, 'epoch': 2.9}
{'loss': 0.0049, 'learning_rate': 1.3513513513513515e-05, 'epoch': 2.92}
{'loss': 0.0206, 'learning_rate': 1.3243243243243244e-05, 'epoch': 2.94}
{'loss': 0.0743, 'learning_rate': 1.2972972972972975e-05, 'epoch': 2.96}
{'loss': 0.0067, 'learning_rate': 1.2702702702702704e-05, 'epoch': 2.98}
{'loss': 0.0118, 'learning_rate': 1.2432432432432435e-05, 'epoch': 3.01}
{'loss': 0.0018, 'learning_rate': 1.2162162162162164e-05, 'epoch': 3.03}
{'loss': 0.0057, 'learning_rate': 1.1891891891891893e-05, 'epoch': 3.05}
{'loss': 0.0011, 'learning_rate': 1.1621621621621623e-05, 'epoch': 3.07}
{'loss': 0.0007, 'learning_rate': 1.1351351351351352e-05, 'epoch': 3.09}
{'loss': 0.0008, 'learning_rate': 1.1081081081081083e-05, 'epoch': 3.11}
{'loss': 0.0064, 'learning_rate': 1.0810810810810812e-05, 'epoch': 3.14}
{'loss': 0.0038, 'learning_rate': 1.0540540540540541e-05, 'epoch': 3.16}
{'loss': 0.0008, 'learning_rate': 1.0270270270270272e-05, 'epoch': 3.18}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 3.2}
{'loss': 0.0318, 'learning_rate': 9.729729729729732e-06, 'epoch': 3.22}
{'loss': 0.0194, 'learning_rate': 9.45945945945946e-06, 'epoch': 3.24}
{'loss': 0.0004, 'learning_rate': 9.18918918918919e-06, 'epoch': 3.26}
{'loss': 0.0231, 'learning_rate': 8.91891891891892e-06, 'epoch': 3.29}
{'loss': 0.0007, 'learning_rate': 8.64864864864865e-06, 'epoch': 3.31}
{'loss': 0.0005, 'learning_rate': 8.37837837837838e-06, 'epoch': 3.33}
{'loss': 0.0011, 'learning_rate': 8.108108108108109e-06, 'epoch': 3.35}
{'loss': 0.0148, 'learning_rate': 7.837837837837838e-06, 'epoch': 3.37}
{'loss': 0.0044, 'learning_rate': 7.567567567567568e-06, 'epoch': 3.39}
{'loss': 0.0071, 'learning_rate': 7.297297297297298e-06, 'epoch': 3.42}
{'loss': 0.0012, 'learning_rate': 7.027027027027028e-06, 'epoch': 3.44}
{'loss': 0.0035, 'learning_rate': 6.7567567567567575e-06, 'epoch': 3.46}
{'loss': 0.0005, 'learning_rate': 6.486486486486487e-06, 'epoch': 3.48}
{'loss': 0.0368, 'learning_rate': 6.216216216216217e-06, 'epoch': 3.5}
{'loss': 0.0282, 'learning_rate': 5.945945945945946e-06, 'epoch': 3.52}
{'loss': 0.0009, 'learning_rate': 5.675675675675676e-06, 'epoch': 3.55}
{'loss': 0.0008, 'learning_rate': 5.405405405405406e-06, 'epoch': 3.57}
{'loss': 0.0365, 'learning_rate': 5.135135135135136e-06, 'epoch': 3.59}
{'loss': 0.024, 'learning_rate': 4.864864864864866e-06, 'epoch': 3.61}
{'loss': 0.0396, 'learning_rate': 4.594594594594595e-06, 'epoch': 3.63}
{'loss': 0.0003, 'learning_rate': 4.324324324324325e-06, 'epoch': 3.65}
{'loss': 0.0006, 'learning_rate': 4.0540540540540545e-06, 'epoch': 3.68}
{'loss': 0.0003, 'learning_rate': 3.783783783783784e-06, 'epoch': 3.7}
{'loss': 0.0007, 'learning_rate': 3.513513513513514e-06, 'epoch': 3.72}
{'loss': 0.002, 'learning_rate': 3.2432432432432437e-06, 'epoch': 3.74}
{'loss': 0.0342, 'learning_rate': 2.972972972972973e-06, 'epoch': 3.76}
{'loss': 0.0174, 'learning_rate': 2.702702702702703e-06, 'epoch': 3.78}
{'loss': 0.0006, 'learning_rate': 2.432432432432433e-06, 'epoch': 3.81}
{'loss': 0.0005, 'learning_rate': 2.1621621621621623e-06, 'epoch': 3.83}
{'loss': 0.0003, 'learning_rate': 1.891891891891892e-06, 'epoch': 3.85}
{'loss': 0.0003, 'learning_rate': 1.6216216216216219e-06, 'epoch': 3.87}
{'loss': 0.0012, 'learning_rate': 1.3513513513513515e-06, 'epoch': 3.89}
{'loss': 0.0259, 'learning_rate': 1.0810810810810812e-06, 'epoch': 3.91}
{'loss': 0.004, 'learning_rate': 8.108108108108109e-07, 'epoch': 3.94}
{'loss': 0.001, 'learning_rate': 5.405405405405406e-07, 'epoch': 3.96}
{'loss': 0.0007, 'learning_rate': 2.702702702702703e-07, 'epoch': 3.98}
{'loss': 0.003, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 817.2895, 'train_samples_per_second': 7.229, 'train_steps_per_second': 0.905, 'train_loss': 0.10280315201222973, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-740/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-370/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-185/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-555/optimizer.pt
validate!
last validate 0.
INFO 11-23 21:59:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-185', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-185', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:00:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_40_50_0.8_0.4_130_4 epoch 1

------------------------------------------------

0.5263333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_40_50_0.8_0.4_130_4/generated_contents/1
INFO 11-23 22:01:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-370', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-370', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:01:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_40_50_0.8_0.4_130_4 epoch 2

------------------------------------------------

0.6166666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_40_50_0.8_0.4_130_4/generated_contents/2
INFO 11-23 22:03:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-555', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-555', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:03:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_40_50_0.8_0.4_130_4 epoch 3

------------------------------------------------

0.616

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_40_50_0.8_0.4_130_4/generated_contents/3
INFO 11-23 22:05:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-740', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-740', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:05:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_50_40_50_0.8_0.4_130_4 epoch 4

------------------------------------------------

0.6306666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_50_40_50_0.8_0.4_130_4/generated_contents/4
rm -rf /data2/cyzhao/best_ckpt/NI_task937_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-740 /data2/cyzhao/best_ckpt/NI_task937_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-185
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-370
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_40_50_0.8_0.4_130_4/checkpoint-555
searching parameters: NI_task937_30_20_45_0.9_0.35_120_5
/home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.35_120_5
/home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.35_120_5/config.json
generate_and_write_inputs!
INFO 11-23 22:07:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:07:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 22:10:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:10:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 445
expected_example_num: 600
selection_ratio: 0.7416666666666667
finetune_deepseek!
{'loss': 1.6587, 'learning_rate': 4.928571428571429e-05, 'epoch': 0.07}
{'loss': 1.558, 'learning_rate': 4.8571428571428576e-05, 'epoch': 0.14}
{'loss': 0.47, 'learning_rate': 4.785714285714286e-05, 'epoch': 0.21}
{'loss': 0.452, 'learning_rate': 4.714285714285714e-05, 'epoch': 0.29}
{'loss': 0.5763, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.36}
{'loss': 0.1742, 'learning_rate': 4.5714285714285716e-05, 'epoch': 0.43}
{'loss': 0.1272, 'learning_rate': 4.5e-05, 'epoch': 0.5}
{'loss': 0.1036, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.57}
{'loss': 0.3217, 'learning_rate': 4.3571428571428576e-05, 'epoch': 0.64}
{'loss': 0.1845, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.71}
{'loss': 0.1448, 'learning_rate': 4.214285714285714e-05, 'epoch': 0.79}
{'loss': 0.4693, 'learning_rate': 4.1428571428571437e-05, 'epoch': 0.86}
{'loss': 0.065, 'learning_rate': 4.0714285714285717e-05, 'epoch': 0.93}
{'loss': 0.1498, 'learning_rate': 4e-05, 'epoch': 1.0}
{'loss': 0.151, 'learning_rate': 3.928571428571429e-05, 'epoch': 1.07}
{'loss': 0.145, 'learning_rate': 3.857142857142858e-05, 'epoch': 1.14}
{'loss': 0.0477, 'learning_rate': 3.785714285714286e-05, 'epoch': 1.21}
{'loss': 0.0871, 'learning_rate': 3.7142857142857143e-05, 'epoch': 1.29}
{'loss': 0.242, 'learning_rate': 3.642857142857143e-05, 'epoch': 1.36}
{'loss': 0.0638, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.43}
{'loss': 0.0607, 'learning_rate': 3.5e-05, 'epoch': 1.5}
{'loss': 0.1416, 'learning_rate': 3.428571428571429e-05, 'epoch': 1.57}
{'loss': 0.1187, 'learning_rate': 3.357142857142857e-05, 'epoch': 1.64}
{'loss': 0.0537, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.71}
{'loss': 0.0992, 'learning_rate': 3.2142857142857144e-05, 'epoch': 1.79}
{'loss': 0.1743, 'learning_rate': 3.142857142857143e-05, 'epoch': 1.86}
{'loss': 0.0942, 'learning_rate': 3.071428571428572e-05, 'epoch': 1.93}
{'loss': 0.1133, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.0611, 'learning_rate': 2.9285714285714288e-05, 'epoch': 2.07}
{'loss': 0.014, 'learning_rate': 2.857142857142857e-05, 'epoch': 2.14}
{'loss': 0.0837, 'learning_rate': 2.785714285714286e-05, 'epoch': 2.21}
{'loss': 0.0339, 'learning_rate': 2.714285714285714e-05, 'epoch': 2.29}
{'loss': 0.0247, 'learning_rate': 2.642857142857143e-05, 'epoch': 2.36}
{'loss': 0.0648, 'learning_rate': 2.5714285714285714e-05, 'epoch': 2.43}
{'loss': 0.0215, 'learning_rate': 2.5e-05, 'epoch': 2.5}
{'loss': 0.0373, 'learning_rate': 2.4285714285714288e-05, 'epoch': 2.57}
{'loss': 0.0017, 'learning_rate': 2.357142857142857e-05, 'epoch': 2.64}
{'loss': 0.0629, 'learning_rate': 2.2857142857142858e-05, 'epoch': 2.71}
{'loss': 0.0009, 'learning_rate': 2.214285714285714e-05, 'epoch': 2.79}
{'loss': 0.1671, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.86}
{'loss': 0.0196, 'learning_rate': 2.0714285714285718e-05, 'epoch': 2.93}
{'loss': 0.0227, 'learning_rate': 2e-05, 'epoch': 3.0}
{'loss': 0.0009, 'learning_rate': 1.928571428571429e-05, 'epoch': 3.07}
{'loss': 0.0073, 'learning_rate': 1.8571428571428572e-05, 'epoch': 3.14}
{'loss': 0.0154, 'learning_rate': 1.785714285714286e-05, 'epoch': 3.21}
{'loss': 0.0024, 'learning_rate': 1.7142857142857145e-05, 'epoch': 3.29}
{'loss': 0.0008, 'learning_rate': 1.642857142857143e-05, 'epoch': 3.36}
{'loss': 0.011, 'learning_rate': 1.5714285714285715e-05, 'epoch': 3.43}
{'loss': 0.0184, 'learning_rate': 1.5e-05, 'epoch': 3.5}
{'loss': 0.001, 'learning_rate': 1.4285714285714285e-05, 'epoch': 3.57}
{'loss': 0.0385, 'learning_rate': 1.357142857142857e-05, 'epoch': 3.64}
{'loss': 0.0487, 'learning_rate': 1.2857142857142857e-05, 'epoch': 3.71}
{'loss': 0.0002, 'learning_rate': 1.2142857142857144e-05, 'epoch': 3.79}
{'loss': 0.0036, 'learning_rate': 1.1428571428571429e-05, 'epoch': 3.86}
{'loss': 0.0003, 'learning_rate': 1.0714285714285714e-05, 'epoch': 3.93}
{'loss': 0.0004, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0029, 'learning_rate': 9.285714285714286e-06, 'epoch': 4.07}
{'loss': 0.0022, 'learning_rate': 8.571428571428573e-06, 'epoch': 4.14}
{'loss': 0.0002, 'learning_rate': 7.857142857142858e-06, 'epoch': 4.21}
{'loss': 0.011, 'learning_rate': 7.142857142857143e-06, 'epoch': 4.29}
{'loss': 0.0007, 'learning_rate': 6.428571428571429e-06, 'epoch': 4.36}
{'loss': 0.0398, 'learning_rate': 5.7142857142857145e-06, 'epoch': 4.43}
{'loss': 0.0003, 'learning_rate': 5e-06, 'epoch': 4.5}
{'loss': 0.0002, 'learning_rate': 4.285714285714286e-06, 'epoch': 4.57}
{'loss': 0.0023, 'learning_rate': 3.5714285714285714e-06, 'epoch': 4.64}
{'loss': 0.0008, 'learning_rate': 2.8571428571428573e-06, 'epoch': 4.71}
{'loss': 0.0017, 'learning_rate': 2.142857142857143e-06, 'epoch': 4.79}
{'loss': 0.0002, 'learning_rate': 1.4285714285714286e-06, 'epoch': 4.86}
{'loss': 0.0002, 'learning_rate': 7.142857142857143e-07, 'epoch': 4.93}
{'loss': 0.0002, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 494.966, 'train_samples_per_second': 4.495, 'train_steps_per_second': 0.566, 'train_loss': 0.1267783424636166, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-224/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-280/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-168/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-56/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-112/optimizer.pt
validate!
last validate 0.
INFO 11-23 22:19:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-56', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-56', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:20:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.35_120_5 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.35_120_5/generated_contents/1
INFO 11-23 22:21:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-112', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-112', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:21:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.35_120_5 epoch 2

------------------------------------------------

0.5263333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.35_120_5/generated_contents/2
INFO 11-23 22:23:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-168', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-168', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:23:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.35_120_5 epoch 3

------------------------------------------------

0.5296666666666666

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.35_120_5/generated_contents/3
INFO 11-23 22:24:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-224', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-224', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:24:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.35_120_5 epoch 4

------------------------------------------------

0.6113333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.35_120_5/generated_contents/4
INFO 11-23 22:26:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-280', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-280', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:26:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_30_20_45_0.9_0.35_120_5 epoch 5

------------------------------------------------

0.6103333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_30_20_45_0.9_0.35_120_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-56
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-112
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-168
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-224
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_30_20_45_0.9_0.35_120_5/checkpoint-280
searching parameters: NI_task937_10_20_45_1.0_0.3_115_5
/home/cyzhao/NI_task937_exp_1/NI_task937_10_20_45_1.0_0.3_115_5
/home/cyzhao/NI_task937_exp_1/NI_task937_10_20_45_1.0_0.3_115_5/config.json
generate_and_write_inputs!
INFO 11-23 22:28:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:28:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 22:29:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:29:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 151
expected_example_num: 200
selection_ratio: 0.755
finetune_deepseek!
{'loss': 2.6282, 'learning_rate': 4.789473684210526e-05, 'epoch': 0.21}
{'loss': 1.2407, 'learning_rate': 4.5789473684210527e-05, 'epoch': 0.42}
{'loss': 0.5895, 'learning_rate': 4.368421052631579e-05, 'epoch': 0.63}
{'loss': 0.2329, 'learning_rate': 4.157894736842106e-05, 'epoch': 0.84}
{'loss': 0.1673, 'learning_rate': 3.9473684210526316e-05, 'epoch': 1.05}
{'loss': 0.0819, 'learning_rate': 3.736842105263158e-05, 'epoch': 1.26}
{'loss': 0.197, 'learning_rate': 3.526315789473684e-05, 'epoch': 1.47}
{'loss': 0.1089, 'learning_rate': 3.3157894736842106e-05, 'epoch': 1.68}
{'loss': 0.0931, 'learning_rate': 3.105263157894737e-05, 'epoch': 1.89}
{'loss': 0.0665, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.11}
{'loss': 0.0091, 'learning_rate': 2.6842105263157896e-05, 'epoch': 2.32}
{'loss': 0.0487, 'learning_rate': 2.4736842105263158e-05, 'epoch': 2.53}
{'loss': 0.0756, 'learning_rate': 2.2631578947368423e-05, 'epoch': 2.74}
{'loss': 0.067, 'learning_rate': 2.0526315789473685e-05, 'epoch': 2.95}
{'loss': 0.0858, 'learning_rate': 1.8421052631578947e-05, 'epoch': 3.16}
{'loss': 0.0007, 'learning_rate': 1.6315789473684213e-05, 'epoch': 3.37}
{'loss': 0.0012, 'learning_rate': 1.4210526315789475e-05, 'epoch': 3.58}
{'loss': 0.0249, 'learning_rate': 1.2105263157894737e-05, 'epoch': 3.79}
{'loss': 0.0014, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0007, 'learning_rate': 7.894736842105263e-06, 'epoch': 4.21}
{'loss': 0.0197, 'learning_rate': 5.789473684210527e-06, 'epoch': 4.42}
{'loss': 0.0006, 'learning_rate': 3.6842105263157892e-06, 'epoch': 4.63}
{'loss': 0.0011, 'learning_rate': 1.5789473684210528e-06, 'epoch': 4.84}
{'train_runtime': 340.6352, 'train_samples_per_second': 2.216, 'train_steps_per_second': 0.279, 'train_loss': 0.2418052290076096, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-95/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-76/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-23 22:36:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:36:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_45_1.0_0.3_115_5 epoch 1

------------------------------------------------

0.4736666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_45_1.0_0.3_115_5/generated_contents/1
INFO 11-23 22:38:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:38:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_45_1.0_0.3_115_5 epoch 2

------------------------------------------------

0.492

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_45_1.0_0.3_115_5/generated_contents/2
INFO 11-23 22:39:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:40:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_45_1.0_0.3_115_5 epoch 3

------------------------------------------------

0.603

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_45_1.0_0.3_115_5/generated_contents/3
INFO 11-23 22:41:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-76', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-76', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:41:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_45_1.0_0.3_115_5 epoch 4

------------------------------------------------

0.61

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_45_1.0_0.3_115_5/generated_contents/4
INFO 11-23 22:43:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-95', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-95', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:43:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task937_10_20_45_1.0_0.3_115_5 epoch 5

------------------------------------------------

0.6096666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task937_exp_1/NI_task937_10_20_45_1.0_0.3_115_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-57
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-76
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task937_10_20_45_1.0_0.3_115_5/checkpoint-95
searching parameters: NI_task937_50_10_45_1.0_0.4_130_5
/home/cyzhao/NI_task937_exp_1/NI_task937_50_10_45_1.0_0.4_130_5
/home/cyzhao/NI_task937_exp_1/NI_task937_50_10_45_1.0_0.4_130_5/config.json
generate_and_write_inputs!
INFO 11-23 22:45:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:45:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 22:48:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:48:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 341
expected_example_num: 500
selection_ratio: 0.682
finetune_deepseek!
{'loss': 3.9635, 'learning_rate': 4.906976744186046e-05, 'epoch': 0.09}
{'loss': 0.9432, 'learning_rate': 4.8139534883720934e-05, 'epoch': 0.19}
{'loss': 0.5229, 'learning_rate': 4.7209302325581394e-05, 'epoch': 0.28}
{'loss': 0.4247, 'learning_rate': 4.627906976744186e-05, 'epoch': 0.37}
{'loss': 0.2513, 'learning_rate': 4.5348837209302326e-05, 'epoch': 0.47}
{'loss': 0.3202, 'learning_rate': 4.441860465116279e-05, 'epoch': 0.56}
{'loss': 0.2591, 'learning_rate': 4.348837209302326e-05, 'epoch': 0.65}
{'loss': 0.2708, 'learning_rate': 4.2558139534883724e-05, 'epoch': 0.74}
{'loss': 0.2045, 'learning_rate': 4.162790697674418e-05, 'epoch': 0.84}
{'loss': 0.2964, 'learning_rate': 4.0697674418604655e-05, 'epoch': 0.93}
{'loss': 0.1217, 'learning_rate': 3.9767441860465115e-05, 'epoch': 1.02}
{'loss': 0.0858, 'learning_rate': 3.883720930232558e-05, 'epoch': 1.12}
{'loss': 0.1003, 'learning_rate': 3.790697674418605e-05, 'epoch': 1.21}
{'loss': 0.1486, 'learning_rate': 3.697674418604651e-05, 'epoch': 1.3}
{'loss': 0.1832, 'learning_rate': 3.604651162790698e-05, 'epoch': 1.4}
{'loss': 0.1916, 'learning_rate': 3.5116279069767445e-05, 'epoch': 1.49}
{'loss': 0.0676, 'learning_rate': 3.4186046511627904e-05, 'epoch': 1.58}
{'loss': 0.2224, 'learning_rate': 3.3255813953488377e-05, 'epoch': 1.67}
{'loss': 0.0839, 'learning_rate': 3.2325581395348836e-05, 'epoch': 1.77}
{'loss': 0.1618, 'learning_rate': 3.13953488372093e-05, 'epoch': 1.86}
{'loss': 0.0276, 'learning_rate': 3.0465116279069768e-05, 'epoch': 1.95}
{'loss': 0.0533, 'learning_rate': 2.9534883720930234e-05, 'epoch': 2.05}
{'loss': 0.0288, 'learning_rate': 2.8604651162790696e-05, 'epoch': 2.14}
{'loss': 0.0133, 'learning_rate': 2.7674418604651166e-05, 'epoch': 2.23}
{'loss': 0.0761, 'learning_rate': 2.674418604651163e-05, 'epoch': 2.33}
{'loss': 0.0113, 'learning_rate': 2.5813953488372094e-05, 'epoch': 2.42}
{'loss': 0.0525, 'learning_rate': 2.488372093023256e-05, 'epoch': 2.51}
{'loss': 0.0481, 'learning_rate': 2.3953488372093026e-05, 'epoch': 2.6}
{'loss': 0.1864, 'learning_rate': 2.302325581395349e-05, 'epoch': 2.7}
{'loss': 0.0205, 'learning_rate': 2.2093023255813955e-05, 'epoch': 2.79}
{'loss': 0.022, 'learning_rate': 2.116279069767442e-05, 'epoch': 2.88}
{'loss': 0.0288, 'learning_rate': 2.0232558139534883e-05, 'epoch': 2.98}
{'loss': 0.0117, 'learning_rate': 1.930232558139535e-05, 'epoch': 3.07}
{'loss': 0.0053, 'learning_rate': 1.8372093023255815e-05, 'epoch': 3.16}
{'loss': 0.0073, 'learning_rate': 1.744186046511628e-05, 'epoch': 3.26}
{'loss': 0.0057, 'learning_rate': 1.6511627906976744e-05, 'epoch': 3.35}
{'loss': 0.0276, 'learning_rate': 1.558139534883721e-05, 'epoch': 3.44}
{'loss': 0.0433, 'learning_rate': 1.4651162790697676e-05, 'epoch': 3.53}
{'loss': 0.001, 'learning_rate': 1.372093023255814e-05, 'epoch': 3.63}
{'loss': 0.0087, 'learning_rate': 1.2790697674418606e-05, 'epoch': 3.72}
{'loss': 0.0012, 'learning_rate': 1.186046511627907e-05, 'epoch': 3.81}
{'loss': 0.0027, 'learning_rate': 1.0930232558139537e-05, 'epoch': 3.91}
{'loss': 0.0227, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.014, 'learning_rate': 9.069767441860467e-06, 'epoch': 4.09}
{'loss': 0.0245, 'learning_rate': 8.139534883720931e-06, 'epoch': 4.19}
{'loss': 0.0007, 'learning_rate': 7.209302325581396e-06, 'epoch': 4.28}
{'loss': 0.0006, 'learning_rate': 6.279069767441861e-06, 'epoch': 4.37}
{'loss': 0.0005, 'learning_rate': 5.348837209302326e-06, 'epoch': 4.47}
{'loss': 0.006, 'learning_rate': 4.418604651162791e-06, 'epoch': 4.56}
{'loss': 0.014, 'learning_rate': 3.488372093023256e-06, 'epoch': 4.65}
{'loss': 0.0031, 'learning_rate': 2.558139534883721e-06, 'epoch': 4.74}
{'loss': 0.0108, 'learning_rate': 1.627906976744186e-06, 'epoch': 4.84}
{'loss': 0.0003, 'learning_rate': 6.976744186046511e-07, 'epoch': 4.93}
{'train_runtime': 445.8401, 'train_samples_per_second': 3.824, 'train_steps_per_second': 0.482, 'train_loss': 0.17869374113276515, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_10_45_1.0_0.4_130_5/checkpoint-43/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_10_45_1.0_0.4_130_5/checkpoint-215/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_10_45_1.0_0.4_130_5/checkpoint-86/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_10_45_1.0_0.4_130_5/checkpoint-129/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task937_50_10_45_1.0_0.4_130_5/checkpoint-172/optimizer.pt
validate!
last validate 0.
INFO 11-23 22:57:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_10_45_1.0_0.4_130_5/checkpoint-43', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task937_50_10_45_1.0_0.4_130_5/checkpoint-43', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:57:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
