[2023-11-23 22:00:26,866] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
searching parameters: NI_task202_10_15_45_0.8_0.35_125_3
/home/cyzhao/NI_task202_exp_1/NI_task202_10_15_45_0.8_0.35_125_3
/home/cyzhao/NI_task202_exp_1/NI_task202_10_15_45_0.8_0.35_125_3/config.json
generate_and_write_inputs!
INFO 11-23 22:00:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:00:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-23 22:04:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:04:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 90
expected_example_num: 150
selection_ratio: 0.6
finetune_deepseek!
{'loss': 1.1465, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.6793, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4255, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1003, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2007, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.139, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1491, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.1735, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0886, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 246.1472, 'train_samples_per_second': 1.097, 'train_steps_per_second': 0.146, 'train_loss': 0.3447152301669121, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-23 22:11:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:11:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task202_10_15_45_0.8_0.35_125_3 epoch 1

------------------------------------------------

0.9786666666666667

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task202_exp_1/NI_task202_10_15_45_0.8_0.35_125_3/generated_contents/1
INFO 11-23 22:26:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:26:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task202_10_15_45_0.8_0.35_125_3 epoch 2

------------------------------------------------

0.3273333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task202_exp_1/NI_task202_10_15_45_0.8_0.35_125_3/generated_contents/2
INFO 11-23 22:29:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:29:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of NI_task202_10_15_45_0.8_0.35_125_3 epoch 3

------------------------------------------------

0.3273333333333333

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task202_exp_1/NI_task202_10_15_45_0.8_0.35_125_3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task202_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task202_10_15_45_0.8_0.35_125_3/checkpoint-36
searching parameters: NI_task202_40_20_45_0.8_0.3_115_3
/home/cyzhao/NI_task202_exp_1/NI_task202_40_20_45_0.8_0.3_115_3
/home/cyzhao/NI_task202_exp_1/NI_task202_40_20_45_0.8_0.3_115_3/config.json
generate_and_write_inputs!
INFO 11-23 22:31:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:31:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
WARNING 11-23 22:41:05 scheduler.py:146] Input prompt (4199 tokens) is too long and exceeds limit of 4096
WARNING 11-23 22:41:07 scheduler.py:146] Input prompt (4538 tokens) is too long and exceeds limit of 4096
WARNING 11-23 22:43:11 scheduler.py:146] Input prompt (4189 tokens) is too long and exceeds limit of 4096
WARNING 11-23 22:44:04 scheduler.py:146] Input prompt (4590 tokens) is too long and exceeds limit of 4096
WARNING 11-23 22:44:04 scheduler.py:146] Input prompt (4494 tokens) is too long and exceeds limit of 4096
WARNING 11-23 22:47:36 scheduler.py:146] Input prompt (4102 tokens) is too long and exceeds limit of 4096
WARNING 11-23 22:48:02 scheduler.py:146] Input prompt (4150 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-23 22:48:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-23 22:48:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
