[2023-11-28 02:42:31,703] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1659
searching parameters: task1659_20_20_40_0.6_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_20_40_0.6_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_20_40_0.6_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 02:42:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:42:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
WARNING 11-28 02:44:55 scheduler.py:146] Input prompt (4141 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:45:31 scheduler.py:146] Input prompt (4155 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:46:29 scheduler.py:146] Input prompt (4204 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:48:07 scheduler.py:146] Input prompt (4114 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:49:30 scheduler.py:146] Input prompt (4227 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:50:29 scheduler.py:146] Input prompt (4170 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:51:36 scheduler.py:146] Input prompt (4105 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:51:36 scheduler.py:146] Input prompt (4121 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:52:33 scheduler.py:146] Input prompt (4192 tokens) is too long and exceeds limit of 4096
WARNING 11-28 02:52:34 scheduler.py:146] Input prompt (4124 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-28 02:53:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:53:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 269
expected_example_num: 400
selection_ratio: 0.6725
finetune_vicuna!
{'loss': 0.6408, 'learning_rate': 4.803921568627452e-05, 'epoch': 0.12}
{'loss': 0.8238, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.2562, 'learning_rate': 4.411764705882353e-05, 'epoch': 0.35}
[2023-11-28 05:31:49,287] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1659
searching parameters: task1659_30_15_45_0.6_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_15_45_0.6_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_15_45_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-28 05:31:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
[2023-11-28 05:32:47,996] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1659
searching parameters: task1659_20_10_45_0.8_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_45_0.8_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_45_0.8_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-28 05:32:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:33:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-28 05:40:26,497] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1659
searching parameters: task1659_20_10_50_0.4_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.4_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 05:40:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:40:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 05:47:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:47:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 83
expected_example_num: 200
selection_ratio: 0.415
finetune_vicuna!
{'loss': 0.4543, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.2763, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.1656, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.1176, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.0408, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.0439, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0306, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0126, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 309.3008, 'train_samples_per_second': 0.805, 'train_steps_per_second': 0.107, 'train_loss': 0.139904147522016, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-28 05:54:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:54:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_50_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.41227150483490527

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.4_0.3_3_1/generated_contents/1
INFO 11-28 05:57:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:57:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_50_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.5568369144217827

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.4_0.3_3_1/generated_contents/2
INFO 11-28 06:01:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:01:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_50_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.5629017818079839

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.4_0.3_3_1/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-33 /data2/cyzhao/best_ckpt/NI_task1659_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.4_0.3_3_1/checkpoint-22
searching parameters: task1659_10_15_50_0.5_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.5_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 06:04:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:04:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 06:08:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:08:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 80
expected_example_num: 150
selection_ratio: 0.5333333333333333
finetune_vicuna!
{'loss': 1.1382, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.8015, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.317, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.1874, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1485, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0981, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.087, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 253.4166, 'train_samples_per_second': 0.947, 'train_steps_per_second': 0.118, 'train_loss': 0.3723832053442796, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-28 06:14:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:14:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.42222295187349107

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.5_0.35_3_1/generated_contents/1
INFO 11-28 06:18:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:18:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.547019589067394

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.5_0.35_3_1/generated_contents/2
INFO 11-28 06:22:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:22:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.5487090369552375

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.5_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.5_0.35_3_1/checkpoint-30
searching parameters: task1659_30_20_50_0.7_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.7_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.7_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 06:25:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:26:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 06:38:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:38:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 463
expected_example_num: 600
selection_ratio: 0.7716666666666666
finetune_vicuna!
{'loss': 0.4146, 'learning_rate': 4.885057471264368e-05, 'epoch': 0.07}
{'loss': 0.6134, 'learning_rate': 4.770114942528736e-05, 'epoch': 0.14}
{'loss': 0.362, 'learning_rate': 4.655172413793104e-05, 'epoch': 0.21}
{'loss': 0.3101, 'learning_rate': 4.5402298850574716e-05, 'epoch': 0.28}
{'loss': 0.1532, 'learning_rate': 4.4252873563218394e-05, 'epoch': 0.34}
{'loss': 0.2748, 'learning_rate': 4.3103448275862066e-05, 'epoch': 0.41}
{'loss': 0.1277, 'learning_rate': 4.195402298850575e-05, 'epoch': 0.48}
{'loss': 0.2217, 'learning_rate': 4.080459770114943e-05, 'epoch': 0.55}
{'loss': 0.3012, 'learning_rate': 3.965517241379311e-05, 'epoch': 0.62}
{'loss': 0.1736, 'learning_rate': 3.850574712643678e-05, 'epoch': 0.69}
{'loss': 0.169, 'learning_rate': 3.735632183908046e-05, 'epoch': 0.76}
{'loss': 0.2686, 'learning_rate': 3.620689655172414e-05, 'epoch': 0.83}
{'loss': 0.1876, 'learning_rate': 3.505747126436782e-05, 'epoch': 0.9}
{'loss': 0.207, 'learning_rate': 3.390804597701149e-05, 'epoch': 0.97}
{'loss': 0.2182, 'learning_rate': 3.275862068965517e-05, 'epoch': 1.03}
{'loss': 0.0433, 'learning_rate': 3.160919540229885e-05, 'epoch': 1.1}
{'loss': 0.067, 'learning_rate': 3.045977011494253e-05, 'epoch': 1.17}
{'loss': 0.045, 'learning_rate': 2.9310344827586206e-05, 'epoch': 1.24}
{'loss': 0.1353, 'learning_rate': 2.8160919540229884e-05, 'epoch': 1.31}
{'loss': 0.0638, 'learning_rate': 2.7011494252873566e-05, 'epoch': 1.38}
{'loss': 0.0583, 'learning_rate': 2.5862068965517244e-05, 'epoch': 1.45}
{'loss': 0.0909, 'learning_rate': 2.4712643678160922e-05, 'epoch': 1.52}
{'loss': 0.0861, 'learning_rate': 2.3563218390804597e-05, 'epoch': 1.59}
{'loss': 0.0976, 'learning_rate': 2.2413793103448276e-05, 'epoch': 1.66}
{'loss': 0.0912, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}
{'loss': 0.0995, 'learning_rate': 2.0114942528735632e-05, 'epoch': 1.79}
{'loss': 0.1214, 'learning_rate': 1.896551724137931e-05, 'epoch': 1.86}
{'loss': 0.0457, 'learning_rate': 1.781609195402299e-05, 'epoch': 1.93}
{'loss': 0.0649, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0386, 'learning_rate': 1.5517241379310346e-05, 'epoch': 2.07}
{'loss': 0.0345, 'learning_rate': 1.4367816091954022e-05, 'epoch': 2.14}
{'loss': 0.0134, 'learning_rate': 1.3218390804597702e-05, 'epoch': 2.21}
{'loss': 0.0313, 'learning_rate': 1.206896551724138e-05, 'epoch': 2.28}
{'loss': 0.0249, 'learning_rate': 1.091954022988506e-05, 'epoch': 2.34}
{'loss': 0.0262, 'learning_rate': 9.770114942528738e-06, 'epoch': 2.41}
{'loss': 0.0418, 'learning_rate': 8.620689655172414e-06, 'epoch': 2.48}
{'loss': 0.0261, 'learning_rate': 7.4712643678160925e-06, 'epoch': 2.55}
{'loss': 0.0265, 'learning_rate': 6.321839080459771e-06, 'epoch': 2.62}
{'loss': 0.0348, 'learning_rate': 5.172413793103448e-06, 'epoch': 2.69}
{'loss': 0.0218, 'learning_rate': 4.022988505747127e-06, 'epoch': 2.76}
{'loss': 0.0363, 'learning_rate': 2.8735632183908046e-06, 'epoch': 2.83}
{'loss': 0.0212, 'learning_rate': 1.724137931034483e-06, 'epoch': 2.9}
{'loss': 0.0255, 'learning_rate': 5.747126436781609e-07, 'epoch': 2.97}
{'train_runtime': 691.877, 'train_samples_per_second': 2.008, 'train_steps_per_second': 0.251, 'train_loss': 0.12715986578714575, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-174/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-116/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-58/optimizer.pt
validate!
last validate 0.
INFO 11-28 06:55:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-58', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-58', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:55:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_50_0.7_0.35_3_1 epoch 1

------------------------------------------------

0.5730587284694916

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.7_0.35_3_1/generated_contents/1
INFO 11-28 06:58:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-116', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-116', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:59:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_50_0.7_0.35_3_1 epoch 2

------------------------------------------------

0.543791940132338

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.7_0.35_3_1/generated_contents/2
INFO 11-28 07:02:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-174', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-174', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:02:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_50_0.7_0.35_3_1 epoch 3

------------------------------------------------

0.5673014578634119

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.7_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1659_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-58 /data2/cyzhao/best_ckpt/NI_task1659_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-116
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.7_0.35_3_1/checkpoint-174
searching parameters: task1659_30_20_40_0.5_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_40_0.5_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_40_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 07:05:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:05:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 07:19:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:19:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 354
expected_example_num: 600
selection_ratio: 0.59
finetune_vicuna!
{'loss': 0.5979, 'learning_rate': 4.851851851851852e-05, 'epoch': 0.09}
{'loss': 0.3512, 'learning_rate': 4.703703703703704e-05, 'epoch': 0.18}
{'loss': 0.1874, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.163, 'learning_rate': 4.4074074074074076e-05, 'epoch': 0.36}
{'loss': 0.2125, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.117, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.4096, 'learning_rate': 3.962962962962963e-05, 'epoch': 0.62}
{'loss': 0.1674, 'learning_rate': 3.814814814814815e-05, 'epoch': 0.71}
{'loss': 0.1033, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2482, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.1666, 'learning_rate': 3.3703703703703706e-05, 'epoch': 0.98}
{'loss': 0.045, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.0891, 'learning_rate': 3.074074074074074e-05, 'epoch': 1.16}
{'loss': 0.0683, 'learning_rate': 2.925925925925926e-05, 'epoch': 1.24}
{'loss': 0.1119, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.102, 'learning_rate': 2.6296296296296296e-05, 'epoch': 1.42}
{'loss': 0.0993, 'learning_rate': 2.4814814814814816e-05, 'epoch': 1.51}
{'loss': 0.0398, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.0799, 'learning_rate': 2.1851851851851852e-05, 'epoch': 1.69}
{'loss': 0.0484, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.0741, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.072, 'learning_rate': 1.740740740740741e-05, 'epoch': 1.96}
{'loss': 0.0947, 'learning_rate': 1.5925925925925926e-05, 'epoch': 2.04}
{'loss': 0.0105, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0319, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0339, 'learning_rate': 1.1481481481481482e-05, 'epoch': 2.31}
{'loss': 0.0305, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0311, 'learning_rate': 8.518518518518519e-06, 'epoch': 2.49}
{'loss': 0.0229, 'learning_rate': 7.0370370370370375e-06, 'epoch': 2.58}
{'loss': 0.0594, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0496, 'learning_rate': 4.074074074074075e-06, 'epoch': 2.76}
{'loss': 0.0313, 'learning_rate': 2.5925925925925925e-06, 'epoch': 2.84}
{'loss': 0.0316, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 567.5025, 'train_samples_per_second': 1.871, 'train_steps_per_second': 0.238, 'train_loss': 0.11881848438470452, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-135/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-90/optimizer.pt
validate!
last validate 0.
INFO 11-28 07:33:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:33:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_40_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.4031339427799071

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_40_0.5_0.35_3_1/generated_contents/1
INFO 11-28 07:38:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-90', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:38:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_40_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.5115320699042296

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_40_0.5_0.35_3_1/generated_contents/2
INFO 11-28 07:42:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-135', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-135', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:42:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_40_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.5081406075430256

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_40_0.5_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-45
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-90
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_40_0.5_0.35_3_1/checkpoint-135
searching parameters: task1659_20_10_45_0.8_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_45_0.8_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_45_0.8_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 07:45:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:46:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 07:52:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:52:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 120
expected_example_num: 200
selection_ratio: 0.6
finetune_vicuna!
{'loss': 0.9574, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.41, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.2301, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2788, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.1509, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1395, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1357, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.0535, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0638, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0528, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0418, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 282.2487, 'train_samples_per_second': 1.275, 'train_steps_per_second': 0.159, 'train_loss': 0.22399941740764512, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-28 07:59:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:00:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_45_0.8_0.35_3_1 epoch 1

------------------------------------------------

0.49074152143458455

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_45_0.8_0.35_3_1/generated_contents/1
INFO 11-28 08:03:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:03:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_45_0.8_0.35_3_1 epoch 2

------------------------------------------------

0.4127198672342503

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_45_0.8_0.35_3_1/generated_contents/2
INFO 11-28 08:07:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:07:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_45_0.8_0.35_3_1 epoch 3

------------------------------------------------

0.4442232225183096

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_45_0.8_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_45_0.8_0.35_3_1/checkpoint-45
searching parameters: task1659_10_15_50_0.4_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.4_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 08:11:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:11:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 08:15:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:15:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 82
expected_example_num: 150
selection_ratio: 0.5466666666666666
finetune_vicuna!
{'loss': 0.3673, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.1809, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.125, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.092, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.0265, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.0218, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0398, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0045, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 246.3908, 'train_samples_per_second': 0.998, 'train_steps_per_second': 0.134, 'train_loss': 0.103968189486681, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-28 08:20:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:21:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.4091272011846628

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.4_0.3_3_1/generated_contents/1
INFO 11-28 08:24:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:24:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.3880498701238838

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.4_0.3_3_1/generated_contents/2
INFO 11-28 08:28:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:28:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.38814810993258314

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.4_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.4_0.3_3_1/checkpoint-33
searching parameters: task1659_10_15_40_0.7_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_40_0.7_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_40_0.7_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 08:31:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:31:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 08:35:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:35:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 92
expected_example_num: 150
selection_ratio: 0.6133333333333333
finetune_vicuna!
{'loss': 0.907, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5119, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.3822, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.118, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.17, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1098, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0587, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0626, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0335, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 247.4993, 'train_samples_per_second': 1.115, 'train_steps_per_second': 0.145, 'train_loss': 0.26151010104351574, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-28 08:41:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:41:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_40_0.7_0.35_3_1 epoch 1

------------------------------------------------

0.3999595781261126

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_40_0.7_0.35_3_1/generated_contents/1
INFO 11-28 08:44:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:44:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_40_0.7_0.35_3_1 epoch 2

------------------------------------------------

0.46361377559542377

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_40_0.7_0.35_3_1/generated_contents/2
INFO 11-28 08:47:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:48:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_40_0.7_0.35_3_1 epoch 3

------------------------------------------------

0.485870828808814

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_40_0.7_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_40_0.7_0.35_3_1/checkpoint-36
searching parameters: task1659_10_15_50_0.6_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.6_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-28 08:51:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:51:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 08:55:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 08:55:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 92
expected_example_num: 150
selection_ratio: 0.6133333333333333
finetune_vicuna!
{'loss': 0.6531, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.2921, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.152, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0866, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0987, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.0307, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0291, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0282, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0521, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 244.836, 'train_samples_per_second': 1.127, 'train_steps_per_second': 0.147, 'train_loss': 0.1580639591233598, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-28 09:01:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:01:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.5336439200735527

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.6_0.4_3_1/generated_contents/1
INFO 11-28 09:04:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:05:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.5487201328372361

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.6_0.4_3_1/generated_contents/2
INFO 11-28 09:08:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:08:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_15_50_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.5470336054233008

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_15_50_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_15_50_0.6_0.4_3_1/checkpoint-36
searching parameters: task1659_10_20_40_0.4_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_20_40_0.4_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_10_20_40_0.4_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-28 09:12:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:12:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 09:17:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:17:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 86
expected_example_num: 200
selection_ratio: 0.43
finetune_vicuna!
{'loss': 0.4303, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.349, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.2065, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.095, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.0657, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.038, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0307, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0178, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 238.8187, 'train_samples_per_second': 1.08, 'train_steps_per_second': 0.138, 'train_loss': 0.1497938432025187, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-28 09:23:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:23:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_20_40_0.4_0.4_3_1 epoch 1

------------------------------------------------

0.4722129657867987

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_20_40_0.4_0.4_3_1/generated_contents/1
INFO 11-28 09:26:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:26:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_20_40_0.4_0.4_3_1 epoch 2

------------------------------------------------

0.5373263730648459

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_20_40_0.4_0.4_3_1/generated_contents/2
INFO 11-28 09:30:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:30:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_10_20_40_0.4_0.4_3_1 epoch 3

------------------------------------------------

0.5367090836061101

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_10_20_40_0.4_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_10_20_40_0.4_0.4_3_1/checkpoint-33
searching parameters: task1659_30_10_50_0.4_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_10_50_0.4_0.35_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_10_50_0.4_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 09:33:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:34:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 09:42:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:42:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 165
expected_example_num: 300
selection_ratio: 0.55
finetune_vicuna!
{'loss': 0.1909, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.1812, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.1133, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.0491, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.1098, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.0582, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0517, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0349, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.0265, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0053, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.0006, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.036, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0122, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0032, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0013, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 349.7451, 'train_samples_per_second': 1.415, 'train_steps_per_second': 0.18, 'train_loss': 0.05588177688390253, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-28 09:51:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:51:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_10_50_0.4_0.35_3_1 epoch 1

------------------------------------------------

0.324782584996484

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_10_50_0.4_0.35_3_1/generated_contents/1
INFO 11-28 09:56:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 09:56:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_10_50_0.4_0.35_3_1 epoch 2

------------------------------------------------

0.397845793948044

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_10_50_0.4_0.35_3_1/generated_contents/2
INFO 11-28 10:00:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 10:01:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_10_50_0.4_0.35_3_1 epoch 3

------------------------------------------------

0.41025099978144586

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_10_50_0.4_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-21
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_10_50_0.4_0.35_3_1/checkpoint-63
searching parameters: task1659_30_20_45_0.7_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_45_0.7_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_45_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 10:05:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 10:05:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 10:20:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 10:21:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 480
expected_example_num: 600
selection_ratio: 0.8
finetune_vicuna!
{'loss': 0.5161, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.07}
{'loss': 0.292, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.13}
{'loss': 0.2796, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}
{'loss': 0.1211, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.274, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.1448, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.141, 'learning_rate': 4.222222222222222e-05, 'epoch': 0.47}
{'loss': 0.1322, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.1946, 'learning_rate': 4e-05, 'epoch': 0.6}
{'loss': 0.2474, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2311, 'learning_rate': 3.777777777777778e-05, 'epoch': 0.73}
{'loss': 0.1749, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2279, 'learning_rate': 3.555555555555556e-05, 'epoch': 0.87}
{'loss': 0.1729, 'learning_rate': 3.444444444444445e-05, 'epoch': 0.93}
{'loss': 0.1172, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0536, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.091, 'learning_rate': 3.111111111111111e-05, 'epoch': 1.13}
{'loss': 0.0752, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.0768, 'learning_rate': 2.8888888888888888e-05, 'epoch': 1.27}
{'loss': 0.1206, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.103, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}
{'loss': 0.0491, 'learning_rate': 2.5555555555555554e-05, 'epoch': 1.47}
{'loss': 0.0857, 'learning_rate': 2.4444444444444445e-05, 'epoch': 1.53}
{'loss': 0.0688, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.0403, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1051, 'learning_rate': 2.111111111111111e-05, 'epoch': 1.73}
{'loss': 0.0553, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.0438, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.0317, 'learning_rate': 1.777777777777778e-05, 'epoch': 1.93}
{'loss': 0.0424, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0367, 'learning_rate': 1.5555555555555555e-05, 'epoch': 2.07}
{'loss': 0.032, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0286, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}
{'loss': 0.0259, 'learning_rate': 1.2222222222222222e-05, 'epoch': 2.27}
{'loss': 0.0164, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0344, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0214, 'learning_rate': 8.88888888888889e-06, 'epoch': 2.47}
{'loss': 0.0247, 'learning_rate': 7.777777777777777e-06, 'epoch': 2.53}
{'loss': 0.0257, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}
{'loss': 0.0287, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0279, 'learning_rate': 4.444444444444445e-06, 'epoch': 2.73}
{'loss': 0.0231, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0216, 'learning_rate': 2.2222222222222225e-06, 'epoch': 2.87}
{'loss': 0.0321, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'loss': 0.0161, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 913.7744, 'train_samples_per_second': 1.576, 'train_steps_per_second': 0.197, 'train_loss': 0.10453778699868255, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-180/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-120/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-60/optimizer.pt
validate!
last validate 0.
INFO 11-28 10:42:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-60', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-60', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 10:43:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_45_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.4782312492425571

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_45_0.7_0.3_3_1/generated_contents/1
INFO 11-28 10:46:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-120', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-120', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 10:46:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_45_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.42711763453574286

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_45_0.7_0.3_3_1/generated_contents/2
INFO 11-28 10:50:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-180', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-180', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 10:50:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_45_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.5126684783006715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_45_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-60
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-120
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_45_0.7_0.3_3_1/checkpoint-180
searching parameters: task1659_20_10_50_0.7_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.7_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 10:53:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 10:53:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
WARNING 11-28 10:57:56 scheduler.py:146] Input prompt (4140 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-28 11:01:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:01:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 123
expected_example_num: 200
selection_ratio: 0.615
finetune_vicuna!
{'loss': 1.2017, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 0.4897, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.3008, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.3956, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1513, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.1428, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.084, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.1984, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0475, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.026, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.06, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.0602, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 394.9864, 'train_samples_per_second': 0.934, 'train_steps_per_second': 0.122, 'train_loss': 0.2631560224108398, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-28 11:10:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:10:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_50_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.5295525084220534

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.7_0.3_3_1/generated_contents/1
INFO 11-28 11:13:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:14:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_50_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.557224101179466

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.7_0.3_3_1/generated_contents/2
INFO 11-28 11:17:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:17:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_10_50_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.5374304753060816

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_10_50_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_10_50_0.7_0.3_3_1/checkpoint-48
searching parameters: task1659_20_20_50_0.8_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_20_50_0.8_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_20_50_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 11:20:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:21:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 11:29:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:30:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 300
expected_example_num: 400
selection_ratio: 0.75
finetune_vicuna!
{'loss': 1.2509, 'learning_rate': 4.824561403508772e-05, 'epoch': 0.11}
{'loss': 0.3851, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.4212, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.32}
{'loss': 0.1977, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.3618, 'learning_rate': 4.12280701754386e-05, 'epoch': 0.53}
{'loss': 0.3171, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.3559, 'learning_rate': 3.771929824561404e-05, 'epoch': 0.74}
{'loss': 0.2534, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.1608, 'learning_rate': 3.421052631578947e-05, 'epoch': 0.95}
{'loss': 0.137, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.1188, 'learning_rate': 3.0701754385964913e-05, 'epoch': 1.16}
{'loss': 0.0959, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.088, 'learning_rate': 2.7192982456140354e-05, 'epoch': 1.37}
{'loss': 0.0698, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.0791, 'learning_rate': 2.368421052631579e-05, 'epoch': 1.58}
{'loss': 0.0865, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.0651, 'learning_rate': 2.0175438596491227e-05, 'epoch': 1.79}
{'loss': 0.0825, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.0742, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0488, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.035, 'learning_rate': 1.3157894736842106e-05, 'epoch': 2.21}
{'loss': 0.0326, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.0209, 'learning_rate': 9.649122807017545e-06, 'epoch': 2.42}
{'loss': 0.0197, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0141, 'learning_rate': 6.140350877192982e-06, 'epoch': 2.63}
{'loss': 0.03, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0367, 'learning_rate': 2.631578947368421e-06, 'epoch': 2.84}
{'loss': 0.0336, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 558.1214, 'train_samples_per_second': 1.613, 'train_steps_per_second': 0.204, 'train_loss': 0.17187199068435452, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-76/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-114/optimizer.pt
validate!
last validate 0.
INFO 11-28 11:43:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:43:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_20_50_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.5218244257430545

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_20_50_0.8_0.3_3_1/generated_contents/1
INFO 11-28 11:47:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-76', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-76', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:48:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_20_50_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.5465989821761544

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_20_50_0.8_0.3_3_1/generated_contents/2
INFO 11-28 11:51:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-114', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-114', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:51:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_20_20_50_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.5710108901219944

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_20_20_50_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-76
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_20_20_50_0.8_0.3_3_1/checkpoint-114
searching parameters: task1659_30_20_50_0.8_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.8_0.3_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 11:54:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 11:54:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
WARNING 11-28 11:56:45 scheduler.py:146] Input prompt (4171 tokens) is too long and exceeds limit of 4096
WARNING 11-28 11:58:02 scheduler.py:146] Input prompt (4110 tokens) is too long and exceeds limit of 4096
WARNING 11-28 12:00:36 scheduler.py:146] Input prompt (4172 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-28 12:08:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 12:08:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 431
expected_example_num: 600
selection_ratio: 0.7183333333333334
finetune_vicuna!
{'loss': 0.445, 'learning_rate': 4.876543209876544e-05, 'epoch': 0.07}
{'loss': 0.2368, 'learning_rate': 4.7530864197530866e-05, 'epoch': 0.15}
{'loss': 0.4089, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.22}
{'loss': 0.3167, 'learning_rate': 4.506172839506173e-05, 'epoch': 0.3}
{'loss': 0.2849, 'learning_rate': 4.3827160493827164e-05, 'epoch': 0.37}
{'loss': 0.2632, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.2346, 'learning_rate': 4.135802469135803e-05, 'epoch': 0.52}
{'loss': 0.3244, 'learning_rate': 4.012345679012346e-05, 'epoch': 0.59}
{'loss': 0.1932, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2463, 'learning_rate': 3.7654320987654326e-05, 'epoch': 0.74}
{'loss': 0.1701, 'learning_rate': 3.6419753086419754e-05, 'epoch': 0.81}
{'loss': 0.1444, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.1953, 'learning_rate': 3.395061728395062e-05, 'epoch': 0.96}
{'loss': 0.0966, 'learning_rate': 3.271604938271605e-05, 'epoch': 1.04}
{'loss': 0.1086, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}
{'loss': 0.0775, 'learning_rate': 3.0246913580246916e-05, 'epoch': 1.19}
{'loss': 0.0913, 'learning_rate': 2.9012345679012347e-05, 'epoch': 1.26}
{'loss': 0.0947, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0759, 'learning_rate': 2.654320987654321e-05, 'epoch': 1.41}
{'loss': 0.0597, 'learning_rate': 2.5308641975308646e-05, 'epoch': 1.48}
{'loss': 0.0534, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.56}
{'loss': 0.0741, 'learning_rate': 2.2839506172839506e-05, 'epoch': 1.63}
{'loss': 0.1149, 'learning_rate': 2.1604938271604937e-05, 'epoch': 1.7}
{'loss': 0.1233, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.0585, 'learning_rate': 1.91358024691358e-05, 'epoch': 1.85}
{'loss': 0.0735, 'learning_rate': 1.7901234567901236e-05, 'epoch': 1.93}
{'loss': 0.0618, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0562, 'learning_rate': 1.54320987654321e-05, 'epoch': 2.07}
{'loss': 0.0494, 'learning_rate': 1.419753086419753e-05, 'epoch': 2.15}
{'loss': 0.0416, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0113, 'learning_rate': 1.1728395061728396e-05, 'epoch': 2.3}
{'loss': 0.0278, 'learning_rate': 1.0493827160493827e-05, 'epoch': 2.37}
{'loss': 0.031, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.44}
{'loss': 0.018, 'learning_rate': 8.02469135802469e-06, 'epoch': 2.52}
{'loss': 0.0206, 'learning_rate': 6.790123456790123e-06, 'epoch': 2.59}
{'loss': 0.0449, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0204, 'learning_rate': 4.3209876543209875e-06, 'epoch': 2.74}
{'loss': 0.0265, 'learning_rate': 3.0864197530864196e-06, 'epoch': 2.81}
{'loss': 0.022, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.89}
{'loss': 0.0317, 'learning_rate': 6.17283950617284e-07, 'epoch': 2.96}
{'train_runtime': 726.7063, 'train_samples_per_second': 1.779, 'train_steps_per_second': 0.223, 'train_loss': 0.12464184999281977, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-54/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-162/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-108/optimizer.pt
validate!
last validate 0.
INFO 11-28 12:25:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-54', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-54', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 12:26:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_50_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.5660882943268218

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.8_0.3_3_1/generated_contents/1
INFO 11-28 12:29:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-108', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-108', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 12:30:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_50_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.5651366357532595

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.8_0.3_3_1/generated_contents/2
INFO 11-28 12:33:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-162', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-162', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 12:34:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1659_30_20_50_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.5613355949802546

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1659_exp_1/task1659_30_20_50_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-54
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-108
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1659_30_20_50_0.8_0.3_3_1/checkpoint-162
searching parameters: task1659_20_20_50_0.8_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_20_50_0.8_0.4_3_1
/home/cyzhao/NI_task1659_exp_1/task1659_20_20_50_0.8_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-28 12:37:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 12:37:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 12:46:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 12:46:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
