[2023-11-24 22:30:15,075] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task620
searching parameters: task620_15_15_40_0.9_0.3_125_3
/home/cyzhao/NI_task620_exp_2/task620_15_15_40_0.9_0.3_125_3
/home/cyzhao/NI_task620_exp_2/task620_15_15_40_0.9_0.3_125_3/config.json
generate_and_write_inputs!
INFO 11-24 22:30:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:30:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 22:35:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:35:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 125
expected_example_num: 225
selection_ratio: 0.5555555555555556
finetune_deepseek!
{'loss': 1.0012, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 0.6201, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.2414, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.3648, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1804, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.1665, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.0759, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.1811, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0816, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.0973, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0667, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.1133, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 277.4152, 'train_samples_per_second': 1.352, 'train_steps_per_second': 0.173, 'train_loss': 0.26587488191823166, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task620_15_15_40_0.9_0.3_125_3/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task620_15_15_40_0.9_0.3_125_3/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task620_15_15_40_0.9_0.3_125_3/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-24 22:42:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task620_15_15_40_0.9_0.3_125_3/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task620_15_15_40_0.9_0.3_125_3/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:42:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
