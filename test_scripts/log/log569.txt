[2023-11-28 02:36:14,722] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 02:36:41,609] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task569
searching parameters: task569_30_20_40_0.4_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.4_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.4_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 02:36:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:37:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 02:43:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:44:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 314
expected_example_num: 600
selection_ratio: 0.5233333333333333
finetune_deepseek!
{'loss': 2.2019, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.1}
{'loss': 0.8516, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}
{'loss': 0.5623, 'learning_rate': 4.5e-05, 'epoch': 0.3}
{'loss': 0.5352, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.6026, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.4042, 'learning_rate': 4e-05, 'epoch': 0.6}
{'loss': 0.4676, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.7}
{'loss': 0.4986, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3986, 'learning_rate': 3.5e-05, 'epoch': 0.9}
{'loss': 0.4166, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2501, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.1}
{'loss': 0.1959, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.2533, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.3}
{'loss': 0.1885, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}
{'loss': 0.1997, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1903, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.209, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.7}
{'loss': 0.2268, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.1535, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.9}
{'loss': 0.1527, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1282, 'learning_rate': 1.5e-05, 'epoch': 2.1}
{'loss': 0.1206, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}
{'loss': 0.1337, 'learning_rate': 1.1666666666666668e-05, 'epoch': 2.3}
{'loss': 0.1133, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0748, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.1071, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}
{'loss': 0.0976, 'learning_rate': 5e-06, 'epoch': 2.7}
{'loss': 0.0954, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0918, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.9}
{'loss': 0.0804, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 346.0707, 'train_samples_per_second': 2.722, 'train_steps_per_second': 0.347, 'train_loss': 0.3334038625160853, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-40/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-80/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-120/optimizer.pt
validate!
last validate 0.
INFO 11-28 02:51:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-40', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-40', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:52:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.4_0.3_3_8 epoch 1

------------------------------------------------

0.38280855988474316

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.4_0.3_3_8/generated_contents/1
INFO 11-28 02:53:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-80', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-80', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:53:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.4_0.3_3_8 epoch 2

------------------------------------------------

0.3727675651849032

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.4_0.3_3_8/generated_contents/2
INFO 11-28 02:54:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-120', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-120', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:55:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.4_0.3_3_8 epoch 3

------------------------------------------------

0.38664926725250676

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.4_0.3_3_8/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-120 /data2/cyzhao/best_ckpt/NI_task569_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-40
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.4_0.3_3_8/checkpoint-80
searching parameters: task569_20_10_45_0.7_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_45_0.7_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_45_0.7_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 02:56:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 02:56:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 03:00:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:01:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 136
expected_example_num: 200
selection_ratio: 0.68
finetune_deepseek!
{'loss': 2.0558, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.9421, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.8874, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.6175, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.4052, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.3365, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.3079, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.2613, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.1828, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.1576, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.1178, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.1246, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'train_runtime': 240.2175, 'train_samples_per_second': 1.698, 'train_steps_per_second': 0.212, 'train_loss': 0.508254184442408, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-28 03:06:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:06:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_45_0.7_0.3_3_8 epoch 1

------------------------------------------------

0.40753340323931014

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_45_0.7_0.3_3_8/generated_contents/1
INFO 11-28 03:08:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:08:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_45_0.7_0.3_3_8 epoch 2

------------------------------------------------

0.41158333113891427

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_45_0.7_0.3_3_8/generated_contents/2
INFO 11-28 03:09:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:09:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_45_0.7_0.3_3_8 epoch 3

------------------------------------------------

0.4104493910999258

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_45_0.7_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task569_exp_8
mv /data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-34 /data2/cyzhao/best_ckpt/NI_task569_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_45_0.7_0.3_3_8/checkpoint-51
searching parameters: task569_30_15_40_0.6_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_15_40_0.6_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_15_40_0.6_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 03:11:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:11:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 03:18:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:18:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 299
expected_example_num: 450
selection_ratio: 0.6644444444444444
finetune_deepseek!
{'loss': 1.5845, 'learning_rate': 4.824561403508772e-05, 'epoch': 0.11}
{'loss': 0.7649, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.5702, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.32}
{'loss': 0.5027, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.6025, 'learning_rate': 4.12280701754386e-05, 'epoch': 0.53}
{'loss': 0.4606, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.476, 'learning_rate': 3.771929824561404e-05, 'epoch': 0.74}
{'loss': 0.3558, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.3698, 'learning_rate': 3.421052631578947e-05, 'epoch': 0.95}
{'loss': 0.2543, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.2105, 'learning_rate': 3.0701754385964913e-05, 'epoch': 1.16}
{'loss': 0.1839, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.1813, 'learning_rate': 2.7192982456140354e-05, 'epoch': 1.37}
{'loss': 0.1891, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.1623, 'learning_rate': 2.368421052631579e-05, 'epoch': 1.58}
{'loss': 0.1429, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.1648, 'learning_rate': 2.0175438596491227e-05, 'epoch': 1.79}
{'loss': 0.1553, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.164, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0726, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0871, 'learning_rate': 1.3157894736842106e-05, 'epoch': 2.21}
{'loss': 0.0625, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.0835, 'learning_rate': 9.649122807017545e-06, 'epoch': 2.42}
{'loss': 0.0582, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0528, 'learning_rate': 6.140350877192982e-06, 'epoch': 2.63}
{'loss': 0.0683, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0879, 'learning_rate': 2.631578947368421e-06, 'epoch': 2.84}
{'loss': 0.085, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 332.63, 'train_samples_per_second': 2.697, 'train_steps_per_second': 0.343, 'train_loss': 0.28698377376585676, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-76/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-114/optimizer.pt
validate!
last validate 0.
INFO 11-28 03:26:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:26:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_15_40_0.6_0.3_3_8 epoch 1

------------------------------------------------

0.3939006184052616

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_15_40_0.6_0.3_3_8/generated_contents/1
INFO 11-28 03:27:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-76', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-76', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:28:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_15_40_0.6_0.3_3_8 epoch 2

------------------------------------------------

0.4076419511240427

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_15_40_0.6_0.3_3_8/generated_contents/2
INFO 11-28 03:29:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-114', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-114', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:29:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_15_40_0.6_0.3_3_8 epoch 3

------------------------------------------------

0.40248680351614097

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_15_40_0.6_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-76
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_15_40_0.6_0.3_3_8/checkpoint-114
searching parameters: task569_30_20_40_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 03:31:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:31:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 03:40:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:40:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 386
expected_example_num: 600
selection_ratio: 0.6433333333333333
finetune_deepseek!
{'loss': 1.6625, 'learning_rate': 4.8639455782312926e-05, 'epoch': 0.08}
{'loss': 0.9144, 'learning_rate': 4.7278911564625856e-05, 'epoch': 0.16}
{'loss': 0.7674, 'learning_rate': 4.591836734693878e-05, 'epoch': 0.24}
{'loss': 0.6318, 'learning_rate': 4.4557823129251704e-05, 'epoch': 0.33}
{'loss': 0.6309, 'learning_rate': 4.319727891156463e-05, 'epoch': 0.41}
{'loss': 0.5591, 'learning_rate': 4.183673469387756e-05, 'epoch': 0.49}
{'loss': 0.5806, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.505, 'learning_rate': 3.9115646258503405e-05, 'epoch': 0.65}
{'loss': 0.6384, 'learning_rate': 3.775510204081633e-05, 'epoch': 0.73}
{'loss': 0.5226, 'learning_rate': 3.639455782312925e-05, 'epoch': 0.82}
{'loss': 0.4532, 'learning_rate': 3.5034013605442175e-05, 'epoch': 0.9}
{'loss': 0.519, 'learning_rate': 3.36734693877551e-05, 'epoch': 0.98}
{'loss': 0.4429, 'learning_rate': 3.231292517006803e-05, 'epoch': 1.06}
{'loss': 0.2103, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1895, 'learning_rate': 2.959183673469388e-05, 'epoch': 1.22}
{'loss': 0.218, 'learning_rate': 2.8231292517006803e-05, 'epoch': 1.31}
{'loss': 0.2307, 'learning_rate': 2.687074829931973e-05, 'epoch': 1.39}
{'loss': 0.1801, 'learning_rate': 2.5510204081632654e-05, 'epoch': 1.47}
{'loss': 0.2463, 'learning_rate': 2.4149659863945578e-05, 'epoch': 1.55}
{'loss': 0.2514, 'learning_rate': 2.2789115646258505e-05, 'epoch': 1.63}
{'loss': 0.2144, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1785, 'learning_rate': 2.0068027210884355e-05, 'epoch': 1.8}
{'loss': 0.1917, 'learning_rate': 1.8707482993197282e-05, 'epoch': 1.88}
{'loss': 0.1854, 'learning_rate': 1.7346938775510206e-05, 'epoch': 1.96}
{'loss': 0.1704, 'learning_rate': 1.5986394557823133e-05, 'epoch': 2.04}
{'loss': 0.0929, 'learning_rate': 1.4625850340136055e-05, 'epoch': 2.12}
{'loss': 0.0711, 'learning_rate': 1.3265306122448982e-05, 'epoch': 2.2}
{'loss': 0.0605, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0643, 'learning_rate': 1.054421768707483e-05, 'epoch': 2.37}
{'loss': 0.097, 'learning_rate': 9.183673469387756e-06, 'epoch': 2.45}
{'loss': 0.0477, 'learning_rate': 7.823129251700681e-06, 'epoch': 2.53}
{'loss': 0.0778, 'learning_rate': 6.462585034013606e-06, 'epoch': 2.61}
{'loss': 0.0709, 'learning_rate': 5.102040816326531e-06, 'epoch': 2.69}
{'loss': 0.1015, 'learning_rate': 3.741496598639456e-06, 'epoch': 2.78}
{'loss': 0.0459, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'loss': 0.0689, 'learning_rate': 1.020408163265306e-06, 'epoch': 2.94}
{'train_runtime': 393.7427, 'train_samples_per_second': 2.941, 'train_steps_per_second': 0.373, 'train_loss': 0.33003226226689863, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-147/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-98/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-49/optimizer.pt
validate!
last validate 0.
INFO 11-28 03:49:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-49', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-49', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:49:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.8_0.3_3_8 epoch 1

------------------------------------------------

0.3971177911568122

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.3_3_8/generated_contents/1
INFO 11-28 03:50:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-98', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-98', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:51:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.8_0.3_3_8 epoch 2

------------------------------------------------

0.4124836403328947

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.3_3_8/generated_contents/2
INFO 11-28 03:52:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-147', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-147', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:52:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.8_0.3_3_8 epoch 3

------------------------------------------------

0.41016976993352894

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task569_exp_8
mv /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-98 /data2/cyzhao/best_ckpt/NI_task569_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-49
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.3_3_8/checkpoint-147
searching parameters: task569_30_10_45_0.7_0.35_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_10_45_0.7_0.35_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_10_45_0.7_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-28 03:54:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 03:54:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 04:00:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:00:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 205
expected_example_num: 300
selection_ratio: 0.6833333333333333
finetune_deepseek!
{'loss': 1.5588, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.15}
{'loss': 1.0398, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.6436, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.46}
{'loss': 0.7941, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.73, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.77}
{'loss': 0.5017, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.4802, 'learning_rate': 3.205128205128206e-05, 'epoch': 1.08}
{'loss': 0.1971, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.2916, 'learning_rate': 2.6923076923076923e-05, 'epoch': 1.38}
{'loss': 0.2273, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1923, 'learning_rate': 2.1794871794871795e-05, 'epoch': 1.69}
{'loss': 0.1657, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.2416, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.082, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.1029, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.31}
{'loss': 0.0705, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0877, 'learning_rate': 6.41025641025641e-06, 'epoch': 2.62}
{'loss': 0.1043, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'loss': 0.0947, 'learning_rate': 1.282051282051282e-06, 'epoch': 2.92}
{'train_runtime': 283.9614, 'train_samples_per_second': 2.166, 'train_steps_per_second': 0.275, 'train_loss': 0.3914203436519855, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-78/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-52/optimizer.pt
validate!
last validate 0.
INFO 11-28 04:07:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:07:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_10_45_0.7_0.35_3_8 epoch 1

------------------------------------------------

0.39042572310942414

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_10_45_0.7_0.35_3_8/generated_contents/1
INFO 11-28 04:08:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-52', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-52', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:09:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_10_45_0.7_0.35_3_8 epoch 2

------------------------------------------------

0.39945781854352147

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_10_45_0.7_0.35_3_8/generated_contents/2
INFO 11-28 04:10:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-78', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:10:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_10_45_0.7_0.35_3_8 epoch 3

------------------------------------------------

0.3986023509173485

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_10_45_0.7_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-52
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_10_45_0.7_0.35_3_8/checkpoint-78
searching parameters: task569_10_20_45_0.7_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.7_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.7_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-28 04:12:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:12:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 04:15:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:15:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 111
expected_example_num: 200
selection_ratio: 0.555
finetune_deepseek!
{'loss': 1.9146, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.29}
{'loss': 1.1415, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.6054, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.86}
{'loss': 0.3239, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2489, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}
{'loss': 0.2476, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.3349, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1285, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.1202, 'learning_rate': 7.142857142857143e-06, 'epoch': 2.57}
{'loss': 0.1233, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 223.6557, 'train_samples_per_second': 1.489, 'train_steps_per_second': 0.188, 'train_loss': 0.5002513797510237, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-28 04:20:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:20:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_20_45_0.7_0.4_3_8 epoch 1

------------------------------------------------

0.3674021533319297

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.7_0.4_3_8/generated_contents/1
INFO 11-28 04:21:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:22:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_20_45_0.7_0.4_3_8 epoch 2

------------------------------------------------

0.3857717841008178

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.7_0.4_3_8/generated_contents/2
INFO 11-28 04:23:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:23:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_20_45_0.7_0.4_3_8 epoch 3

------------------------------------------------

0.38701456356091324

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.7_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-28
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.7_0.4_3_8/checkpoint-42
searching parameters: task569_30_10_40_0.7_0.35_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_10_40_0.7_0.35_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_10_40_0.7_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-28 04:25:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:25:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 04:31:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:31:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 181
expected_example_num: 300
selection_ratio: 0.6033333333333334
finetune_deepseek!
{'loss': 1.8708, 'learning_rate': 4.710144927536232e-05, 'epoch': 0.17}
{'loss': 0.871, 'learning_rate': 4.4202898550724645e-05, 'epoch': 0.35}
{'loss': 0.6662, 'learning_rate': 4.130434782608696e-05, 'epoch': 0.52}
{'loss': 0.64, 'learning_rate': 3.8405797101449274e-05, 'epoch': 0.7}
{'loss': 0.5168, 'learning_rate': 3.5507246376811596e-05, 'epoch': 0.87}
{'loss': 0.4295, 'learning_rate': 3.260869565217392e-05, 'epoch': 1.04}
{'loss': 0.2467, 'learning_rate': 2.971014492753623e-05, 'epoch': 1.22}
{'loss': 0.1854, 'learning_rate': 2.6811594202898553e-05, 'epoch': 1.39}
{'loss': 0.2289, 'learning_rate': 2.391304347826087e-05, 'epoch': 1.57}
{'loss': 0.2071, 'learning_rate': 2.101449275362319e-05, 'epoch': 1.74}
{'loss': 0.276, 'learning_rate': 1.8115942028985507e-05, 'epoch': 1.91}
{'loss': 0.1357, 'learning_rate': 1.5217391304347828e-05, 'epoch': 2.09}
{'loss': 0.081, 'learning_rate': 1.2318840579710146e-05, 'epoch': 2.26}
{'loss': 0.092, 'learning_rate': 9.420289855072464e-06, 'epoch': 2.43}
{'loss': 0.0684, 'learning_rate': 6.521739130434783e-06, 'epoch': 2.61}
{'loss': 0.0816, 'learning_rate': 3.6231884057971017e-06, 'epoch': 2.78}
{'loss': 0.0659, 'learning_rate': 7.246376811594203e-07, 'epoch': 2.96}
{'train_runtime': 265.9989, 'train_samples_per_second': 2.041, 'train_steps_per_second': 0.259, 'train_loss': 0.3870382272786852, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-69/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-23/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-46/optimizer.pt
validate!
last validate 0.
INFO 11-28 04:37:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-23', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-23', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:37:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_10_40_0.7_0.35_3_8 epoch 1

------------------------------------------------

0.38333327092161884

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_10_40_0.7_0.35_3_8/generated_contents/1
INFO 11-28 04:39:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-46', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-46', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:39:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_10_40_0.7_0.35_3_8 epoch 2

------------------------------------------------

0.4034817126159158

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_10_40_0.7_0.35_3_8/generated_contents/2
INFO 11-28 04:40:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-69', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-69', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:41:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_10_40_0.7_0.35_3_8 epoch 3

------------------------------------------------

0.40751641098732994

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_10_40_0.7_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-23
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-46
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_10_40_0.7_0.35_3_8/checkpoint-69
searching parameters: task569_20_15_45_0.4_0.35_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_15_45_0.4_0.35_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_15_45_0.4_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-28 04:42:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:42:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 04:47:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:47:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 186
expected_example_num: 300
selection_ratio: 0.62
finetune_deepseek!
{'loss': 1.9603, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.17}
{'loss': 0.692, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.4476, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.4651, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2177, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}
{'loss': 0.339, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.3336, 'learning_rate': 3.055555555555556e-05, 'epoch': 1.17}
{'loss': 0.2422, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1361, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1524, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1134, 'learning_rate': 1.9444444444444445e-05, 'epoch': 1.83}
{'loss': 0.1767, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0927, 'learning_rate': 1.388888888888889e-05, 'epoch': 2.17}
{'loss': 0.1258, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.1058, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0542, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0454, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.83}
{'loss': 0.0513, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 271.5014, 'train_samples_per_second': 2.055, 'train_steps_per_second': 0.265, 'train_loss': 0.3195147748208708, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-72/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-28 04:53:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:54:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_15_45_0.4_0.35_3_8 epoch 1

------------------------------------------------

0.32970336570494096

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_15_45_0.4_0.35_3_8/generated_contents/1
INFO 11-28 04:56:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:56:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_15_45_0.4_0.35_3_8 epoch 2

------------------------------------------------

0.3672310345138824

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_15_45_0.4_0.35_3_8/generated_contents/2
INFO 11-28 04:57:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-72', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-72', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:57:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_15_45_0.4_0.35_3_8 epoch 3

------------------------------------------------

0.37093605882096564

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_15_45_0.4_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-48
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_15_45_0.4_0.35_3_8/checkpoint-72
searching parameters: task569_10_15_45_0.6_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_10_15_45_0.6_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_10_15_45_0.6_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 04:59:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 04:59:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 05:01:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:01:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 93
expected_example_num: 150
selection_ratio: 0.62
finetune_deepseek!
{'loss': 1.494, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.8546, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4753, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2413, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.331, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2204, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1038, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.1124, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.132, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 211.9521, 'train_samples_per_second': 1.316, 'train_steps_per_second': 0.17, 'train_loss': 0.44053992877403897, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-28 05:06:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:06:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_15_45_0.6_0.3_3_8 epoch 1

------------------------------------------------

0.3698492203988184

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_15_45_0.6_0.3_3_8/generated_contents/1
INFO 11-28 05:07:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:08:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_15_45_0.6_0.3_3_8 epoch 2

------------------------------------------------

0.3801739797973143

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_15_45_0.6_0.3_3_8/generated_contents/2
INFO 11-28 05:09:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:09:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_15_45_0.6_0.3_3_8 epoch 3

------------------------------------------------

0.3895414294415034

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_15_45_0.6_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_15_45_0.6_0.3_3_8/checkpoint-36
searching parameters: task569_10_20_45_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.8_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 05:10:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:11:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 05:13:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:14:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 131
expected_example_num: 200
selection_ratio: 0.655
finetune_deepseek!
{'loss': 2.3157, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.9785, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.6285, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.6376, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.2987, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.2704, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.2399, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.2, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.2169, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.0751, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.093, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.1045, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'train_runtime': 236.595, 'train_samples_per_second': 1.661, 'train_steps_per_second': 0.216, 'train_loss': 0.48076466134950224, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-28 05:19:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:19:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_20_45_0.8_0.3_3_8 epoch 1

------------------------------------------------

0.3932970906515918

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.8_0.3_3_8/generated_contents/1
INFO 11-28 05:21:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:21:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_20_45_0.8_0.3_3_8 epoch 2

------------------------------------------------

0.3828540542693917

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.8_0.3_3_8/generated_contents/2
INFO 11-28 05:22:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:22:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_10_20_45_0.8_0.3_3_8 epoch 3

------------------------------------------------

0.3914043893650998

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_10_20_45_0.8_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-34
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_10_20_45_0.8_0.3_3_8/checkpoint-51
searching parameters: task569_30_20_50_0.5_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.5_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.5_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-28 05:24:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:24:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 05:31:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:32:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 340
expected_example_num: 600
selection_ratio: 0.5666666666666667
finetune_deepseek!
{'loss': 1.7023, 'learning_rate': 4.8449612403100775e-05, 'epoch': 0.09}
{'loss': 0.8747, 'learning_rate': 4.6899224806201553e-05, 'epoch': 0.19}
{'loss': 0.7946, 'learning_rate': 4.5348837209302326e-05, 'epoch': 0.28}
{'loss': 0.5315, 'learning_rate': 4.3798449612403104e-05, 'epoch': 0.37}
{'loss': 0.5028, 'learning_rate': 4.2248062015503877e-05, 'epoch': 0.47}
{'loss': 0.4771, 'learning_rate': 4.0697674418604655e-05, 'epoch': 0.56}
{'loss': 0.4025, 'learning_rate': 3.914728682170543e-05, 'epoch': 0.65}
{'loss': 0.4042, 'learning_rate': 3.7596899224806207e-05, 'epoch': 0.74}
{'loss': 0.4891, 'learning_rate': 3.604651162790698e-05, 'epoch': 0.84}
{'loss': 0.4303, 'learning_rate': 3.449612403100775e-05, 'epoch': 0.93}
{'loss': 0.3034, 'learning_rate': 3.294573643410852e-05, 'epoch': 1.02}
{'loss': 0.2522, 'learning_rate': 3.13953488372093e-05, 'epoch': 1.12}
{'loss': 0.1783, 'learning_rate': 2.9844961240310077e-05, 'epoch': 1.21}
{'loss': 0.208, 'learning_rate': 2.8294573643410853e-05, 'epoch': 1.3}
{'loss': 0.1529, 'learning_rate': 2.674418604651163e-05, 'epoch': 1.4}
{'loss': 0.2533, 'learning_rate': 2.5193798449612404e-05, 'epoch': 1.49}
{'loss': 0.2105, 'learning_rate': 2.364341085271318e-05, 'epoch': 1.58}
{'loss': 0.1976, 'learning_rate': 2.2093023255813955e-05, 'epoch': 1.67}
{'loss': 0.1969, 'learning_rate': 2.054263565891473e-05, 'epoch': 1.77}
{'loss': 0.2352, 'learning_rate': 1.8992248062015506e-05, 'epoch': 1.86}
{'loss': 0.1652, 'learning_rate': 1.744186046511628e-05, 'epoch': 1.95}
{'loss': 0.168, 'learning_rate': 1.5891472868217057e-05, 'epoch': 2.05}
{'loss': 0.1164, 'learning_rate': 1.434108527131783e-05, 'epoch': 2.14}
{'loss': 0.1064, 'learning_rate': 1.2790697674418606e-05, 'epoch': 2.23}
{'loss': 0.087, 'learning_rate': 1.1240310077519382e-05, 'epoch': 2.33}
{'loss': 0.1106, 'learning_rate': 9.689922480620156e-06, 'epoch': 2.42}
{'loss': 0.0983, 'learning_rate': 8.139534883720931e-06, 'epoch': 2.51}
{'loss': 0.0959, 'learning_rate': 6.589147286821707e-06, 'epoch': 2.6}
{'loss': 0.1049, 'learning_rate': 5.0387596899224804e-06, 'epoch': 2.7}
{'loss': 0.0864, 'learning_rate': 3.488372093023256e-06, 'epoch': 2.79}
{'loss': 0.1119, 'learning_rate': 1.937984496124031e-06, 'epoch': 2.88}
{'loss': 0.0944, 'learning_rate': 3.8759689922480623e-07, 'epoch': 2.98}
{'train_runtime': 359.4909, 'train_samples_per_second': 2.837, 'train_steps_per_second': 0.359, 'train_loss': 0.3150881704433944, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-43/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-86/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-129/optimizer.pt
validate!
last validate 0.
INFO 11-28 05:40:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-43', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-43', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:40:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_50_0.5_0.4_3_8 epoch 1

------------------------------------------------

0.3674980340535967

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.5_0.4_3_8/generated_contents/1
INFO 11-28 05:42:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-86', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-86', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:42:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_50_0.5_0.4_3_8 epoch 2

------------------------------------------------

0.37225303882644395

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.5_0.4_3_8/generated_contents/2
INFO 11-28 05:43:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-129', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-129', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:43:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_50_0.5_0.4_3_8 epoch 3

------------------------------------------------

0.37591126296961214

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.5_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-43
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-86
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.5_0.4_3_8/checkpoint-129
searching parameters: task569_20_10_50_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_50_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_50_0.8_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 05:45:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:45:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 05:49:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:49:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 131
expected_example_num: 200
selection_ratio: 0.655
finetune_deepseek!
{'loss': 1.5305, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.9227, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.7244, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.5854, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.2855, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.2885, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.1943, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.2171, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.2189, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.087, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.1151, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.0908, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'train_runtime': 238.6378, 'train_samples_per_second': 1.647, 'train_steps_per_second': 0.214, 'train_loss': 0.41738613797169105, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-28 05:55:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:55:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_50_0.8_0.3_3_8 epoch 1

------------------------------------------------

0.39934364745937695

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_50_0.8_0.3_3_8/generated_contents/1
INFO 11-28 05:56:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:56:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_50_0.8_0.3_3_8 epoch 2

------------------------------------------------

0.4026831711879267

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_50_0.8_0.3_3_8/generated_contents/2
INFO 11-28 05:58:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:58:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_50_0.8_0.3_3_8 epoch 3

------------------------------------------------

0.4094935789440048

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_50_0.8_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-34
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_50_0.8_0.3_3_8/checkpoint-51
searching parameters: task569_20_10_40_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.8_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.8_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 05:59:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 05:59:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 06:03:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:04:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 128
expected_example_num: 200
selection_ratio: 0.64
finetune_deepseek!
{'loss': 1.983, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 1.3197, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.7853, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.6559, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.3061, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.2509, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2623, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.3406, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1512, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.1071, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0865, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.1055, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 236.3467, 'train_samples_per_second': 1.625, 'train_steps_per_second': 0.203, 'train_loss': 0.5295125028739373, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-28 06:09:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:09:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_40_0.8_0.3_3_8 epoch 1

------------------------------------------------

0.38801540736815304

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.8_0.3_3_8/generated_contents/1
INFO 11-28 06:10:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:10:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_40_0.8_0.3_3_8 epoch 2

------------------------------------------------

0.3992572527103858

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.8_0.3_3_8/generated_contents/2
INFO 11-28 06:12:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:12:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_40_0.8_0.3_3_8 epoch 3

------------------------------------------------

0.3932376009032526

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.8_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.8_0.3_3_8/checkpoint-48
searching parameters: task569_20_10_40_0.5_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.5_0.3_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.5_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-28 06:13:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:13:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 06:17:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:17:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 91
expected_example_num: 200
selection_ratio: 0.455
finetune_deepseek!
{'loss': 1.5489, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5092, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4312, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1683, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1819, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2087, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1138, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0776, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0957, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 211.756, 'train_samples_per_second': 1.289, 'train_steps_per_second': 0.17, 'train_loss': 0.3705899996889962, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-28 06:22:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:22:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_40_0.5_0.3_3_8 epoch 1

------------------------------------------------

0.36244072567797814

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.5_0.3_3_8/generated_contents/1
INFO 11-28 06:24:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:24:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_40_0.5_0.3_3_8 epoch 2

------------------------------------------------

0.3670770676620208

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.5_0.3_3_8/generated_contents/2
INFO 11-28 06:25:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:25:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_10_40_0.5_0.3_3_8 epoch 3

------------------------------------------------

0.37552997437362384

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_10_40_0.5_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_10_40_0.5_0.3_3_8/checkpoint-36
searching parameters: task569_20_20_50_0.8_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_20_50_0.8_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_20_20_50_0.8_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-28 06:27:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:27:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 06:33:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:33:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 271
expected_example_num: 400
selection_ratio: 0.6775
finetune_deepseek!
{'loss': 2.3145, 'learning_rate': 4.803921568627452e-05, 'epoch': 0.12}
{'loss': 0.9675, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.6871, 'learning_rate': 4.411764705882353e-05, 'epoch': 0.35}
{'loss': 0.6778, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.5475, 'learning_rate': 4.0196078431372555e-05, 'epoch': 0.59}
{'loss': 0.515, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.5633, 'learning_rate': 3.627450980392157e-05, 'epoch': 0.82}
{'loss': 0.5504, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.4128, 'learning_rate': 3.235294117647059e-05, 'epoch': 1.06}
{'loss': 0.2563, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.2144, 'learning_rate': 2.8431372549019608e-05, 'epoch': 1.29}
{'loss': 0.2141, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.2593, 'learning_rate': 2.4509803921568626e-05, 'epoch': 1.53}
{'loss': 0.228, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.168, 'learning_rate': 2.058823529411765e-05, 'epoch': 1.76}
{'loss': 0.2381, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.2114, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1014, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.1168, 'learning_rate': 1.2745098039215686e-05, 'epoch': 2.24}
{'loss': 0.0753, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.0767, 'learning_rate': 8.823529411764707e-06, 'epoch': 2.47}
{'loss': 0.0587, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.0727, 'learning_rate': 4.901960784313726e-06, 'epoch': 2.71}
{'loss': 0.0607, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'loss': 0.0737, 'learning_rate': 9.80392156862745e-07, 'epoch': 2.94}
{'train_runtime': 322.3396, 'train_samples_per_second': 2.522, 'train_steps_per_second': 0.316, 'train_loss': 0.3798810963829358, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-68/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-102/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-28 06:41:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:41:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_20_50_0.8_0.4_3_8 epoch 1

------------------------------------------------

0.40765465172674736

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_20_50_0.8_0.4_3_8/generated_contents/1
INFO 11-28 06:42:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-68', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-68', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:42:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_20_50_0.8_0.4_3_8 epoch 2

------------------------------------------------

0.40510035454358906

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_20_50_0.8_0.4_3_8/generated_contents/2
INFO 11-28 06:43:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-102', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-102', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:44:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_20_20_50_0.8_0.4_3_8 epoch 3

------------------------------------------------

0.4116373800160785

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_20_20_50_0.8_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-34
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-68
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_20_20_50_0.8_0.4_3_8/checkpoint-102
searching parameters: task569_30_20_50_0.8_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.8_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.8_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-28 06:45:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:45:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 06:55:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 06:55:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 398
expected_example_num: 600
selection_ratio: 0.6633333333333333
finetune_deepseek!
{'loss': 1.8795, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.08}
{'loss': 0.8775, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.16}
{'loss': 0.9103, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.24}
{'loss': 0.7394, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.32}
{'loss': 0.5847, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.6591, 'learning_rate': 4.2e-05, 'epoch': 0.48}
{'loss': 0.4402, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.56}
{'loss': 0.5893, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.64}
{'loss': 0.4084, 'learning_rate': 3.8e-05, 'epoch': 0.72}
{'loss': 0.5723, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.5527, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.88}
{'loss': 0.4473, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.96}
{'loss': 0.339, 'learning_rate': 3.266666666666667e-05, 'epoch': 1.04}
{'loss': 0.2071, 'learning_rate': 3.1333333333333334e-05, 'epoch': 1.12}
{'loss': 0.2667, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.2044, 'learning_rate': 2.8666666666666668e-05, 'epoch': 1.28}
{'loss': 0.2979, 'learning_rate': 2.733333333333333e-05, 'epoch': 1.36}
{'loss': 0.2204, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.44}
{'loss': 0.2524, 'learning_rate': 2.466666666666667e-05, 'epoch': 1.52}
{'loss': 0.214, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.2309, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.68}
{'loss': 0.2471, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.76}
{'loss': 0.2673, 'learning_rate': 1.9333333333333333e-05, 'epoch': 1.84}
{'loss': 0.2448, 'learning_rate': 1.8e-05, 'epoch': 1.92}
{'loss': 0.2557, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0837, 'learning_rate': 1.5333333333333334e-05, 'epoch': 2.08}
{'loss': 0.1107, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.16}
{'loss': 0.0837, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.24}
{'loss': 0.0769, 'learning_rate': 1.1333333333333334e-05, 'epoch': 2.32}
{'loss': 0.0929, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.1026, 'learning_rate': 8.666666666666668e-06, 'epoch': 2.48}
{'loss': 0.0923, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.56}
{'loss': 0.0837, 'learning_rate': 6e-06, 'epoch': 2.64}
{'loss': 0.1098, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.72}
{'loss': 0.0862, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0486, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.88}
{'loss': 0.0566, 'learning_rate': 6.666666666666667e-07, 'epoch': 2.96}
{'train_runtime': 401.3592, 'train_samples_per_second': 2.975, 'train_steps_per_second': 0.374, 'train_loss': 0.34671979983647666, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-100/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-150/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-50/optimizer.pt
validate!
last validate 0.
INFO 11-28 07:04:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-50', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-50', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:04:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_50_0.8_0.4_3_8 epoch 1

------------------------------------------------

0.39403528247838854

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.8_0.4_3_8/generated_contents/1
INFO 11-28 07:06:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-100', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-100', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:06:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_50_0.8_0.4_3_8 epoch 2

------------------------------------------------

0.39692491892409393

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.8_0.4_3_8/generated_contents/2
INFO 11-28 07:07:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-150', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-150', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:07:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_50_0.8_0.4_3_8 epoch 3

------------------------------------------------

0.41037004745571215

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_50_0.8_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-50
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-100
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_50_0.8_0.4_3_8/checkpoint-150
searching parameters: task569_20_20_50_0.8_0.4_3_8
searching parameters: task569_20_20_50_0.8_0.4_3_8
searching parameters: task569_30_20_40_0.8_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.4_3_8
/home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-28 07:09:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:09:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 07:17:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:18:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 385
expected_example_num: 600
selection_ratio: 0.6416666666666667
finetune_deepseek!
{'loss': 2.2458, 'learning_rate': 4.8639455782312926e-05, 'epoch': 0.08}
{'loss': 0.9973, 'learning_rate': 4.7278911564625856e-05, 'epoch': 0.16}
{'loss': 0.6633, 'learning_rate': 4.591836734693878e-05, 'epoch': 0.24}
{'loss': 0.7968, 'learning_rate': 4.4557823129251704e-05, 'epoch': 0.33}
{'loss': 0.576, 'learning_rate': 4.319727891156463e-05, 'epoch': 0.41}
{'loss': 0.5385, 'learning_rate': 4.183673469387756e-05, 'epoch': 0.49}
{'loss': 0.49, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.499, 'learning_rate': 3.9115646258503405e-05, 'epoch': 0.65}
{'loss': 0.4712, 'learning_rate': 3.775510204081633e-05, 'epoch': 0.73}
{'loss': 0.5358, 'learning_rate': 3.639455782312925e-05, 'epoch': 0.82}
{'loss': 0.4292, 'learning_rate': 3.5034013605442175e-05, 'epoch': 0.9}
{'loss': 0.4014, 'learning_rate': 3.36734693877551e-05, 'epoch': 0.98}
{'loss': 0.3105, 'learning_rate': 3.231292517006803e-05, 'epoch': 1.06}
{'loss': 0.2218, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1837, 'learning_rate': 2.959183673469388e-05, 'epoch': 1.22}
{'loss': 0.1988, 'learning_rate': 2.8231292517006803e-05, 'epoch': 1.31}
{'loss': 0.2133, 'learning_rate': 2.687074829931973e-05, 'epoch': 1.39}
{'loss': 0.2039, 'learning_rate': 2.5510204081632654e-05, 'epoch': 1.47}
{'loss': 0.1969, 'learning_rate': 2.4149659863945578e-05, 'epoch': 1.55}
{'loss': 0.2237, 'learning_rate': 2.2789115646258505e-05, 'epoch': 1.63}
{'loss': 0.2073, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.2487, 'learning_rate': 2.0068027210884355e-05, 'epoch': 1.8}
{'loss': 0.1857, 'learning_rate': 1.8707482993197282e-05, 'epoch': 1.88}
{'loss': 0.2056, 'learning_rate': 1.7346938775510206e-05, 'epoch': 1.96}
{'loss': 0.1739, 'learning_rate': 1.5986394557823133e-05, 'epoch': 2.04}
{'loss': 0.0586, 'learning_rate': 1.4625850340136055e-05, 'epoch': 2.12}
{'loss': 0.0828, 'learning_rate': 1.3265306122448982e-05, 'epoch': 2.2}
{'loss': 0.0735, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0909, 'learning_rate': 1.054421768707483e-05, 'epoch': 2.37}
{'loss': 0.0657, 'learning_rate': 9.183673469387756e-06, 'epoch': 2.45}
{'loss': 0.0875, 'learning_rate': 7.823129251700681e-06, 'epoch': 2.53}
{'loss': 0.0831, 'learning_rate': 6.462585034013606e-06, 'epoch': 2.61}
{'loss': 0.0416, 'learning_rate': 5.102040816326531e-06, 'epoch': 2.69}
{'loss': 0.0657, 'learning_rate': 3.741496598639456e-06, 'epoch': 2.78}
{'loss': 0.0831, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'loss': 0.0588, 'learning_rate': 1.020408163265306e-06, 'epoch': 2.94}
{'train_runtime': 390.5094, 'train_samples_per_second': 2.958, 'train_steps_per_second': 0.376, 'train_loss': 0.3334539026832905, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-147/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-98/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-49/optimizer.pt
validate!
last validate 0.
INFO 11-28 07:26:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-49', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-49', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:27:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.8_0.4_3_8 epoch 1

------------------------------------------------

0.42973784001780285

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.4_3_8/generated_contents/1
INFO 11-28 07:28:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-98', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-98', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:28:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.8_0.4_3_8 epoch 2

------------------------------------------------

0.41447108969675256

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.4_3_8/generated_contents/2
INFO 11-28 07:29:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-147', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-147', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:30:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task569_30_20_40_0.8_0.4_3_8 epoch 3

------------------------------------------------

0.4105050808513813

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/task569_30_20_40_0.8_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task569_exp_8
mv /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-49 /data2/cyzhao/best_ckpt/NI_task569_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-98
rm -rf /data2/cyzhao/ckpt_data_p2ms/task569_30_20_40_0.8_0.4_3_8/checkpoint-147
searching parameters: task569_30_20_40_0.8_0.4_3_8
{'generation_epochs': 30, 'generation_batch_size': 20, 'generation_top_k': 40, 'generation_temperature': 0.8, 'min_frequency': 0.4, 'training_epochs': 3}
test best ckpt.
INFO 11-28 07:31:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task569_exp_8', tokenizer='/data2/cyzhao/best_ckpt/NI_task569_exp_8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 07:31:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task569_exp_8

------------------------------------------------

0.4340849732251144

------------------------------------------------


The best ckpt on test set gain 0.4340849732251144
Genrated contents are stored in /home/cyzhao/NI_task569_exp_8/best_ckpt_generated_content
