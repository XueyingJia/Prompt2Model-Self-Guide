[2023-11-26 06:12:25,415] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_45_0.8_0.3_60_3
/home/cyzhao/NI_task121_exp_5/task121_30_20_45_0.8_0.3_60_3
/home/cyzhao/NI_task121_exp_5/task121_30_20_45_0.8_0.3_60_3/config.json
generate_and_write_inputs!
INFO 11-26 06:12:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:12:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:16:05,411] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_15_40_0.6_0.3_50_3
/home/cyzhao/NI_task121_exp_5/task121_30_15_40_0.6_0.3_50_3
/home/cyzhao/NI_task121_exp_5/task121_30_15_40_0.6_0.3_50_3/config.json
generate_and_write_inputs!
INFO 11-26 06:16:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:16:26 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:20:28,158] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_50_0.6_0.3_55_3_0.7
/home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.6_0.3_55_3_0.7
/home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.6_0.3_55_3_0.7/config.json
generate_and_write_inputs!
INFO 11-26 06:20:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:20:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:25:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:25:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 166
expected_example_num: 600
selection_ratio: 0.27666666666666667
finetune_vicuna!
{'loss': 1.1807, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.5153, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.3956, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.3423, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.3083, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.2232, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1668, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1701, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.138, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1157, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.123, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0924, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0872, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0565, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0755, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 238.9828, 'train_samples_per_second': 2.084, 'train_steps_per_second': 0.264, 'train_loss': 0.2573224145268637, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:31:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:31:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_20_50_0.6_0.3_55_3_0.7 epoch 1

------------------------------------------------

0.5412367011766779

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.6_0.3_55_3_0.7/generated_contents/1
[2023-11-26 06:39:23,329] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_15_45_0.6_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/config.json
generate_and_write_inputs!
INFO 11-26 06:39:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:39:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:42:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:42:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 133
expected_example_num: 300
selection_ratio: 0.44333333333333336
finetune_vicuna!
{'loss': 0.4819, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.9172, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.452, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.3137, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.2534, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.1328, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.1518, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.0983, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.1338, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.0547, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.0739, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.0587, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'train_runtime': 230.2076, 'train_samples_per_second': 1.733, 'train_steps_per_second': 0.222, 'train_loss': 0.24762306932140798, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:47:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.35_60_3_0.8 epoch 1

------------------------------------------------

0.5361496212632482

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/generated_contents/1
INFO 11-26 06:48:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.35_60_3_0.8 epoch 2

------------------------------------------------

0.5444804006647321

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/generated_contents/2
INFO 11-26 06:48:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:49:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.35_60_3_0.8 epoch 3

------------------------------------------------

0.5975153386368955

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34
searching parameters: task121_10_15_45_0.6_0.4_50_3_0.6
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/config.json
generate_and_write_inputs!
INFO 11-26 06:49:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:49:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:50:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:51:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 44
expected_example_num: 150
selection_ratio: 0.29333333333333333
finetune_vicuna!
{'loss': 0.7321, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.3271, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2423, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0973, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 179.5455, 'train_samples_per_second': 0.735, 'train_steps_per_second': 0.1, 'train_loss': 0.31604865938425064, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:54:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:55:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.6_0.4_50_3_0.6 epoch 1

------------------------------------------------

0.5202349355340938

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/generated_contents/1
INFO 11-26 06:55:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:55:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.6_0.4_50_3_0.6 epoch 2

------------------------------------------------

0.6025151025676053

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/generated_contents/2
INFO 11-26 06:55:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.6_0.4_50_3_0.6 epoch 3

------------------------------------------------

0.5631780933725963

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_5
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18
searching parameters: task121_10_15_45_0.7_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.7_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.7_0.35_60_3_0.8/config.json
generate_and_write_inputs!
INFO 11-26 06:56:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
[2023-11-26 07:04:53,354] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_15_40_0.5_0.3_50_3_0.4_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.4_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.4_5/config.json
generate_and_write_inputs!
INFO 11-26 07:04:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:05:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:06:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:07:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 43
expected_example_num: 300
selection_ratio: 0.14333333333333334
finetune_vicuna!
{'loss': 1.228, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.281, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1368, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0875, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 191.6928, 'train_samples_per_second': 0.673, 'train_steps_per_second': 0.094, 'train_loss': 0.39270154552327263, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:11:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:11:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.5_0.3_50_3_0.4_5 epoch 1

------------------------------------------------

0.4298509688091004

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.4_5/generated_contents/1
INFO 11-26 07:11:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:12:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.5_0.3_50_3_0.4_5 epoch 2

------------------------------------------------

0.567320436906992

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.4_5/generated_contents/2
INFO 11-26 07:12:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:12:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.5_0.3_50_3_0.4_5 epoch 3

------------------------------------------------

0.5450899302960766

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.4_5/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.4_5/checkpoint-18
searching parameters: task121_20_10_45_0.7_0.3_50_3_0.5_5
/home/cyzhao/NI_task121_exp_5/task121_20_10_45_0.7_0.3_50_3_0.5_5
/home/cyzhao/NI_task121_exp_5/task121_20_10_45_0.7_0.3_50_3_0.5_5/config.json
generate_and_write_inputs!
INFO 11-26 07:13:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:13:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:15:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:15:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 38
expected_example_num: 200
selection_ratio: 0.19
finetune_vicuna!
{'loss': 0.7783, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3289, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1832, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 185.0064, 'train_samples_per_second': 0.616, 'train_steps_per_second': 0.081, 'train_loss': 0.3647147158781687, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:19:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:19:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.7_0.3_50_3_0.5_5 epoch 1

------------------------------------------------

0.5233019592605946

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_10_45_0.7_0.3_50_3_0.5_5/generated_contents/1
INFO 11-26 07:19:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:19:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.7_0.3_50_3_0.5_5 epoch 2

------------------------------------------------

0.5831990747342466

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_10_45_0.7_0.3_50_3_0.5_5/generated_contents/2
INFO 11-26 07:20:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:20:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.7_0.3_50_3_0.5_5 epoch 3

------------------------------------------------

0.5913036073680117

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_10_45_0.7_0.3_50_3_0.5_5/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_5
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-15 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.3_50_3_0.5_5/checkpoint-10
searching parameters: task121_20_20_40_0.4_0.35_50_3_0.6_5
/home/cyzhao/NI_task121_exp_5/task121_20_20_40_0.4_0.35_50_3_0.6_5
/home/cyzhao/NI_task121_exp_5/task121_20_20_40_0.4_0.35_50_3_0.6_5/config.json
generate_and_write_inputs!
INFO 11-26 07:20:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:20:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:23:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:23:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 89
expected_example_num: 400
selection_ratio: 0.2225
finetune_vicuna!
{'loss': 0.5736, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.3571, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4496, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1417, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.094, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1292, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0593, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0481, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0634, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 211.4256, 'train_samples_per_second': 1.263, 'train_steps_per_second': 0.17, 'train_loss': 0.21289248309201664, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:28:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:28:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.4_0.35_50_3_0.6_5 epoch 1

------------------------------------------------

0.6129033342280519

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_20_40_0.4_0.35_50_3_0.6_5/generated_contents/1
INFO 11-26 07:28:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:29:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.4_0.35_50_3_0.6_5 epoch 2

------------------------------------------------

0.6256277753940431

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_20_40_0.4_0.35_50_3_0.6_5/generated_contents/2
INFO 11-26 07:29:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:29:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.4_0.35_50_3_0.6_5 epoch 3

------------------------------------------------

0.6247353649905121

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_20_40_0.4_0.35_50_3_0.6_5/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_5
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-24 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.35_50_3_0.6_5/checkpoint-36
searching parameters: task121_20_15_40_0.4_0.35_50_3_0.3_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.4_0.35_50_3_0.3_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.4_0.35_50_3_0.3_5/config.json
generate_and_write_inputs!
INFO 11-26 07:29:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:30:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:31:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:31:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 28
expected_example_num: 300
selection_ratio: 0.09333333333333334
finetune_vicuna!
{'loss': 0.7241, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1901, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0843, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 178.6315, 'train_samples_per_second': 0.47, 'train_steps_per_second': 0.067, 'train_loss': 0.33283840616544086, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:35:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:36:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.4_0.35_50_3_0.3_5 epoch 1

------------------------------------------------

0.5783558175345133

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.4_0.35_50_3_0.3_5/generated_contents/1
INFO 11-26 07:36:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:36:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.4_0.35_50_3_0.3_5 epoch 2

------------------------------------------------

0.580779550234017

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.4_0.35_50_3_0.3_5/generated_contents/2
INFO 11-26 07:36:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:36:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.4_0.35_50_3_0.3_5 epoch 3

------------------------------------------------

0.5824975033072193

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.4_0.35_50_3_0.3_5/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.4_0.35_50_3_0.3_5/checkpoint-12
searching parameters: task121_20_15_40_0.5_0.3_50_3_0.7_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.7_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.7_5/config.json
generate_and_write_inputs!
INFO 11-26 07:37:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:37:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:38:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:39:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 75
expected_example_num: 300
selection_ratio: 0.25
finetune_vicuna!
{'loss': 1.5798, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.9654, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.4882, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.2547, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.202, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1182, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.1134, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 201.5149, 'train_samples_per_second': 1.117, 'train_steps_per_second': 0.149, 'train_loss': 0.5018292168776194, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:43:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:43:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.5_0.3_50_3_0.7_5 epoch 1

------------------------------------------------

0.46177026942412064

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.7_5/generated_contents/1
INFO 11-26 07:43:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:44:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.5_0.3_50_3_0.7_5 epoch 2

------------------------------------------------

0.511067049758277

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.7_5/generated_contents/2
INFO 11-26 07:44:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:44:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.5_0.3_50_3_0.7_5 epoch 3

------------------------------------------------

0.5285845459699726

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.7_5/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.5_0.3_50_3_0.7_5/checkpoint-30
searching parameters: task121_30_10_50_0.4_0.35_60_3_0.3_5
/home/cyzhao/NI_task121_exp_5/task121_30_10_50_0.4_0.35_60_3_0.3_5
/home/cyzhao/NI_task121_exp_5/task121_30_10_50_0.4_0.35_60_3_0.3_5/config.json
generate_and_write_inputs!
INFO 11-26 07:45:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:45:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 07:57:27,022] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_15_50_0.8_0.35_80_120_45_75_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_15_50_0.8_0.35_80_120_45_75_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_15_50_0.8_0.35_80_120_45_75_3_5/config.json
generate_and_write_inputs!
[2023-11-26 07:59:43,139] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_50_0.7_0.3_80_120_40_75_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.7_0.3_80_120_40_75_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.7_0.3_80_120_40_75_3_5/config.json
generate_and_write_inputs!
INFO 11-26 07:59:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:00:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:04:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:04:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 275
expected_example_num: 600
selection_ratio: 0.4583333333333333
finetune_vicuna!
{'loss': 0.6547, 'learning_rate': 4.80952380952381e-05, 'epoch': 0.11}
{'loss': 0.3479, 'learning_rate': 4.6190476190476194e-05, 'epoch': 0.23}
{'loss': 0.2528, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.34}
[2023-11-26 08:11:18,639] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_40_0.6_0.3_90_120_50_80_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_20_40_0.6_0.3_90_120_50_80_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/config.json
generate_and_write_inputs!
INFO 11-26 08:11:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:11:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:15:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:15:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 75
expected_example_num: 600
selection_ratio: 0.125
finetune_vicuna!
{'loss': 1.7163, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.7171, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3132, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.22, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1361, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0839, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.1023, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 203.1129, 'train_samples_per_second': 1.108, 'train_steps_per_second': 0.148, 'train_loss': 0.4483506053686142, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:19:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:20:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.3_90_120_50_80_3_5 epoch 1

------------------------------------------------

0.42755570206880256

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/generated_contents/1
INFO 11-26 08:20:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:20:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.3_90_120_50_80_3_5 epoch 2

------------------------------------------------

0.4721971559123759

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/generated_contents/2
INFO 11-26 08:21:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:21:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.3_90_120_50_80_3_5 epoch 3

------------------------------------------------

0.503825783644112

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-30 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.3_90_120_50_80_3_5/checkpoint-20
searching parameters: task121_30_10_50_0.8_0.4_90_140_45_80_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_10_50_0.8_0.4_90_140_45_80_3_5
/home/cyzhao/NI_task121_exp_5/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/config.json
generate_and_write_inputs!
INFO 11-26 08:21:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:22:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:24:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:24:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 94
expected_example_num: 300
selection_ratio: 0.31333333333333335
finetune_vicuna!
{'loss': 1.1545, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5314, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.3649, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1837, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1789, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1142, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1037, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0739, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0754, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 202.8921, 'train_samples_per_second': 1.39, 'train_steps_per_second': 0.177, 'train_loss': 0.3089484829041693, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:28:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:29:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.8_0.4_90_140_45_80_3_5 epoch 1

------------------------------------------------

0.3663966962435896

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/generated_contents/1
INFO 11-26 08:29:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.8_0.4_90_140_45_80_3_5/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:29:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
