[2023-11-26 06:12:25,415] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_45_0.8_0.3_60_3
/home/cyzhao/NI_task121_exp_5/task121_30_20_45_0.8_0.3_60_3
/home/cyzhao/NI_task121_exp_5/task121_30_20_45_0.8_0.3_60_3/config.json
generate_and_write_inputs!
INFO 11-26 06:12:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:12:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:16:05,411] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_15_40_0.6_0.3_50_3
/home/cyzhao/NI_task121_exp_5/task121_30_15_40_0.6_0.3_50_3
/home/cyzhao/NI_task121_exp_5/task121_30_15_40_0.6_0.3_50_3/config.json
generate_and_write_inputs!
INFO 11-26 06:16:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:16:26 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:20:28,158] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_50_0.6_0.3_55_3_0.7
/home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.6_0.3_55_3_0.7
/home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.6_0.3_55_3_0.7/config.json
generate_and_write_inputs!
INFO 11-26 06:20:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:20:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:25:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:25:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 166
expected_example_num: 600
selection_ratio: 0.27666666666666667
finetune_vicuna!
{'loss': 1.1807, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.5153, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.3956, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.3423, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.3083, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.2232, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1668, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1701, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.138, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1157, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.123, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0924, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0872, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0565, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0755, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 238.9828, 'train_samples_per_second': 2.084, 'train_steps_per_second': 0.264, 'train_loss': 0.2573224145268637, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:31:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.3_55_3_0.7/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:31:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_20_50_0.6_0.3_55_3_0.7 epoch 1

------------------------------------------------

0.5412367011766779

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_30_20_50_0.6_0.3_55_3_0.7/generated_contents/1
[2023-11-26 06:39:23,329] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_15_45_0.6_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/config.json
generate_and_write_inputs!
INFO 11-26 06:39:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:39:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:42:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:42:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 133
expected_example_num: 300
selection_ratio: 0.44333333333333336
finetune_vicuna!
{'loss': 0.4819, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.9172, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.452, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.3137, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.2534, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.1328, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.1518, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.0983, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.1338, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.0547, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.0739, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.0587, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'train_runtime': 230.2076, 'train_samples_per_second': 1.733, 'train_steps_per_second': 0.222, 'train_loss': 0.24762306932140798, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:47:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.35_60_3_0.8 epoch 1

------------------------------------------------

0.5361496212632482

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/generated_contents/1
INFO 11-26 06:48:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.35_60_3_0.8 epoch 2

------------------------------------------------

0.5444804006647321

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/generated_contents/2
INFO 11-26 06:48:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:49:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.35_60_3_0.8 epoch 3

------------------------------------------------

0.5975153386368955

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_20_15_45_0.6_0.35_60_3_0.8/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-51 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.35_60_3_0.8/checkpoint-34
searching parameters: task121_10_15_45_0.6_0.4_50_3_0.6
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/config.json
generate_and_write_inputs!
INFO 11-26 06:49:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:49:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:50:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:51:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 44
expected_example_num: 150
selection_ratio: 0.29333333333333333
finetune_vicuna!
{'loss': 0.7321, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.3271, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2423, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0973, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 179.5455, 'train_samples_per_second': 0.735, 'train_steps_per_second': 0.1, 'train_loss': 0.31604865938425064, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:54:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:55:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.6_0.4_50_3_0.6 epoch 1

------------------------------------------------

0.5202349355340938

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/generated_contents/1
INFO 11-26 06:55:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:55:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.6_0.4_50_3_0.6 epoch 2

------------------------------------------------

0.6025151025676053

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/generated_contents/2
INFO 11-26 06:55:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.6_0.4_50_3_0.6 epoch 3

------------------------------------------------

0.5631780933725963

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.6_0.4_50_3_0.6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_5
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.6_0.4_50_3_0.6/checkpoint-18
searching parameters: task121_10_15_45_0.7_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.7_0.35_60_3_0.8
/home/cyzhao/NI_task121_exp_5/task121_10_15_45_0.7_0.35_60_3_0.8/config.json
generate_and_write_inputs!
INFO 11-26 06:56:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
[2023-11-26 07:04:53,354] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_15_40_0.5_0.3_50_3_0.4_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.4_5
/home/cyzhao/NI_task121_exp_5/task121_20_15_40_0.5_0.3_50_3_0.4_5/config.json
generate_and_write_inputs!
INFO 11-26 07:04:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:05:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
