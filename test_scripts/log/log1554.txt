[2023-11-24 06:05:39,678] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NI_task1554
searching parameters: NI_task1554_30_15_40_0.8_0.35_125_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_30_15_40_0.8_0.35_125_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_30_15_40_0.8_0.35_125_5/config.json
generate_and_write_inputs!
INFO 11-24 06:05:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:06:01 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 06:14:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:14:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 382
expected_example_num: 450
selection_ratio: 0.8488888888888889
finetune_deepseek!
{'loss': 1.5695, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.08}
{'loss': 0.3893, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.17}
{'loss': 0.5397, 'learning_rate': 4.75e-05, 'epoch': 0.25}
{'loss': 0.5225, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.33}
{'loss': 0.3479, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.42}
{'loss': 0.4352, 'learning_rate': 4.5e-05, 'epoch': 0.5}
{'loss': 0.3167, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.58}
{'loss': 0.3166, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.67}
{'loss': 0.3898, 'learning_rate': 4.25e-05, 'epoch': 0.75}
{'loss': 0.244, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.83}
{'loss': 0.1697, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.92}
{'loss': 0.2093, 'learning_rate': 4e-05, 'epoch': 1.0}
{'loss': 0.115, 'learning_rate': 3.9166666666666665e-05, 'epoch': 1.08}
{'loss': 0.2756, 'learning_rate': 3.8333333333333334e-05, 'epoch': 1.17}
{'loss': 0.0999, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.25}
{'loss': 0.2373, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}
{'loss': 0.3043, 'learning_rate': 3.5833333333333335e-05, 'epoch': 1.42}
{'loss': 0.0867, 'learning_rate': 3.5e-05, 'epoch': 1.5}
{'loss': 0.2814, 'learning_rate': 3.4166666666666666e-05, 'epoch': 1.58}
{'loss': 0.1653, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.67}
{'loss': 0.1075, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.75}
{'loss': 0.0768, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.83}
{'loss': 0.147, 'learning_rate': 3.0833333333333335e-05, 'epoch': 1.92}
{'loss': 0.2432, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.0896, 'learning_rate': 2.916666666666667e-05, 'epoch': 2.08}
{'loss': 0.1088, 'learning_rate': 2.8333333333333335e-05, 'epoch': 2.17}
{'loss': 0.0738, 'learning_rate': 2.7500000000000004e-05, 'epoch': 2.25}
{'loss': 0.2025, 'learning_rate': 2.6666666666666667e-05, 'epoch': 2.33}
{'loss': 0.1488, 'learning_rate': 2.5833333333333336e-05, 'epoch': 2.42}
{'loss': 0.117, 'learning_rate': 2.5e-05, 'epoch': 2.5}
{'loss': 0.1154, 'learning_rate': 2.4166666666666667e-05, 'epoch': 2.58}
{'loss': 0.137, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}
{'loss': 0.0872, 'learning_rate': 2.25e-05, 'epoch': 2.75}
{'loss': 0.1482, 'learning_rate': 2.1666666666666667e-05, 'epoch': 2.83}
{'loss': 0.0478, 'learning_rate': 2.0833333333333336e-05, 'epoch': 2.92}
{'loss': 0.1902, 'learning_rate': 2e-05, 'epoch': 3.0}
{'loss': 0.051, 'learning_rate': 1.9166666666666667e-05, 'epoch': 3.08}
{'loss': 0.13, 'learning_rate': 1.8333333333333333e-05, 'epoch': 3.17}
{'loss': 0.0409, 'learning_rate': 1.75e-05, 'epoch': 3.25}
{'loss': 0.0725, 'learning_rate': 1.6666666666666667e-05, 'epoch': 3.33}
{'loss': 0.0511, 'learning_rate': 1.5833333333333333e-05, 'epoch': 3.42}
{'loss': 0.086, 'learning_rate': 1.5e-05, 'epoch': 3.5}
{'loss': 0.0582, 'learning_rate': 1.4166666666666668e-05, 'epoch': 3.58}
{'loss': 0.0301, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.67}
{'loss': 0.0636, 'learning_rate': 1.25e-05, 'epoch': 3.75}
{'loss': 0.1086, 'learning_rate': 1.1666666666666668e-05, 'epoch': 3.83}
{'loss': 0.0634, 'learning_rate': 1.0833333333333334e-05, 'epoch': 3.92}
{'loss': 0.0198, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0317, 'learning_rate': 9.166666666666666e-06, 'epoch': 4.08}
{'loss': 0.0524, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.17}
{'loss': 0.042, 'learning_rate': 7.5e-06, 'epoch': 4.25}
{'loss': 0.0198, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.33}
{'loss': 0.0278, 'learning_rate': 5.833333333333334e-06, 'epoch': 4.42}
{'loss': 0.0153, 'learning_rate': 5e-06, 'epoch': 4.5}
{'loss': 0.0369, 'learning_rate': 4.166666666666667e-06, 'epoch': 4.58}
{'loss': 0.015, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.67}
{'loss': 0.0179, 'learning_rate': 2.5e-06, 'epoch': 4.75}
{'loss': 0.0068, 'learning_rate': 1.6666666666666667e-06, 'epoch': 4.83}
{'loss': 0.01, 'learning_rate': 8.333333333333333e-07, 'epoch': 4.92}
{'loss': 0.0157, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 688.0644, 'train_samples_per_second': 2.776, 'train_steps_per_second': 0.349, 'train_loss': 0.16871431937906892, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-240/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-96/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-144/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-192/optimizer.pt
validate!
last validate 0.
INFO 11-24 06:29:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:29:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_30_15_40_0.8_0.35_125_5 epoch 1

------------------------------------------------

0.505

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_30_15_40_0.8_0.35_125_5/generated_contents/1
INFO 11-24 06:30:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-96', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-96', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:30:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_30_15_40_0.8_0.35_125_5 epoch 2

------------------------------------------------

0.556

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_30_15_40_0.8_0.35_125_5/generated_contents/2
INFO 11-24 06:31:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-144', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-144', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:31:26 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_30_15_40_0.8_0.35_125_5 epoch 3

------------------------------------------------

0.663

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_30_15_40_0.8_0.35_125_5/generated_contents/3
INFO 11-24 06:32:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-192', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-192', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:32:24 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_30_15_40_0.8_0.35_125_5 epoch 4

------------------------------------------------

0.618

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_30_15_40_0.8_0.35_125_5/generated_contents/4
INFO 11-24 06:33:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-240', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-240', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:33:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_30_15_40_0.8_0.35_125_5 epoch 5

------------------------------------------------

0.65

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_30_15_40_0.8_0.35_125_5/generated_contents/5
mv /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-144 /data2/cyzhao/best_ckpt/NI_task1554_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-48
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-96
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-192
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_30_15_40_0.8_0.35_125_5/checkpoint-240
searching parameters: NI_task1554_20_10_45_1.0_0.5_125_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_45_1.0_0.5_125_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_45_1.0_0.5_125_5/config.json
generate_and_write_inputs!
INFO 11-24 06:34:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:34:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 06:37:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:38:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 129
expected_example_num: 200
selection_ratio: 0.645
finetune_deepseek!
{'loss': 0.9913, 'learning_rate': 4.7647058823529414e-05, 'epoch': 0.24}
{'loss': 0.9927, 'learning_rate': 4.5294117647058826e-05, 'epoch': 0.47}
{'loss': 0.579, 'learning_rate': 4.294117647058823e-05, 'epoch': 0.71}
{'loss': 0.7082, 'learning_rate': 4.058823529411765e-05, 'epoch': 0.94}
{'loss': 0.4804, 'learning_rate': 3.8235294117647055e-05, 'epoch': 1.18}
{'loss': 0.2799, 'learning_rate': 3.5882352941176474e-05, 'epoch': 1.41}
{'loss': 0.2146, 'learning_rate': 3.352941176470588e-05, 'epoch': 1.65}
{'loss': 0.3866, 'learning_rate': 3.11764705882353e-05, 'epoch': 1.88}
{'loss': 0.1148, 'learning_rate': 2.8823529411764703e-05, 'epoch': 2.12}
{'loss': 0.186, 'learning_rate': 2.647058823529412e-05, 'epoch': 2.35}
{'loss': 0.2223, 'learning_rate': 2.411764705882353e-05, 'epoch': 2.59}
{'loss': 0.3152, 'learning_rate': 2.1764705882352943e-05, 'epoch': 2.82}
{'loss': 0.311, 'learning_rate': 1.9411764705882355e-05, 'epoch': 3.06}
{'loss': 0.1372, 'learning_rate': 1.7058823529411767e-05, 'epoch': 3.29}
{'loss': 0.0724, 'learning_rate': 1.4705882352941177e-05, 'epoch': 3.53}
{'loss': 0.2439, 'learning_rate': 1.2352941176470589e-05, 'epoch': 3.76}
{'loss': 0.5057, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.1505, 'learning_rate': 7.647058823529413e-06, 'epoch': 4.24}
{'loss': 0.1135, 'learning_rate': 5.294117647058824e-06, 'epoch': 4.47}
{'loss': 0.1069, 'learning_rate': 2.9411764705882355e-06, 'epoch': 4.71}
{'loss': 0.1699, 'learning_rate': 5.882352941176471e-07, 'epoch': 4.94}
{'train_runtime': 410.3839, 'train_samples_per_second': 1.572, 'train_steps_per_second': 0.207, 'train_loss': 0.34619715354021857, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-68/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-85/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-24 06:46:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:46:41 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_45_1.0_0.5_125_5 epoch 1

------------------------------------------------

0.502

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_45_1.0_0.5_125_5/generated_contents/1
INFO 11-24 06:47:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:47:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_45_1.0_0.5_125_5 epoch 2

------------------------------------------------

0.502

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_45_1.0_0.5_125_5/generated_contents/2
INFO 11-24 06:48:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:48:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_45_1.0_0.5_125_5 epoch 3

------------------------------------------------

0.502

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_45_1.0_0.5_125_5/generated_contents/3
INFO 11-24 06:49:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-68', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-68', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:49:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_45_1.0_0.5_125_5 epoch 4

------------------------------------------------

0.507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_45_1.0_0.5_125_5/generated_contents/4
INFO 11-24 06:50:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-85', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-85', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:50:37 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_45_1.0_0.5_125_5 epoch 5

------------------------------------------------

0.504

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_45_1.0_0.5_125_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-34
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-51
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-68
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_45_1.0_0.5_125_5/checkpoint-85
searching parameters: NI_task1554_50_10_45_1.0_0.35_115_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_1.0_0.35_115_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_1.0_0.35_115_4/config.json
generate_and_write_inputs!
INFO 11-24 06:51:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:51:46 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 07:01:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:02:15 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 375
expected_example_num: 500
selection_ratio: 0.75
finetune_deepseek!
{'loss': 2.7496, 'learning_rate': 4.893617021276596e-05, 'epoch': 0.09}
{'loss': 1.206, 'learning_rate': 4.787234042553192e-05, 'epoch': 0.17}
{'loss': 0.6459, 'learning_rate': 4.680851063829788e-05, 'epoch': 0.26}
{'loss': 0.2143, 'learning_rate': 4.574468085106383e-05, 'epoch': 0.34}
{'loss': 0.3719, 'learning_rate': 4.468085106382979e-05, 'epoch': 0.43}
{'loss': 0.5195, 'learning_rate': 4.3617021276595746e-05, 'epoch': 0.51}
{'loss': 0.3157, 'learning_rate': 4.2553191489361704e-05, 'epoch': 0.6}
{'loss': 0.1461, 'learning_rate': 4.148936170212766e-05, 'epoch': 0.68}
{'loss': 0.3435, 'learning_rate': 4.0425531914893614e-05, 'epoch': 0.77}
{'loss': 0.2591, 'learning_rate': 3.936170212765958e-05, 'epoch': 0.85}
{'loss': 0.0765, 'learning_rate': 3.829787234042553e-05, 'epoch': 0.94}
{'loss': 0.2541, 'learning_rate': 3.723404255319149e-05, 'epoch': 1.02}
{'loss': 0.1495, 'learning_rate': 3.617021276595745e-05, 'epoch': 1.11}
{'loss': 0.1739, 'learning_rate': 3.5106382978723407e-05, 'epoch': 1.19}
{'loss': 0.1585, 'learning_rate': 3.4042553191489365e-05, 'epoch': 1.28}
{'loss': 0.2375, 'learning_rate': 3.2978723404255317e-05, 'epoch': 1.36}
{'loss': 0.2639, 'learning_rate': 3.191489361702128e-05, 'epoch': 1.45}
{'loss': 0.1398, 'learning_rate': 3.085106382978723e-05, 'epoch': 1.53}
{'loss': 0.1115, 'learning_rate': 2.9787234042553192e-05, 'epoch': 1.62}
{'loss': 0.288, 'learning_rate': 2.8723404255319154e-05, 'epoch': 1.7}
{'loss': 0.1343, 'learning_rate': 2.765957446808511e-05, 'epoch': 1.79}
{'loss': 0.1376, 'learning_rate': 2.6595744680851064e-05, 'epoch': 1.87}
{'loss': 0.106, 'learning_rate': 2.5531914893617022e-05, 'epoch': 1.96}
{'loss': 0.1239, 'learning_rate': 2.446808510638298e-05, 'epoch': 2.04}
{'loss': 0.099, 'learning_rate': 2.340425531914894e-05, 'epoch': 2.13}
{'loss': 0.1366, 'learning_rate': 2.2340425531914894e-05, 'epoch': 2.21}
{'loss': 0.0854, 'learning_rate': 2.1276595744680852e-05, 'epoch': 2.3}
{'loss': 0.1137, 'learning_rate': 2.0212765957446807e-05, 'epoch': 2.38}
{'loss': 0.0205, 'learning_rate': 1.9148936170212766e-05, 'epoch': 2.47}
{'loss': 0.1003, 'learning_rate': 1.8085106382978724e-05, 'epoch': 2.55}
{'loss': 0.1175, 'learning_rate': 1.7021276595744682e-05, 'epoch': 2.64}
{'loss': 0.0475, 'learning_rate': 1.595744680851064e-05, 'epoch': 2.72}
{'loss': 0.0507, 'learning_rate': 1.4893617021276596e-05, 'epoch': 2.81}
{'loss': 0.0731, 'learning_rate': 1.3829787234042554e-05, 'epoch': 2.89}
{'loss': 0.0831, 'learning_rate': 1.2765957446808511e-05, 'epoch': 2.98}
{'loss': 0.033, 'learning_rate': 1.170212765957447e-05, 'epoch': 3.06}
{'loss': 0.0246, 'learning_rate': 1.0638297872340426e-05, 'epoch': 3.15}
{'loss': 0.031, 'learning_rate': 9.574468085106383e-06, 'epoch': 3.23}
{'loss': 0.0177, 'learning_rate': 8.510638297872341e-06, 'epoch': 3.32}
{'loss': 0.024, 'learning_rate': 7.446808510638298e-06, 'epoch': 3.4}
{'loss': 0.0263, 'learning_rate': 6.3829787234042555e-06, 'epoch': 3.49}
{'loss': 0.0154, 'learning_rate': 5.319148936170213e-06, 'epoch': 3.57}
{'loss': 0.0389, 'learning_rate': 4.255319148936171e-06, 'epoch': 3.66}
{'loss': 0.0213, 'learning_rate': 3.1914893617021277e-06, 'epoch': 3.74}
{'loss': 0.01, 'learning_rate': 2.1276595744680853e-06, 'epoch': 3.83}
{'loss': 0.0195, 'learning_rate': 1.0638297872340427e-06, 'epoch': 3.91}
{'loss': 0.027, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 504.7591, 'train_samples_per_second': 2.972, 'train_steps_per_second': 0.372, 'train_loss': 0.2200567680906425, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-94/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-188/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-141/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-47/optimizer.pt
validate!
last validate 0.
INFO 11-24 07:13:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-47', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-47', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:13:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_1.0_0.35_115_4 epoch 1

------------------------------------------------

0.521

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_1.0_0.35_115_4/generated_contents/1
INFO 11-24 07:14:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-94', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-94', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:14:36 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_1.0_0.35_115_4 epoch 2

------------------------------------------------

0.552

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_1.0_0.35_115_4/generated_contents/2
INFO 11-24 07:15:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-141', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-141', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:15:36 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_1.0_0.35_115_4 epoch 3

------------------------------------------------

0.586

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_1.0_0.35_115_4/generated_contents/3
INFO 11-24 07:16:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-188', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-188', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:16:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_1.0_0.35_115_4 epoch 4

------------------------------------------------

0.603

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_1.0_0.35_115_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-47
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-94
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-141
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_1.0_0.35_115_4/checkpoint-188
searching parameters: NI_task1554_40_40_40_0.9_0.35_130_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_40_0.9_0.35_130_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_40_0.9_0.35_130_4/config.json
generate_and_write_inputs!
INFO 11-24 07:17:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:17:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 07:33:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:33:27 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 1260
expected_example_num: 1600
selection_ratio: 0.7875
finetune_deepseek!
{'loss': 2.465, 'learning_rate': 4.968354430379747e-05, 'epoch': 0.03}
{'loss': 1.372, 'learning_rate': 4.936708860759494e-05, 'epoch': 0.05}
{'loss': 1.1708, 'learning_rate': 4.905063291139241e-05, 'epoch': 0.08}
{'loss': 0.3274, 'learning_rate': 4.8734177215189874e-05, 'epoch': 0.1}
{'loss': 0.261, 'learning_rate': 4.8417721518987346e-05, 'epoch': 0.13}
{'loss': 0.5286, 'learning_rate': 4.810126582278481e-05, 'epoch': 0.15}
{'loss': 0.3848, 'learning_rate': 4.778481012658228e-05, 'epoch': 0.18}
{'loss': 0.5638, 'learning_rate': 4.7468354430379746e-05, 'epoch': 0.2}
{'loss': 0.7096, 'learning_rate': 4.715189873417722e-05, 'epoch': 0.23}
{'loss': 0.3328, 'learning_rate': 4.683544303797468e-05, 'epoch': 0.25}
{'loss': 0.3165, 'learning_rate': 4.6518987341772154e-05, 'epoch': 0.28}
{'loss': 0.282, 'learning_rate': 4.6202531645569625e-05, 'epoch': 0.3}
{'loss': 0.5497, 'learning_rate': 4.588607594936709e-05, 'epoch': 0.33}
{'loss': 0.2994, 'learning_rate': 4.556962025316456e-05, 'epoch': 0.35}
{'loss': 0.3909, 'learning_rate': 4.525316455696203e-05, 'epoch': 0.38}
{'loss': 0.2079, 'learning_rate': 4.49367088607595e-05, 'epoch': 0.41}
{'loss': 0.2144, 'learning_rate': 4.462025316455696e-05, 'epoch': 0.43}
{'loss': 0.36, 'learning_rate': 4.430379746835443e-05, 'epoch': 0.46}
{'loss': 0.5151, 'learning_rate': 4.3987341772151904e-05, 'epoch': 0.48}
{'loss': 0.2595, 'learning_rate': 4.367088607594937e-05, 'epoch': 0.51}
{'loss': 0.488, 'learning_rate': 4.3354430379746834e-05, 'epoch': 0.53}
{'loss': 0.2587, 'learning_rate': 4.3037974683544305e-05, 'epoch': 0.56}
{'loss': 0.1937, 'learning_rate': 4.2721518987341776e-05, 'epoch': 0.58}
{'loss': 0.3634, 'learning_rate': 4.240506329113924e-05, 'epoch': 0.61}
{'loss': 0.4282, 'learning_rate': 4.208860759493671e-05, 'epoch': 0.63}
{'loss': 0.228, 'learning_rate': 4.177215189873418e-05, 'epoch': 0.66}
{'loss': 0.3009, 'learning_rate': 4.145569620253165e-05, 'epoch': 0.68}
{'loss': 0.2483, 'learning_rate': 4.113924050632912e-05, 'epoch': 0.71}
{'loss': 0.3003, 'learning_rate': 4.0822784810126584e-05, 'epoch': 0.73}
{'loss': 0.2902, 'learning_rate': 4.050632911392405e-05, 'epoch': 0.76}
{'loss': 0.378, 'learning_rate': 4.018987341772152e-05, 'epoch': 0.78}
{'loss': 0.2501, 'learning_rate': 3.987341772151899e-05, 'epoch': 0.81}
{'loss': 0.19, 'learning_rate': 3.9556962025316456e-05, 'epoch': 0.84}
{'loss': 0.2905, 'learning_rate': 3.924050632911392e-05, 'epoch': 0.86}
{'loss': 0.3515, 'learning_rate': 3.89240506329114e-05, 'epoch': 0.89}
{'loss': 0.1992, 'learning_rate': 3.8607594936708864e-05, 'epoch': 0.91}
{'loss': 0.2457, 'learning_rate': 3.829113924050633e-05, 'epoch': 0.94}
{'loss': 0.2838, 'learning_rate': 3.79746835443038e-05, 'epoch': 0.96}
{'loss': 0.1934, 'learning_rate': 3.765822784810127e-05, 'epoch': 0.99}
{'loss': 0.3582, 'learning_rate': 3.7341772151898736e-05, 'epoch': 1.01}
{'loss': 0.1052, 'learning_rate': 3.70253164556962e-05, 'epoch': 1.04}
{'loss': 0.1355, 'learning_rate': 3.670886075949367e-05, 'epoch': 1.06}
{'loss': 0.1488, 'learning_rate': 3.639240506329114e-05, 'epoch': 1.09}
{'loss': 0.1734, 'learning_rate': 3.607594936708861e-05, 'epoch': 1.11}
{'loss': 0.1231, 'learning_rate': 3.575949367088608e-05, 'epoch': 1.14}
{'loss': 0.2763, 'learning_rate': 3.5443037974683544e-05, 'epoch': 1.16}
{'loss': 0.2593, 'learning_rate': 3.5126582278481015e-05, 'epoch': 1.19}
{'loss': 0.3276, 'learning_rate': 3.4810126582278487e-05, 'epoch': 1.22}
{'loss': 0.1367, 'learning_rate': 3.449367088607595e-05, 'epoch': 1.24}
{'loss': 0.2244, 'learning_rate': 3.4177215189873416e-05, 'epoch': 1.27}
{'loss': 0.1581, 'learning_rate': 3.386075949367089e-05, 'epoch': 1.29}
{'loss': 0.1081, 'learning_rate': 3.354430379746836e-05, 'epoch': 1.32}
{'loss': 0.3372, 'learning_rate': 3.322784810126582e-05, 'epoch': 1.34}
{'loss': 0.1008, 'learning_rate': 3.291139240506329e-05, 'epoch': 1.37}
{'loss': 0.2187, 'learning_rate': 3.2594936708860766e-05, 'epoch': 1.39}
{'loss': 0.0756, 'learning_rate': 3.227848101265823e-05, 'epoch': 1.42}
{'loss': 0.1169, 'learning_rate': 3.1962025316455695e-05, 'epoch': 1.44}
{'loss': 0.1034, 'learning_rate': 3.1645569620253167e-05, 'epoch': 1.47}
{'loss': 0.0923, 'learning_rate': 3.132911392405064e-05, 'epoch': 1.49}
{'loss': 0.118, 'learning_rate': 3.10126582278481e-05, 'epoch': 1.52}
{'loss': 0.128, 'learning_rate': 3.0696202531645574e-05, 'epoch': 1.54}
{'loss': 0.0989, 'learning_rate': 3.0379746835443042e-05, 'epoch': 1.57}
{'loss': 0.204, 'learning_rate': 3.0063291139240506e-05, 'epoch': 1.59}
{'loss': 0.2132, 'learning_rate': 2.9746835443037974e-05, 'epoch': 1.62}
{'loss': 0.2382, 'learning_rate': 2.9430379746835446e-05, 'epoch': 1.65}
{'loss': 0.0809, 'learning_rate': 2.9113924050632914e-05, 'epoch': 1.67}
{'loss': 0.2412, 'learning_rate': 2.879746835443038e-05, 'epoch': 1.7}
{'loss': 0.0657, 'learning_rate': 2.848101265822785e-05, 'epoch': 1.72}
{'loss': 0.1434, 'learning_rate': 2.8164556962025318e-05, 'epoch': 1.75}
{'loss': 0.1255, 'learning_rate': 2.7848101265822786e-05, 'epoch': 1.77}
{'loss': 0.1822, 'learning_rate': 2.7531645569620257e-05, 'epoch': 1.8}
{'loss': 0.198, 'learning_rate': 2.7215189873417722e-05, 'epoch': 1.82}
{'loss': 0.1319, 'learning_rate': 2.689873417721519e-05, 'epoch': 1.85}
{'loss': 0.196, 'learning_rate': 2.6582278481012658e-05, 'epoch': 1.87}
{'loss': 0.1059, 'learning_rate': 2.626582278481013e-05, 'epoch': 1.9}
{'loss': 0.1019, 'learning_rate': 2.5949367088607597e-05, 'epoch': 1.92}
{'loss': 0.1351, 'learning_rate': 2.5632911392405062e-05, 'epoch': 1.95}
{'loss': 0.0991, 'learning_rate': 2.5316455696202533e-05, 'epoch': 1.97}
{'loss': 0.1545, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.1175, 'learning_rate': 2.468354430379747e-05, 'epoch': 2.03}
{'loss': 0.0921, 'learning_rate': 2.4367088607594937e-05, 'epoch': 2.05}
{'loss': 0.0591, 'learning_rate': 2.4050632911392405e-05, 'epoch': 2.08}
{'loss': 0.1057, 'learning_rate': 2.3734177215189873e-05, 'epoch': 2.1}
{'loss': 0.0346, 'learning_rate': 2.341772151898734e-05, 'epoch': 2.13}
{'loss': 0.0437, 'learning_rate': 2.3101265822784813e-05, 'epoch': 2.15}
{'loss': 0.1864, 'learning_rate': 2.278481012658228e-05, 'epoch': 2.18}
{'loss': 0.1003, 'learning_rate': 2.246835443037975e-05, 'epoch': 2.2}
{'loss': 0.0369, 'learning_rate': 2.2151898734177217e-05, 'epoch': 2.23}
{'loss': 0.0319, 'learning_rate': 2.1835443037974685e-05, 'epoch': 2.25}
{'loss': 0.1804, 'learning_rate': 2.1518987341772153e-05, 'epoch': 2.28}
{'loss': 0.0899, 'learning_rate': 2.120253164556962e-05, 'epoch': 2.3}
{'loss': 0.1131, 'learning_rate': 2.088607594936709e-05, 'epoch': 2.33}
{'loss': 0.057, 'learning_rate': 2.056962025316456e-05, 'epoch': 2.35}
{'loss': 0.0624, 'learning_rate': 2.0253164556962025e-05, 'epoch': 2.38}
{'loss': 0.1594, 'learning_rate': 1.9936708860759496e-05, 'epoch': 2.41}
{'loss': 0.0465, 'learning_rate': 1.962025316455696e-05, 'epoch': 2.43}
{'loss': 0.1294, 'learning_rate': 1.9303797468354432e-05, 'epoch': 2.46}
{'loss': 0.0539, 'learning_rate': 1.89873417721519e-05, 'epoch': 2.48}
{'loss': 0.0411, 'learning_rate': 1.8670886075949368e-05, 'epoch': 2.51}
{'loss': 0.0441, 'learning_rate': 1.8354430379746836e-05, 'epoch': 2.53}
{'loss': 0.0525, 'learning_rate': 1.8037974683544304e-05, 'epoch': 2.56}
{'loss': 0.0871, 'learning_rate': 1.7721518987341772e-05, 'epoch': 2.58}
{'loss': 0.1275, 'learning_rate': 1.7405063291139243e-05, 'epoch': 2.61}
{'loss': 0.0936, 'learning_rate': 1.7088607594936708e-05, 'epoch': 2.63}
{'loss': 0.0562, 'learning_rate': 1.677215189873418e-05, 'epoch': 2.66}
{'loss': 0.0688, 'learning_rate': 1.6455696202531644e-05, 'epoch': 2.68}
{'loss': 0.0961, 'learning_rate': 1.6139240506329115e-05, 'epoch': 2.71}
{'loss': 0.0996, 'learning_rate': 1.5822784810126583e-05, 'epoch': 2.73}
{'loss': 0.0414, 'learning_rate': 1.550632911392405e-05, 'epoch': 2.76}
{'loss': 0.1092, 'learning_rate': 1.5189873417721521e-05, 'epoch': 2.78}
{'loss': 0.0846, 'learning_rate': 1.4873417721518987e-05, 'epoch': 2.81}
{'loss': 0.0629, 'learning_rate': 1.4556962025316457e-05, 'epoch': 2.84}
{'loss': 0.0376, 'learning_rate': 1.4240506329113925e-05, 'epoch': 2.86}
{'loss': 0.0294, 'learning_rate': 1.3924050632911393e-05, 'epoch': 2.89}
{'loss': 0.0734, 'learning_rate': 1.3607594936708861e-05, 'epoch': 2.91}
{'loss': 0.167, 'learning_rate': 1.3291139240506329e-05, 'epoch': 2.94}
{'loss': 0.0423, 'learning_rate': 1.2974683544303799e-05, 'epoch': 2.96}
{'loss': 0.1262, 'learning_rate': 1.2658227848101267e-05, 'epoch': 2.99}
{'loss': 0.0635, 'learning_rate': 1.2341772151898735e-05, 'epoch': 3.01}
{'loss': 0.0542, 'learning_rate': 1.2025316455696203e-05, 'epoch': 3.04}
{'loss': 0.0185, 'learning_rate': 1.170886075949367e-05, 'epoch': 3.06}
{'loss': 0.0407, 'learning_rate': 1.139240506329114e-05, 'epoch': 3.09}
{'loss': 0.0084, 'learning_rate': 1.1075949367088608e-05, 'epoch': 3.11}
{'loss': 0.018, 'learning_rate': 1.0759493670886076e-05, 'epoch': 3.14}
{'loss': 0.0123, 'learning_rate': 1.0443037974683544e-05, 'epoch': 3.16}
{'loss': 0.0479, 'learning_rate': 1.0126582278481012e-05, 'epoch': 3.19}
{'loss': 0.0397, 'learning_rate': 9.81012658227848e-06, 'epoch': 3.22}
{'loss': 0.0666, 'learning_rate': 9.49367088607595e-06, 'epoch': 3.24}
{'loss': 0.0564, 'learning_rate': 9.177215189873418e-06, 'epoch': 3.27}
{'loss': 0.0047, 'learning_rate': 8.860759493670886e-06, 'epoch': 3.29}
{'loss': 0.0186, 'learning_rate': 8.544303797468354e-06, 'epoch': 3.32}
{'loss': 0.0335, 'learning_rate': 8.227848101265822e-06, 'epoch': 3.34}
{'loss': 0.0262, 'learning_rate': 7.911392405063292e-06, 'epoch': 3.37}
{'loss': 0.018, 'learning_rate': 7.5949367088607605e-06, 'epoch': 3.39}
{'loss': 0.0124, 'learning_rate': 7.2784810126582285e-06, 'epoch': 3.42}
{'loss': 0.0977, 'learning_rate': 6.9620253164556965e-06, 'epoch': 3.44}
{'loss': 0.0418, 'learning_rate': 6.6455696202531645e-06, 'epoch': 3.47}
{'loss': 0.0128, 'learning_rate': 6.329113924050633e-06, 'epoch': 3.49}
{'loss': 0.0319, 'learning_rate': 6.012658227848101e-06, 'epoch': 3.52}
{'loss': 0.0057, 'learning_rate': 5.69620253164557e-06, 'epoch': 3.54}
{'loss': 0.0282, 'learning_rate': 5.379746835443038e-06, 'epoch': 3.57}
{'loss': 0.0136, 'learning_rate': 5.063291139240506e-06, 'epoch': 3.59}
{'loss': 0.0231, 'learning_rate': 4.746835443037975e-06, 'epoch': 3.62}
{'loss': 0.0306, 'learning_rate': 4.430379746835443e-06, 'epoch': 3.65}
{'loss': 0.0141, 'learning_rate': 4.113924050632911e-06, 'epoch': 3.67}
{'loss': 0.0402, 'learning_rate': 3.7974683544303802e-06, 'epoch': 3.7}
{'loss': 0.023, 'learning_rate': 3.4810126582278482e-06, 'epoch': 3.72}
{'loss': 0.0186, 'learning_rate': 3.1645569620253167e-06, 'epoch': 3.75}
{'loss': 0.0086, 'learning_rate': 2.848101265822785e-06, 'epoch': 3.77}
{'loss': 0.0228, 'learning_rate': 2.531645569620253e-06, 'epoch': 3.8}
{'loss': 0.005, 'learning_rate': 2.2151898734177215e-06, 'epoch': 3.82}
{'loss': 0.0402, 'learning_rate': 1.8987341772151901e-06, 'epoch': 3.85}
{'loss': 0.0269, 'learning_rate': 1.5822784810126583e-06, 'epoch': 3.87}
{'loss': 0.026, 'learning_rate': 1.2658227848101265e-06, 'epoch': 3.9}
{'loss': 0.0175, 'learning_rate': 9.493670886075951e-07, 'epoch': 3.92}
{'loss': 0.0055, 'learning_rate': 6.329113924050633e-07, 'epoch': 3.95}
{'loss': 0.0103, 'learning_rate': 3.1645569620253163e-07, 'epoch': 3.97}
{'loss': 0.0295, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 1190.7136, 'train_samples_per_second': 4.233, 'train_steps_per_second': 0.531, 'train_loss': 0.17651271651487185, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-316/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-158/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-632/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-474/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:00:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-158', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-158', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:00:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_40_0.9_0.35_130_4 epoch 1

------------------------------------------------

0.504

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_40_0.9_0.35_130_4/generated_contents/1
INFO 11-24 08:01:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-316', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-316', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:01:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_40_0.9_0.35_130_4 epoch 2

------------------------------------------------

0.597

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_40_0.9_0.35_130_4/generated_contents/2
INFO 11-24 08:02:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-474', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-474', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:02:33 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_40_0.9_0.35_130_4 epoch 3

------------------------------------------------

0.577

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_40_0.9_0.35_130_4/generated_contents/3
INFO 11-24 08:03:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-632', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-632', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:03:31 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_40_0.9_0.35_130_4 epoch 4

------------------------------------------------

0.579

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_40_0.9_0.35_130_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-158
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-316
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-474
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_40_0.9_0.35_130_4/checkpoint-632
searching parameters: NI_task1554_50_10_45_0.8_0.4_130_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_0.8_0.4_130_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_0.8_0.4_130_5/config.json
generate_and_write_inputs!
INFO 11-24 08:04:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:04:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:16:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:16:36 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 449
expected_example_num: 500
selection_ratio: 0.898
finetune_deepseek!
{'loss': 1.9733, 'learning_rate': 4.9298245614035086e-05, 'epoch': 0.07}
{'loss': 0.685, 'learning_rate': 4.859649122807018e-05, 'epoch': 0.14}
{'loss': 0.4785, 'learning_rate': 4.789473684210526e-05, 'epoch': 0.21}
{'loss': 0.2227, 'learning_rate': 4.719298245614036e-05, 'epoch': 0.28}
{'loss': 0.508, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.35}
{'loss': 0.3937, 'learning_rate': 4.5789473684210527e-05, 'epoch': 0.42}
{'loss': 0.2908, 'learning_rate': 4.508771929824562e-05, 'epoch': 0.49}
{'loss': 0.2237, 'learning_rate': 4.43859649122807e-05, 'epoch': 0.56}
{'loss': 0.3446, 'learning_rate': 4.368421052631579e-05, 'epoch': 0.63}
{'loss': 0.2968, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.7}
{'loss': 0.2412, 'learning_rate': 4.228070175438597e-05, 'epoch': 0.77}
{'loss': 0.5699, 'learning_rate': 4.157894736842106e-05, 'epoch': 0.84}
{'loss': 0.2516, 'learning_rate': 4.087719298245614e-05, 'epoch': 0.91}
{'loss': 0.3753, 'learning_rate': 4.017543859649123e-05, 'epoch': 0.98}
{'loss': 0.1656, 'learning_rate': 3.9473684210526316e-05, 'epoch': 1.05}
{'loss': 0.2967, 'learning_rate': 3.877192982456141e-05, 'epoch': 1.12}
{'loss': 0.2403, 'learning_rate': 3.80701754385965e-05, 'epoch': 1.19}
{'loss': 0.1824, 'learning_rate': 3.736842105263158e-05, 'epoch': 1.26}
{'loss': 0.3696, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}
{'loss': 0.2164, 'learning_rate': 3.5964912280701756e-05, 'epoch': 1.4}
{'loss': 0.3571, 'learning_rate': 3.526315789473684e-05, 'epoch': 1.47}
{'loss': 0.3175, 'learning_rate': 3.456140350877193e-05, 'epoch': 1.54}
{'loss': 0.2803, 'learning_rate': 3.385964912280702e-05, 'epoch': 1.61}
{'loss': 0.2384, 'learning_rate': 3.3157894736842106e-05, 'epoch': 1.68}
{'loss': 0.1365, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.75}
{'loss': 0.1026, 'learning_rate': 3.175438596491228e-05, 'epoch': 1.82}
{'loss': 0.1224, 'learning_rate': 3.105263157894737e-05, 'epoch': 1.89}
{'loss': 0.0943, 'learning_rate': 3.035087719298246e-05, 'epoch': 1.96}
{'loss': 0.4023, 'learning_rate': 2.9649122807017543e-05, 'epoch': 2.04}
{'loss': 0.0893, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.11}
{'loss': 0.1218, 'learning_rate': 2.824561403508772e-05, 'epoch': 2.18}
{'loss': 0.1687, 'learning_rate': 2.754385964912281e-05, 'epoch': 2.25}
{'loss': 0.1505, 'learning_rate': 2.6842105263157896e-05, 'epoch': 2.32}
{'loss': 0.0785, 'learning_rate': 2.6140350877192983e-05, 'epoch': 2.39}
{'loss': 0.2083, 'learning_rate': 2.5438596491228074e-05, 'epoch': 2.46}
{'loss': 0.1091, 'learning_rate': 2.4736842105263158e-05, 'epoch': 2.53}
{'loss': 0.1129, 'learning_rate': 2.4035087719298245e-05, 'epoch': 2.6}
{'loss': 0.1377, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}
{'loss': 0.095, 'learning_rate': 2.2631578947368423e-05, 'epoch': 2.74}
{'loss': 0.0993, 'learning_rate': 2.1929824561403507e-05, 'epoch': 2.81}
{'loss': 0.1234, 'learning_rate': 2.1228070175438598e-05, 'epoch': 2.88}
{'loss': 0.0599, 'learning_rate': 2.0526315789473685e-05, 'epoch': 2.95}
{'loss': 0.0533, 'learning_rate': 1.9824561403508773e-05, 'epoch': 3.02}
{'loss': 0.0373, 'learning_rate': 1.9122807017543863e-05, 'epoch': 3.09}
{'loss': 0.0507, 'learning_rate': 1.8421052631578947e-05, 'epoch': 3.16}
{'loss': 0.0638, 'learning_rate': 1.7719298245614035e-05, 'epoch': 3.23}
{'loss': 0.0616, 'learning_rate': 1.7017543859649125e-05, 'epoch': 3.3}
{'loss': 0.0839, 'learning_rate': 1.6315789473684213e-05, 'epoch': 3.37}
{'loss': 0.13, 'learning_rate': 1.56140350877193e-05, 'epoch': 3.44}
{'loss': 0.0247, 'learning_rate': 1.4912280701754386e-05, 'epoch': 3.51}
{'loss': 0.0419, 'learning_rate': 1.4210526315789475e-05, 'epoch': 3.58}
{'loss': 0.0183, 'learning_rate': 1.3508771929824562e-05, 'epoch': 3.65}
{'loss': 0.1153, 'learning_rate': 1.2807017543859651e-05, 'epoch': 3.72}
{'loss': 0.0502, 'learning_rate': 1.2105263157894737e-05, 'epoch': 3.79}
{'loss': 0.0841, 'learning_rate': 1.1403508771929824e-05, 'epoch': 3.86}
{'loss': 0.018, 'learning_rate': 1.0701754385964913e-05, 'epoch': 3.93}
{'loss': 0.0214, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0496, 'learning_rate': 9.298245614035088e-06, 'epoch': 4.07}
{'loss': 0.0127, 'learning_rate': 8.596491228070176e-06, 'epoch': 4.14}
{'loss': 0.0514, 'learning_rate': 7.894736842105263e-06, 'epoch': 4.21}
{'loss': 0.0197, 'learning_rate': 7.192982456140351e-06, 'epoch': 4.28}
{'loss': 0.0343, 'learning_rate': 6.4912280701754385e-06, 'epoch': 4.35}
{'loss': 0.034, 'learning_rate': 5.789473684210527e-06, 'epoch': 4.42}
{'loss': 0.0234, 'learning_rate': 5.087719298245614e-06, 'epoch': 4.49}
{'loss': 0.036, 'learning_rate': 4.3859649122807014e-06, 'epoch': 4.56}
{'loss': 0.0124, 'learning_rate': 3.6842105263157892e-06, 'epoch': 4.63}
{'loss': 0.0138, 'learning_rate': 2.9824561403508774e-06, 'epoch': 4.7}
{'loss': 0.0399, 'learning_rate': 2.2807017543859652e-06, 'epoch': 4.77}
{'loss': 0.0208, 'learning_rate': 1.5789473684210528e-06, 'epoch': 4.84}
{'loss': 0.027, 'learning_rate': 8.771929824561404e-07, 'epoch': 4.91}
{'loss': 0.0282, 'learning_rate': 1.7543859649122808e-07, 'epoch': 4.98}
{'train_runtime': 773.3325, 'train_samples_per_second': 2.903, 'train_steps_per_second': 0.369, 'train_loss': 0.18512172970467394, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-228/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-285/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-114/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-171/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:33:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:33:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_0.8_0.4_130_5 epoch 1

------------------------------------------------

0.574

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_0.8_0.4_130_5/generated_contents/1
INFO 11-24 08:34:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-114', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-114', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:34:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_0.8_0.4_130_5 epoch 2

------------------------------------------------

0.625

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_0.8_0.4_130_5/generated_contents/2
INFO 11-24 08:35:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-171', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-171', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:35:46 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_0.8_0.4_130_5 epoch 3

------------------------------------------------

0.714

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_0.8_0.4_130_5/generated_contents/3
INFO 11-24 08:36:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-228', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-228', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:36:45 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_0.8_0.4_130_5 epoch 4

------------------------------------------------

0.656

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_0.8_0.4_130_5/generated_contents/4
INFO 11-24 08:37:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-285', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-285', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:37:44 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_10_45_0.8_0.4_130_5 epoch 5

------------------------------------------------

0.73

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_10_45_0.8_0.4_130_5/generated_contents/5
rm -rf /data2/cyzhao/best_ckpt/NI_task1554_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-285 /data2/cyzhao/best_ckpt/NI_task1554_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-57
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-114
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-171
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_10_45_0.8_0.4_130_5/checkpoint-228
searching parameters: NI_task1554_20_10_50_1.0_0.35_120_3
/home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_50_1.0_0.35_120_3
/home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_50_1.0_0.35_120_3/config.json
generate_and_write_inputs!
INFO 11-24 08:38:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:38:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:43:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:43:16 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 142
expected_example_num: 200
selection_ratio: 0.71
finetune_deepseek!
{'loss': 2.9361, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.22}
{'loss': 0.3198, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.5742, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.3601, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.181, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}
{'loss': 0.1512, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1374, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.56}
{'loss': 0.1429, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1671, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1452, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.1419, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.44}
{'loss': 0.0796, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0495, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.89}
{'train_runtime': 241.0862, 'train_samples_per_second': 1.767, 'train_steps_per_second': 0.224, 'train_loss': 0.40409722593095565, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-54/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:48:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:49:06 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_50_1.0_0.35_120_3 epoch 1

------------------------------------------------

0.498

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_50_1.0_0.35_120_3/generated_contents/1
INFO 11-24 08:49:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:50:05 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_50_1.0_0.35_120_3 epoch 2

------------------------------------------------

0.533

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_50_1.0_0.35_120_3/generated_contents/2
INFO 11-24 08:50:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-54', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-54', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:51:05 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_20_10_50_1.0_0.35_120_3 epoch 3

------------------------------------------------

0.505

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_20_10_50_1.0_0.35_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-36
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_20_10_50_1.0_0.35_120_3/checkpoint-54
searching parameters: NI_task1554_40_30_40_0.9_0.35_125_3
/home/cyzhao/NI_task1554_exp_1/NI_task1554_40_30_40_0.9_0.35_125_3
/home/cyzhao/NI_task1554_exp_1/NI_task1554_40_30_40_0.9_0.35_125_3/config.json
generate_and_write_inputs!
INFO 11-24 08:51:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:52:10 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 09:08:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:08:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 954
expected_example_num: 1200
selection_ratio: 0.795
finetune_deepseek!
{'loss': 0.8289, 'learning_rate': 4.9444444444444446e-05, 'epoch': 0.03}
{'loss': 0.4229, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.07}
{'loss': 0.7053, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.1}
{'loss': 0.284, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.13}
{'loss': 0.3354, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.17}
{'loss': 0.2208, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}
{'loss': 0.1815, 'learning_rate': 4.6111111111111115e-05, 'epoch': 0.23}
{'loss': 0.2419, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.1435, 'learning_rate': 4.5e-05, 'epoch': 0.3}
{'loss': 0.2998, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.337, 'learning_rate': 4.388888888888889e-05, 'epoch': 0.37}
{'loss': 0.1746, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.1431, 'learning_rate': 4.277777777777778e-05, 'epoch': 0.43}
{'loss': 0.1694, 'learning_rate': 4.222222222222222e-05, 'epoch': 0.47}
{'loss': 0.132, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.2632, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.0747, 'learning_rate': 4.055555555555556e-05, 'epoch': 0.57}
{'loss': 0.0654, 'learning_rate': 4e-05, 'epoch': 0.6}
{'loss': 0.145, 'learning_rate': 3.944444444444445e-05, 'epoch': 0.63}
{'loss': 0.1322, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.0867, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.7}
{'loss': 0.2111, 'learning_rate': 3.777777777777778e-05, 'epoch': 0.73}
{'loss': 0.088, 'learning_rate': 3.722222222222222e-05, 'epoch': 0.77}
{'loss': 0.1571, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.1304, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}
{'loss': 0.1482, 'learning_rate': 3.555555555555556e-05, 'epoch': 0.87}
{'loss': 0.3419, 'learning_rate': 3.5e-05, 'epoch': 0.9}
{'loss': 0.0895, 'learning_rate': 3.444444444444445e-05, 'epoch': 0.93}
{'loss': 0.1733, 'learning_rate': 3.388888888888889e-05, 'epoch': 0.97}
{'loss': 0.1866, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1044, 'learning_rate': 3.277777777777778e-05, 'epoch': 1.03}
{'loss': 0.0702, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.0747, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.1}
{'loss': 0.0594, 'learning_rate': 3.111111111111111e-05, 'epoch': 1.13}
{'loss': 0.0792, 'learning_rate': 3.055555555555556e-05, 'epoch': 1.17}
{'loss': 0.1628, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.0585, 'learning_rate': 2.9444444444444448e-05, 'epoch': 1.23}
{'loss': 0.0548, 'learning_rate': 2.8888888888888888e-05, 'epoch': 1.27}
{'loss': 0.1065, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.3}
{'loss': 0.0336, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0701, 'learning_rate': 2.7222222222222223e-05, 'epoch': 1.37}
{'loss': 0.0551, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}
{'loss': 0.1557, 'learning_rate': 2.6111111111111114e-05, 'epoch': 1.43}
{'loss': 0.1208, 'learning_rate': 2.5555555555555554e-05, 'epoch': 1.47}
{'loss': 0.0728, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1211, 'learning_rate': 2.4444444444444445e-05, 'epoch': 1.53}
{'loss': 0.1282, 'learning_rate': 2.3888888888888892e-05, 'epoch': 1.57}
{'loss': 0.1176, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1182, 'learning_rate': 2.277777777777778e-05, 'epoch': 1.63}
{'loss': 0.0852, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.0241, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.7}
{'loss': 0.0602, 'learning_rate': 2.111111111111111e-05, 'epoch': 1.73}
{'loss': 0.0373, 'learning_rate': 2.0555555555555555e-05, 'epoch': 1.77}
{'loss': 0.1159, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.0333, 'learning_rate': 1.9444444444444445e-05, 'epoch': 1.83}
{'loss': 0.1271, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.114, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.9}
{'loss': 0.0325, 'learning_rate': 1.777777777777778e-05, 'epoch': 1.93}
{'loss': 0.0417, 'learning_rate': 1.7222222222222224e-05, 'epoch': 1.97}
{'loss': 0.0474, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.019, 'learning_rate': 1.6111111111111115e-05, 'epoch': 2.03}
{'loss': 0.0445, 'learning_rate': 1.5555555555555555e-05, 'epoch': 2.07}
{'loss': 0.0203, 'learning_rate': 1.5e-05, 'epoch': 2.1}
{'loss': 0.0286, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0224, 'learning_rate': 1.388888888888889e-05, 'epoch': 2.17}
{'loss': 0.0157, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}
{'loss': 0.045, 'learning_rate': 1.2777777777777777e-05, 'epoch': 2.23}
{'loss': 0.0317, 'learning_rate': 1.2222222222222222e-05, 'epoch': 2.27}
{'loss': 0.0385, 'learning_rate': 1.1666666666666668e-05, 'epoch': 2.3}
{'loss': 0.0314, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.06, 'learning_rate': 1.0555555555555555e-05, 'epoch': 2.37}
{'loss': 0.0548, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0434, 'learning_rate': 9.444444444444445e-06, 'epoch': 2.43}
{'loss': 0.0812, 'learning_rate': 8.88888888888889e-06, 'epoch': 2.47}
{'loss': 0.0465, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0106, 'learning_rate': 7.777777777777777e-06, 'epoch': 2.53}
{'loss': 0.0779, 'learning_rate': 7.222222222222222e-06, 'epoch': 2.57}
{'loss': 0.0173, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}
{'loss': 0.0226, 'learning_rate': 6.111111111111111e-06, 'epoch': 2.63}
{'loss': 0.0961, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0443, 'learning_rate': 5e-06, 'epoch': 2.7}
{'loss': 0.0256, 'learning_rate': 4.444444444444445e-06, 'epoch': 2.73}
{'loss': 0.0401, 'learning_rate': 3.888888888888889e-06, 'epoch': 2.77}
{'loss': 0.0298, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0418, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.83}
{'loss': 0.0407, 'learning_rate': 2.2222222222222225e-06, 'epoch': 2.87}
{'loss': 0.0148, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.9}
{'loss': 0.0052, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'loss': 0.0233, 'learning_rate': 5.555555555555556e-07, 'epoch': 2.97}
{'loss': 0.0706, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 879.6938, 'train_samples_per_second': 3.253, 'train_steps_per_second': 0.409, 'train_loss': 0.11710685870817138, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-240/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-360/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-120/optimizer.pt
validate!
last validate 0.
INFO 11-24 09:33:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-120', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-120', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:34:06 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_30_40_0.9_0.35_125_3 epoch 1

------------------------------------------------

0.654

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_30_40_0.9_0.35_125_3/generated_contents/1
INFO 11-24 09:34:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-240', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-240', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:35:13 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_30_40_0.9_0.35_125_3 epoch 2

------------------------------------------------

0.671

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_30_40_0.9_0.35_125_3/generated_contents/2
INFO 11-24 09:35:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-360', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-360', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:36:15 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_30_40_0.9_0.35_125_3 epoch 3

------------------------------------------------

0.615

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_30_40_0.9_0.35_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-120
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-240
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_30_40_0.9_0.35_125_3/checkpoint-360
searching parameters: NI_task1554_50_20_50_1.0_0.35_130_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_50_20_50_1.0_0.35_130_5
/home/cyzhao/NI_task1554_exp_1/NI_task1554_50_20_50_1.0_0.35_130_5/config.json
generate_and_write_inputs!
INFO 11-24 09:37:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:37:19 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 09:50:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:50:49 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 775
expected_example_num: 1000
selection_ratio: 0.775
finetune_deepseek!
{'loss': 2.5301, 'learning_rate': 4.958762886597938e-05, 'epoch': 0.04}
{'loss': 0.7357, 'learning_rate': 4.9175257731958763e-05, 'epoch': 0.08}
{'loss': 0.3066, 'learning_rate': 4.8762886597938144e-05, 'epoch': 0.12}
{'loss': 0.8613, 'learning_rate': 4.835051546391753e-05, 'epoch': 0.16}
{'loss': 1.1181, 'learning_rate': 4.793814432989691e-05, 'epoch': 0.21}
{'loss': 0.466, 'learning_rate': 4.7525773195876285e-05, 'epoch': 0.25}
{'loss': 0.3057, 'learning_rate': 4.711340206185567e-05, 'epoch': 0.29}
{'loss': 0.2032, 'learning_rate': 4.670103092783505e-05, 'epoch': 0.33}
{'loss': 0.332, 'learning_rate': 4.628865979381444e-05, 'epoch': 0.37}
{'loss': 0.2348, 'learning_rate': 4.5876288659793814e-05, 'epoch': 0.41}
{'loss': 0.5042, 'learning_rate': 4.5463917525773195e-05, 'epoch': 0.45}
{'loss': 0.3786, 'learning_rate': 4.505154639175258e-05, 'epoch': 0.49}
{'loss': 0.6306, 'learning_rate': 4.463917525773196e-05, 'epoch': 0.54}
{'loss': 0.4751, 'learning_rate': 4.422680412371134e-05, 'epoch': 0.58}
{'loss': 0.2909, 'learning_rate': 4.3814432989690723e-05, 'epoch': 0.62}
{'loss': 0.6153, 'learning_rate': 4.3402061855670104e-05, 'epoch': 0.66}
{'loss': 0.3412, 'learning_rate': 4.298969072164949e-05, 'epoch': 0.7}
{'loss': 0.2208, 'learning_rate': 4.2577319587628865e-05, 'epoch': 0.74}
{'loss': 0.2981, 'learning_rate': 4.216494845360825e-05, 'epoch': 0.78}
{'loss': 0.1275, 'learning_rate': 4.175257731958763e-05, 'epoch': 0.82}
{'loss': 0.3805, 'learning_rate': 4.1340206185567006e-05, 'epoch': 0.87}
{'loss': 0.2935, 'learning_rate': 4.0927835051546394e-05, 'epoch': 0.91}
{'loss': 0.5664, 'learning_rate': 4.0515463917525774e-05, 'epoch': 0.95}
{'loss': 0.4028, 'learning_rate': 4.010309278350516e-05, 'epoch': 0.99}
{'loss': 0.2789, 'learning_rate': 3.9690721649484535e-05, 'epoch': 1.03}
{'loss': 0.3389, 'learning_rate': 3.9278350515463916e-05, 'epoch': 1.07}
{'loss': 0.3514, 'learning_rate': 3.88659793814433e-05, 'epoch': 1.11}
{'loss': 0.1188, 'learning_rate': 3.8453608247422684e-05, 'epoch': 1.15}
{'loss': 0.1542, 'learning_rate': 3.8041237113402064e-05, 'epoch': 1.2}
{'loss': 0.2733, 'learning_rate': 3.7628865979381445e-05, 'epoch': 1.24}
{'loss': 0.245, 'learning_rate': 3.7216494845360825e-05, 'epoch': 1.28}
{'loss': 0.282, 'learning_rate': 3.680412371134021e-05, 'epoch': 1.32}
{'loss': 0.3743, 'learning_rate': 3.6391752577319586e-05, 'epoch': 1.36}
{'loss': 0.2958, 'learning_rate': 3.597938144329897e-05, 'epoch': 1.4}
{'loss': 0.2046, 'learning_rate': 3.5567010309278354e-05, 'epoch': 1.44}
{'loss': 0.1413, 'learning_rate': 3.515463917525773e-05, 'epoch': 1.48}
{'loss': 0.321, 'learning_rate': 3.4742268041237115e-05, 'epoch': 1.53}
{'loss': 0.1142, 'learning_rate': 3.4329896907216495e-05, 'epoch': 1.57}
{'loss': 0.1945, 'learning_rate': 3.391752577319588e-05, 'epoch': 1.61}
{'loss': 0.1845, 'learning_rate': 3.3505154639175256e-05, 'epoch': 1.65}
{'loss': 0.2014, 'learning_rate': 3.309278350515464e-05, 'epoch': 1.69}
{'loss': 0.1777, 'learning_rate': 3.2680412371134024e-05, 'epoch': 1.73}
{'loss': 0.1749, 'learning_rate': 3.2268041237113405e-05, 'epoch': 1.77}
{'loss': 0.3259, 'learning_rate': 3.1855670103092785e-05, 'epoch': 1.81}
{'loss': 0.2569, 'learning_rate': 3.1443298969072166e-05, 'epoch': 1.86}
{'loss': 0.1653, 'learning_rate': 3.1030927835051546e-05, 'epoch': 1.9}
{'loss': 0.1207, 'learning_rate': 3.0618556701030933e-05, 'epoch': 1.94}
{'loss': 0.1803, 'learning_rate': 3.020618556701031e-05, 'epoch': 1.98}
{'loss': 0.2387, 'learning_rate': 2.979381443298969e-05, 'epoch': 2.02}
{'loss': 0.1011, 'learning_rate': 2.9381443298969075e-05, 'epoch': 2.06}
{'loss': 0.1916, 'learning_rate': 2.896907216494846e-05, 'epoch': 2.1}
{'loss': 0.0916, 'learning_rate': 2.8556701030927836e-05, 'epoch': 2.14}
{'loss': 0.1671, 'learning_rate': 2.8144329896907216e-05, 'epoch': 2.19}
{'loss': 0.1672, 'learning_rate': 2.77319587628866e-05, 'epoch': 2.23}
{'loss': 0.0684, 'learning_rate': 2.7319587628865977e-05, 'epoch': 2.27}
{'loss': 0.133, 'learning_rate': 2.690721649484536e-05, 'epoch': 2.31}
{'loss': 0.1111, 'learning_rate': 2.6494845360824745e-05, 'epoch': 2.35}
{'loss': 0.1397, 'learning_rate': 2.6082474226804126e-05, 'epoch': 2.39}
{'loss': 0.0878, 'learning_rate': 2.5670103092783503e-05, 'epoch': 2.43}
{'loss': 0.061, 'learning_rate': 2.5257731958762887e-05, 'epoch': 2.47}
{'loss': 0.078, 'learning_rate': 2.484536082474227e-05, 'epoch': 2.52}
{'loss': 0.1521, 'learning_rate': 2.443298969072165e-05, 'epoch': 2.56}
{'loss': 0.1307, 'learning_rate': 2.402061855670103e-05, 'epoch': 2.6}
{'loss': 0.2079, 'learning_rate': 2.3608247422680412e-05, 'epoch': 2.64}
{'loss': 0.2525, 'learning_rate': 2.3195876288659796e-05, 'epoch': 2.68}
{'loss': 0.0988, 'learning_rate': 2.2783505154639176e-05, 'epoch': 2.72}
{'loss': 0.1543, 'learning_rate': 2.237113402061856e-05, 'epoch': 2.76}
{'loss': 0.0974, 'learning_rate': 2.1958762886597937e-05, 'epoch': 2.8}
{'loss': 0.0584, 'learning_rate': 2.154639175257732e-05, 'epoch': 2.85}
{'loss': 0.1614, 'learning_rate': 2.1134020618556702e-05, 'epoch': 2.89}
{'loss': 0.1172, 'learning_rate': 2.0721649484536086e-05, 'epoch': 2.93}
{'loss': 0.078, 'learning_rate': 2.0309278350515466e-05, 'epoch': 2.97}
{'loss': 0.096, 'learning_rate': 1.9896907216494843e-05, 'epoch': 3.01}
{'loss': 0.0767, 'learning_rate': 1.9484536082474227e-05, 'epoch': 3.05}
{'loss': 0.0653, 'learning_rate': 1.9072164948453608e-05, 'epoch': 3.09}
{'loss': 0.0606, 'learning_rate': 1.865979381443299e-05, 'epoch': 3.13}
{'loss': 0.0434, 'learning_rate': 1.8247422680412372e-05, 'epoch': 3.18}
{'loss': 0.0875, 'learning_rate': 1.7835051546391753e-05, 'epoch': 3.22}
{'loss': 0.0493, 'learning_rate': 1.7422680412371133e-05, 'epoch': 3.26}
{'loss': 0.1035, 'learning_rate': 1.7010309278350517e-05, 'epoch': 3.3}
{'loss': 0.0557, 'learning_rate': 1.6597938144329898e-05, 'epoch': 3.34}
{'loss': 0.051, 'learning_rate': 1.618556701030928e-05, 'epoch': 3.38}
{'loss': 0.0386, 'learning_rate': 1.577319587628866e-05, 'epoch': 3.42}
{'loss': 0.0496, 'learning_rate': 1.5360824742268042e-05, 'epoch': 3.46}
{'loss': 0.0638, 'learning_rate': 1.4948453608247423e-05, 'epoch': 3.51}
{'loss': 0.0611, 'learning_rate': 1.4536082474226805e-05, 'epoch': 3.55}
{'loss': 0.0508, 'learning_rate': 1.4123711340206186e-05, 'epoch': 3.59}
{'loss': 0.0344, 'learning_rate': 1.371134020618557e-05, 'epoch': 3.63}
{'loss': 0.0855, 'learning_rate': 1.3298969072164948e-05, 'epoch': 3.67}
{'loss': 0.0346, 'learning_rate': 1.2886597938144329e-05, 'epoch': 3.71}
{'loss': 0.046, 'learning_rate': 1.2474226804123713e-05, 'epoch': 3.75}
{'loss': 0.0242, 'learning_rate': 1.2061855670103093e-05, 'epoch': 3.79}
{'loss': 0.0249, 'learning_rate': 1.1649484536082475e-05, 'epoch': 3.84}
{'loss': 0.0282, 'learning_rate': 1.1237113402061856e-05, 'epoch': 3.88}
{'loss': 0.1273, 'learning_rate': 1.0824742268041238e-05, 'epoch': 3.92}
{'loss': 0.0363, 'learning_rate': 1.041237113402062e-05, 'epoch': 3.96}
{'loss': 0.0539, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0065, 'learning_rate': 9.587628865979383e-06, 'epoch': 4.04}
{'loss': 0.0168, 'learning_rate': 9.175257731958762e-06, 'epoch': 4.08}
{'loss': 0.0118, 'learning_rate': 8.762886597938144e-06, 'epoch': 4.12}
{'loss': 0.0036, 'learning_rate': 8.350515463917526e-06, 'epoch': 4.16}
{'loss': 0.0224, 'learning_rate': 7.938144329896907e-06, 'epoch': 4.21}
{'loss': 0.0459, 'learning_rate': 7.525773195876289e-06, 'epoch': 4.25}
{'loss': 0.0042, 'learning_rate': 7.11340206185567e-06, 'epoch': 4.29}
{'loss': 0.0247, 'learning_rate': 6.701030927835052e-06, 'epoch': 4.33}
{'loss': 0.0049, 'learning_rate': 6.288659793814433e-06, 'epoch': 4.37}
{'loss': 0.0183, 'learning_rate': 5.876288659793814e-06, 'epoch': 4.41}
{'loss': 0.0341, 'learning_rate': 5.4639175257731965e-06, 'epoch': 4.45}
{'loss': 0.0269, 'learning_rate': 5.051546391752578e-06, 'epoch': 4.49}
{'loss': 0.0144, 'learning_rate': 4.639175257731959e-06, 'epoch': 4.54}
{'loss': 0.027, 'learning_rate': 4.2268041237113405e-06, 'epoch': 4.58}
{'loss': 0.0344, 'learning_rate': 3.8144329896907223e-06, 'epoch': 4.62}
{'loss': 0.0326, 'learning_rate': 3.402061855670103e-06, 'epoch': 4.66}
{'loss': 0.0124, 'learning_rate': 2.989690721649485e-06, 'epoch': 4.7}
{'loss': 0.0036, 'learning_rate': 2.577319587628866e-06, 'epoch': 4.74}
{'loss': 0.0073, 'learning_rate': 2.1649484536082473e-06, 'epoch': 4.78}
{'loss': 0.0166, 'learning_rate': 1.752577319587629e-06, 'epoch': 4.82}
{'loss': 0.0039, 'learning_rate': 1.3402061855670102e-06, 'epoch': 4.87}
{'loss': 0.0136, 'learning_rate': 9.278350515463918e-07, 'epoch': 4.91}
{'loss': 0.0075, 'learning_rate': 5.154639175257732e-07, 'epoch': 4.95}
{'loss': 0.002, 'learning_rate': 1.0309278350515465e-07, 'epoch': 4.99}
{'train_runtime': 1042.0983, 'train_samples_per_second': 3.718, 'train_steps_per_second': 0.465, 'train_loss': 0.1905175314870538, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-291/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-194/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-485/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-388/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-97/optimizer.pt
validate!
last validate 0.
INFO 11-24 10:14:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-97', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-97', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:14:17 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_20_50_1.0_0.35_130_5 epoch 1

------------------------------------------------

0.5

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_20_50_1.0_0.35_130_5/generated_contents/1
INFO 11-24 10:15:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-194', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-194', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:15:18 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_20_50_1.0_0.35_130_5 epoch 2

------------------------------------------------

0.597

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_20_50_1.0_0.35_130_5/generated_contents/2
INFO 11-24 10:16:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-291', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-291', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:16:18 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_20_50_1.0_0.35_130_5 epoch 3

------------------------------------------------

0.513

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_20_50_1.0_0.35_130_5/generated_contents/3
INFO 11-24 10:17:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-388', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-388', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:17:20 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_20_50_1.0_0.35_130_5 epoch 4

------------------------------------------------

0.606

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_20_50_1.0_0.35_130_5/generated_contents/4
INFO 11-24 10:18:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-485', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-485', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:18:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_50_20_50_1.0_0.35_130_5 epoch 5

------------------------------------------------

0.643

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_50_20_50_1.0_0.35_130_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-97
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-194
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-291
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-388
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_50_20_50_1.0_0.35_130_5/checkpoint-485
searching parameters: NI_task1554_10_20_40_0.8_0.35_115_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_10_20_40_0.8_0.35_115_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_10_20_40_0.8_0.35_115_4/config.json
generate_and_write_inputs!
INFO 11-24 10:19:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:19:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 10:22:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:22:45 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 167
expected_example_num: 200
selection_ratio: 0.835
finetune_deepseek!
{'loss': 1.6674, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.19}
{'loss': 1.2761, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.38}
{'loss': 0.8121, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.57}
{'loss': 0.4051, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.76}
{'loss': 0.4808, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.95}
{'loss': 0.2373, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.14}
{'loss': 0.4447, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.33}
{'loss': 0.3214, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.52}
{'loss': 0.3824, 'learning_rate': 2.857142857142857e-05, 'epoch': 1.71}
{'loss': 0.1982, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.9}
{'loss': 0.2505, 'learning_rate': 2.380952380952381e-05, 'epoch': 2.1}
{'loss': 0.1612, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.29}
{'loss': 0.2614, 'learning_rate': 1.9047619047619046e-05, 'epoch': 2.48}
{'loss': 0.1883, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.67}
{'loss': 0.2784, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.86}
{'loss': 0.1681, 'learning_rate': 1.1904761904761905e-05, 'epoch': 3.05}
{'loss': 0.2189, 'learning_rate': 9.523809523809523e-06, 'epoch': 3.24}
{'loss': 0.1565, 'learning_rate': 7.142857142857143e-06, 'epoch': 3.43}
{'loss': 0.1901, 'learning_rate': 4.7619047619047615e-06, 'epoch': 3.62}
{'loss': 0.1964, 'learning_rate': 2.3809523809523808e-06, 'epoch': 3.81}
{'loss': 0.1019, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 331.4275, 'train_samples_per_second': 2.016, 'train_steps_per_second': 0.253, 'train_loss': 0.3998756018422899, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-84/optimizer.pt
validate!
last validate 0.
INFO 11-24 10:29:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:30:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_10_20_40_0.8_0.35_115_4 epoch 1

------------------------------------------------

0.498

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_10_20_40_0.8_0.35_115_4/generated_contents/1
INFO 11-24 10:30:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:31:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_10_20_40_0.8_0.35_115_4 epoch 2

------------------------------------------------

0.502

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_10_20_40_0.8_0.35_115_4/generated_contents/2
INFO 11-24 10:31:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:32:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_10_20_40_0.8_0.35_115_4 epoch 3

------------------------------------------------

0.502

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_10_20_40_0.8_0.35_115_4/generated_contents/3
INFO 11-24 10:32:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-84', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-84', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:33:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_10_20_40_0.8_0.35_115_4 epoch 4

------------------------------------------------

0.502

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_10_20_40_0.8_0.35_115_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-21
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-63
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_10_20_40_0.8_0.35_115_4/checkpoint-84
searching parameters: NI_task1554_40_40_45_0.9_0.4_130_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_45_0.9_0.4_130_4
/home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_45_0.9_0.4_130_4/config.json
generate_and_write_inputs!
INFO 11-24 10:34:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:34:16 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
WARNING 11-24 10:51:20 scheduler.py:146] Input prompt (4108 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-24 10:52:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 10:52:31 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 1234
expected_example_num: 1600
selection_ratio: 0.77125
finetune_deepseek!
{'loss': 0.6756, 'learning_rate': 4.967741935483871e-05, 'epoch': 0.03}
{'loss': 0.3212, 'learning_rate': 4.935483870967742e-05, 'epoch': 0.05}
{'loss': 0.1975, 'learning_rate': 4.903225806451613e-05, 'epoch': 0.08}
{'loss': 0.1158, 'learning_rate': 4.870967741935484e-05, 'epoch': 0.1}
{'loss': 0.3519, 'learning_rate': 4.8387096774193554e-05, 'epoch': 0.13}
{'loss': 0.8389, 'learning_rate': 4.806451612903226e-05, 'epoch': 0.15}
{'loss': 0.1789, 'learning_rate': 4.774193548387097e-05, 'epoch': 0.18}
{'loss': 0.3145, 'learning_rate': 4.741935483870968e-05, 'epoch': 0.21}
{'loss': 0.248, 'learning_rate': 4.7096774193548385e-05, 'epoch': 0.23}
{'loss': 0.2042, 'learning_rate': 4.67741935483871e-05, 'epoch': 0.26}
{'loss': 0.1519, 'learning_rate': 4.645161290322581e-05, 'epoch': 0.28}
{'loss': 0.3188, 'learning_rate': 4.612903225806452e-05, 'epoch': 0.31}
{'loss': 0.2483, 'learning_rate': 4.580645161290323e-05, 'epoch': 0.34}
{'loss': 0.1106, 'learning_rate': 4.548387096774194e-05, 'epoch': 0.36}
{'loss': 0.2896, 'learning_rate': 4.516129032258064e-05, 'epoch': 0.39}
{'loss': 0.0771, 'learning_rate': 4.4838709677419356e-05, 'epoch': 0.41}
{'loss': 0.1364, 'learning_rate': 4.451612903225807e-05, 'epoch': 0.44}
{'loss': 0.0996, 'learning_rate': 4.4193548387096775e-05, 'epoch': 0.46}
{'loss': 0.157, 'learning_rate': 4.387096774193549e-05, 'epoch': 0.49}
{'loss': 0.4773, 'learning_rate': 4.3548387096774194e-05, 'epoch': 0.52}
{'loss': 0.1622, 'learning_rate': 4.322580645161291e-05, 'epoch': 0.54}
{'loss': 0.1935, 'learning_rate': 4.2903225806451614e-05, 'epoch': 0.57}
{'loss': 0.3703, 'learning_rate': 4.258064516129032e-05, 'epoch': 0.59}
{'loss': 0.1316, 'learning_rate': 4.225806451612904e-05, 'epoch': 0.62}
{'loss': 0.1356, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.65}
{'loss': 0.1811, 'learning_rate': 4.161290322580645e-05, 'epoch': 0.67}
{'loss': 0.1312, 'learning_rate': 4.1290322580645165e-05, 'epoch': 0.7}
{'loss': 0.0656, 'learning_rate': 4.096774193548387e-05, 'epoch': 0.72}
{'loss': 0.0686, 'learning_rate': 4.0645161290322584e-05, 'epoch': 0.75}
{'loss': 0.3885, 'learning_rate': 4.032258064516129e-05, 'epoch': 0.77}
{'loss': 0.1515, 'learning_rate': 4e-05, 'epoch': 0.8}
{'loss': 0.1251, 'learning_rate': 3.9677419354838716e-05, 'epoch': 0.83}
{'loss': 0.0347, 'learning_rate': 3.935483870967742e-05, 'epoch': 0.85}
{'loss': 0.3542, 'learning_rate': 3.903225806451613e-05, 'epoch': 0.88}
{'loss': 0.1668, 'learning_rate': 3.870967741935484e-05, 'epoch': 0.9}
{'loss': 0.2411, 'learning_rate': 3.838709677419355e-05, 'epoch': 0.93}
{'loss': 0.2374, 'learning_rate': 3.8064516129032254e-05, 'epoch': 0.95}
{'loss': 0.1264, 'learning_rate': 3.7741935483870974e-05, 'epoch': 0.98}
{'loss': 0.0897, 'learning_rate': 3.741935483870968e-05, 'epoch': 1.01}
{'loss': 0.1812, 'learning_rate': 3.7096774193548386e-05, 'epoch': 1.03}
{'loss': 0.0817, 'learning_rate': 3.67741935483871e-05, 'epoch': 1.06}
{'loss': 0.035, 'learning_rate': 3.6451612903225805e-05, 'epoch': 1.08}
{'loss': 0.2369, 'learning_rate': 3.612903225806452e-05, 'epoch': 1.11}
{'loss': 0.0879, 'learning_rate': 3.580645161290323e-05, 'epoch': 1.14}
{'loss': 0.0673, 'learning_rate': 3.548387096774194e-05, 'epoch': 1.16}
{'loss': 0.2009, 'learning_rate': 3.516129032258065e-05, 'epoch': 1.19}
{'loss': 0.1063, 'learning_rate': 3.483870967741936e-05, 'epoch': 1.21}
{'loss': 0.0286, 'learning_rate': 3.451612903225806e-05, 'epoch': 1.24}
{'loss': 0.0752, 'learning_rate': 3.4193548387096776e-05, 'epoch': 1.26}
{'loss': 0.0854, 'learning_rate': 3.387096774193548e-05, 'epoch': 1.29}
{'loss': 0.194, 'learning_rate': 3.3548387096774195e-05, 'epoch': 1.32}
{'loss': 0.1824, 'learning_rate': 3.322580645161291e-05, 'epoch': 1.34}
{'loss': 0.1504, 'learning_rate': 3.2903225806451614e-05, 'epoch': 1.37}
{'loss': 0.0736, 'learning_rate': 3.258064516129033e-05, 'epoch': 1.39}
{'loss': 0.0891, 'learning_rate': 3.2258064516129034e-05, 'epoch': 1.42}
{'loss': 0.1342, 'learning_rate': 3.193548387096774e-05, 'epoch': 1.45}
{'loss': 0.1227, 'learning_rate': 3.161290322580645e-05, 'epoch': 1.47}
{'loss': 0.1069, 'learning_rate': 3.1290322580645166e-05, 'epoch': 1.5}
{'loss': 0.094, 'learning_rate': 3.096774193548387e-05, 'epoch': 1.52}
{'loss': 0.0935, 'learning_rate': 3.0645161290322585e-05, 'epoch': 1.55}
{'loss': 0.1711, 'learning_rate': 3.032258064516129e-05, 'epoch': 1.57}
{'loss': 0.1656, 'learning_rate': 3e-05, 'epoch': 1.6}
{'loss': 0.0612, 'learning_rate': 2.967741935483871e-05, 'epoch': 1.63}
{'loss': 0.093, 'learning_rate': 2.9354838709677417e-05, 'epoch': 1.65}
{'loss': 0.1315, 'learning_rate': 2.9032258064516133e-05, 'epoch': 1.68}
{'loss': 0.0619, 'learning_rate': 2.8709677419354843e-05, 'epoch': 1.7}
{'loss': 0.0191, 'learning_rate': 2.838709677419355e-05, 'epoch': 1.73}
{'loss': 0.0689, 'learning_rate': 2.806451612903226e-05, 'epoch': 1.75}
{'loss': 0.0516, 'learning_rate': 2.7741935483870968e-05, 'epoch': 1.78}
{'loss': 0.1591, 'learning_rate': 2.7419354838709678e-05, 'epoch': 1.81}
{'loss': 0.0521, 'learning_rate': 2.709677419354839e-05, 'epoch': 1.83}
{'loss': 0.0892, 'learning_rate': 2.67741935483871e-05, 'epoch': 1.86}
{'loss': 0.1005, 'learning_rate': 2.645161290322581e-05, 'epoch': 1.88}
{'loss': 0.2223, 'learning_rate': 2.6129032258064516e-05, 'epoch': 1.91}
{'loss': 0.1381, 'learning_rate': 2.5806451612903226e-05, 'epoch': 1.94}
{'loss': 0.1131, 'learning_rate': 2.5483870967741935e-05, 'epoch': 1.96}
{'loss': 0.1066, 'learning_rate': 2.5161290322580645e-05, 'epoch': 1.99}
{'loss': 0.164, 'learning_rate': 2.4838709677419354e-05, 'epoch': 2.01}
{'loss': 0.0681, 'learning_rate': 2.4516129032258064e-05, 'epoch': 2.04}
{'loss': 0.0348, 'learning_rate': 2.4193548387096777e-05, 'epoch': 2.06}
{'loss': 0.0411, 'learning_rate': 2.3870967741935486e-05, 'epoch': 2.09}
{'loss': 0.1575, 'learning_rate': 2.3548387096774193e-05, 'epoch': 2.12}
{'loss': 0.0389, 'learning_rate': 2.3225806451612906e-05, 'epoch': 2.14}
{'loss': 0.0196, 'learning_rate': 2.2903225806451615e-05, 'epoch': 2.17}
{'loss': 0.0977, 'learning_rate': 2.258064516129032e-05, 'epoch': 2.19}
{'loss': 0.0716, 'learning_rate': 2.2258064516129034e-05, 'epoch': 2.22}
{'loss': 0.0327, 'learning_rate': 2.1935483870967744e-05, 'epoch': 2.25}
{'loss': 0.1695, 'learning_rate': 2.1612903225806454e-05, 'epoch': 2.27}
{'loss': 0.0226, 'learning_rate': 2.129032258064516e-05, 'epoch': 2.3}
{'loss': 0.1372, 'learning_rate': 2.0967741935483873e-05, 'epoch': 2.32}
{'loss': 0.0774, 'learning_rate': 2.0645161290322582e-05, 'epoch': 2.35}
{'loss': 0.0583, 'learning_rate': 2.0322580645161292e-05, 'epoch': 2.37}
{'loss': 0.0418, 'learning_rate': 2e-05, 'epoch': 2.4}
{'loss': 0.1071, 'learning_rate': 1.967741935483871e-05, 'epoch': 2.43}
{'loss': 0.0498, 'learning_rate': 1.935483870967742e-05, 'epoch': 2.45}
{'loss': 0.0468, 'learning_rate': 1.9032258064516127e-05, 'epoch': 2.48}
{'loss': 0.0535, 'learning_rate': 1.870967741935484e-05, 'epoch': 2.5}
{'loss': 0.0432, 'learning_rate': 1.838709677419355e-05, 'epoch': 2.53}
{'loss': 0.0283, 'learning_rate': 1.806451612903226e-05, 'epoch': 2.55}
{'loss': 0.0419, 'learning_rate': 1.774193548387097e-05, 'epoch': 2.58}
{'loss': 0.0394, 'learning_rate': 1.741935483870968e-05, 'epoch': 2.61}
{'loss': 0.0586, 'learning_rate': 1.7096774193548388e-05, 'epoch': 2.63}
{'loss': 0.0158, 'learning_rate': 1.6774193548387098e-05, 'epoch': 2.66}
{'loss': 0.0643, 'learning_rate': 1.6451612903225807e-05, 'epoch': 2.68}
{'loss': 0.0065, 'learning_rate': 1.6129032258064517e-05, 'epoch': 2.71}
{'loss': 0.0249, 'learning_rate': 1.5806451612903226e-05, 'epoch': 2.74}
{'loss': 0.0328, 'learning_rate': 1.5483870967741936e-05, 'epoch': 2.76}
{'loss': 0.0519, 'learning_rate': 1.5161290322580646e-05, 'epoch': 2.79}
{'loss': 0.0679, 'learning_rate': 1.4838709677419355e-05, 'epoch': 2.81}
{'loss': 0.0125, 'learning_rate': 1.4516129032258066e-05, 'epoch': 2.84}
{'loss': 0.0227, 'learning_rate': 1.4193548387096774e-05, 'epoch': 2.86}
{'loss': 0.1323, 'learning_rate': 1.3870967741935484e-05, 'epoch': 2.89}
{'loss': 0.057, 'learning_rate': 1.3548387096774195e-05, 'epoch': 2.92}
{'loss': 0.0373, 'learning_rate': 1.3225806451612905e-05, 'epoch': 2.94}
{'loss': 0.0811, 'learning_rate': 1.2903225806451613e-05, 'epoch': 2.97}
{'loss': 0.0091, 'learning_rate': 1.2580645161290322e-05, 'epoch': 2.99}
{'loss': 0.0323, 'learning_rate': 1.2258064516129032e-05, 'epoch': 3.02}
{'loss': 0.0575, 'learning_rate': 1.1935483870967743e-05, 'epoch': 3.05}
{'loss': 0.0176, 'learning_rate': 1.1612903225806453e-05, 'epoch': 3.07}
{'loss': 0.0101, 'learning_rate': 1.129032258064516e-05, 'epoch': 3.1}
{'loss': 0.0091, 'learning_rate': 1.0967741935483872e-05, 'epoch': 3.12}
{'loss': 0.055, 'learning_rate': 1.064516129032258e-05, 'epoch': 3.15}
{'loss': 0.0072, 'learning_rate': 1.0322580645161291e-05, 'epoch': 3.17}
{'loss': 0.0235, 'learning_rate': 1e-05, 'epoch': 3.2}
{'loss': 0.0061, 'learning_rate': 9.67741935483871e-06, 'epoch': 3.23}
{'loss': 0.0066, 'learning_rate': 9.35483870967742e-06, 'epoch': 3.25}
{'loss': 0.0053, 'learning_rate': 9.03225806451613e-06, 'epoch': 3.28}
{'loss': 0.0303, 'learning_rate': 8.70967741935484e-06, 'epoch': 3.3}
{'loss': 0.0169, 'learning_rate': 8.387096774193549e-06, 'epoch': 3.33}
{'loss': 0.0356, 'learning_rate': 8.064516129032258e-06, 'epoch': 3.35}
{'loss': 0.0277, 'learning_rate': 7.741935483870968e-06, 'epoch': 3.38}
{'loss': 0.0162, 'learning_rate': 7.419354838709678e-06, 'epoch': 3.41}
{'loss': 0.0088, 'learning_rate': 7.096774193548387e-06, 'epoch': 3.43}
{'loss': 0.0491, 'learning_rate': 6.774193548387098e-06, 'epoch': 3.46}
{'loss': 0.019, 'learning_rate': 6.451612903225806e-06, 'epoch': 3.48}
{'loss': 0.0099, 'learning_rate': 6.129032258064516e-06, 'epoch': 3.51}
{'loss': 0.0127, 'learning_rate': 5.806451612903226e-06, 'epoch': 3.54}
{'loss': 0.0383, 'learning_rate': 5.483870967741936e-06, 'epoch': 3.56}
{'loss': 0.0616, 'learning_rate': 5.161290322580646e-06, 'epoch': 3.59}
{'loss': 0.0346, 'learning_rate': 4.838709677419355e-06, 'epoch': 3.61}
{'loss': 0.0165, 'learning_rate': 4.516129032258065e-06, 'epoch': 3.64}
{'loss': 0.0053, 'learning_rate': 4.193548387096774e-06, 'epoch': 3.66}
{'loss': 0.0094, 'learning_rate': 3.870967741935484e-06, 'epoch': 3.69}
{'loss': 0.0071, 'learning_rate': 3.5483870967741936e-06, 'epoch': 3.72}
{'loss': 0.0546, 'learning_rate': 3.225806451612903e-06, 'epoch': 3.74}
{'loss': 0.0048, 'learning_rate': 2.903225806451613e-06, 'epoch': 3.77}
{'loss': 0.0095, 'learning_rate': 2.580645161290323e-06, 'epoch': 3.79}
{'loss': 0.0664, 'learning_rate': 2.2580645161290324e-06, 'epoch': 3.82}
{'loss': 0.0086, 'learning_rate': 1.935483870967742e-06, 'epoch': 3.85}
{'loss': 0.0197, 'learning_rate': 1.6129032258064516e-06, 'epoch': 3.87}
{'loss': 0.0484, 'learning_rate': 1.2903225806451614e-06, 'epoch': 3.9}
{'loss': 0.047, 'learning_rate': 9.67741935483871e-07, 'epoch': 3.92}
{'loss': 0.032, 'learning_rate': 6.451612903225807e-07, 'epoch': 3.95}
{'loss': 0.0132, 'learning_rate': 3.2258064516129035e-07, 'epoch': 3.97}
{'loss': 0.0261, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 1611.8501, 'train_samples_per_second': 3.062, 'train_steps_per_second': 0.385, 'train_loss': 0.10566312910448158, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-155/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-465/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-310/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-620/optimizer.pt
validate!
last validate 0.
INFO 11-24 11:38:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-155', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-155', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:38:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_45_0.9_0.4_130_4 epoch 1

------------------------------------------------

0.5

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_45_0.9_0.4_130_4/generated_contents/1
INFO 11-24 11:39:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-310', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-310', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:39:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_45_0.9_0.4_130_4 epoch 2

------------------------------------------------

0.515

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_45_0.9_0.4_130_4/generated_contents/2
INFO 11-24 11:40:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-465', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-465', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:40:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_45_0.9_0.4_130_4 epoch 3

------------------------------------------------

0.51

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_45_0.9_0.4_130_4/generated_contents/3
INFO 11-24 11:41:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-620', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-620', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:42:00 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1554_40_40_45_0.9_0.4_130_4 epoch 4

------------------------------------------------

0.518

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/NI_task1554_40_40_45_0.9_0.4_130_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-155
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-310
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-465
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1554_40_40_45_0.9_0.4_130_4/checkpoint-620
{'generation_epochs': 50, 'generation_batch_size': 10, 'generation_top_k': 45, 'generation_temperature': 0.8, 'min_frequency': 0.4, 'min_input_length': 130, 'training_epochs': 5}
test best ckpt.
INFO 11-24 11:42:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task1554_exp_1', tokenizer='/data2/cyzhao/best_ckpt/NI_task1554_exp_1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:43:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task1554_exp_1

------------------------------------------------

0.741

------------------------------------------------


The best ckpt on test set gain 0.741
Genrated contents are stored in /home/cyzhao/NI_task1554_exp_1/best_ckpt_generated_content
