[2023-11-27 20:06:52,274] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_45_0.7_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_45_0.7_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_45_0.7_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 20:06:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:07:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 20:10:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:10:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 273
expected_example_num: 600
selection_ratio: 0.455
finetune_vicuna!
{'loss': 1.2904, 'learning_rate': 4.80952380952381e-05, 'epoch': 0.11}
{'loss': 0.4772, 'learning_rate': 4.6190476190476194e-05, 'epoch': 0.23}
{'loss': 0.5186, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.34}
{'loss': 0.4284, 'learning_rate': 4.2380952380952385e-05, 'epoch': 0.46}
{'loss': 0.5188, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4323, 'learning_rate': 3.857142857142858e-05, 'epoch': 0.69}
{'loss': 0.3108, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3378, 'learning_rate': 3.476190476190476e-05, 'epoch': 0.91}
{'loss': 0.1979, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.03}
{'loss': 0.138, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2152, 'learning_rate': 2.9047619047619052e-05, 'epoch': 1.26}
{'loss': 0.132, 'learning_rate': 2.714285714285714e-05, 'epoch': 1.37}
{'loss': 0.2153, 'learning_rate': 2.523809523809524e-05, 'epoch': 1.49}
{'loss': 0.1341, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1237, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1451, 'learning_rate': 1.9523809523809524e-05, 'epoch': 1.83}
{'loss': 0.1204, 'learning_rate': 1.761904761904762e-05, 'epoch': 1.94}
{'loss': 0.0582, 'learning_rate': 1.5714285714285715e-05, 'epoch': 2.06}
{'loss': 0.0706, 'learning_rate': 1.3809523809523811e-05, 'epoch': 2.17}
{'loss': 0.0569, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0584, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0326, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.51}
{'loss': 0.0504, 'learning_rate': 6.190476190476191e-06, 'epoch': 2.63}
{'loss': 0.0718, 'learning_rate': 4.285714285714286e-06, 'epoch': 2.74}
{'loss': 0.0612, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'loss': 0.0549, 'learning_rate': 4.761904761904763e-07, 'epoch': 2.97}
{'train_runtime': 291.4682, 'train_samples_per_second': 2.81, 'train_steps_per_second': 0.36, 'train_loss': 0.24074994708810535, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-35/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-105/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-70/optimizer.pt
validate!
last validate 0.
INFO 11-27 20:17:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-35', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-35', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:17:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.7_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_45_0.7_0.35_3_7/generated_contents/1
INFO 11-27 20:17:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-70', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-70', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:17:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.7_0.35_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_45_0.7_0.35_3_7/generated_contents/2
INFO 11-27 20:18:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-105', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-105', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:18:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.7_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_45_0.7_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-35
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-70
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.7_0.35_3_7/checkpoint-105
searching parameters: task121_10_20_50_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 20:18:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:18:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 20:19:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:20:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 93
expected_example_num: 200
selection_ratio: 0.465
finetune_vicuna!
{'loss': 1.6077, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5116, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.3535, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1482, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1836, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2076, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0771, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.083, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0631, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 197.2643, 'train_samples_per_second': 1.414, 'train_steps_per_second': 0.182, 'train_loss': 0.3594771358701918, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 20:24:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:24:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.5_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.35_3_7/generated_contents/1
INFO 11-27 20:24:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:25:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.5_0.35_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.35_3_7/generated_contents/2
INFO 11-27 20:25:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:25:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.5_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.35_3_7/checkpoint-36
searching parameters: task121_30_20_40_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.5_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 20:26:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:26:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 20:29:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:29:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 281
expected_example_num: 600
selection_ratio: 0.4683333333333333
finetune_vicuna!
{'loss': 1.276, 'learning_rate': 4.814814814814815e-05, 'epoch': 0.11}
{'loss': 0.5202, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.22}
{'loss': 0.3806, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.4706, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.3696, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.56}
{'loss': 0.3367, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2844, 'learning_rate': 3.7037037037037037e-05, 'epoch': 0.78}
{'loss': 0.3117, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.2847, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1534, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}
{'loss': 0.2052, 'learning_rate': 2.962962962962963e-05, 'epoch': 1.22}
{'loss': 0.1644, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1364, 'learning_rate': 2.5925925925925925e-05, 'epoch': 1.44}
{'loss': 0.1095, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.56}
{'loss': 0.1453, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1076, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1049, 'learning_rate': 1.8518518518518518e-05, 'epoch': 1.89}
{'loss': 0.089, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0463, 'learning_rate': 1.4814814814814815e-05, 'epoch': 2.11}
{'loss': 0.0816, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0703, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0738, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.44}
{'loss': 0.0533, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.56}
{'loss': 0.0344, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0352, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.78}
{'loss': 0.0562, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.89}
{'loss': 0.0371, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 288.5542, 'train_samples_per_second': 2.921, 'train_steps_per_second': 0.374, 'train_loss': 0.2199393048606537, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-72/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-108/optimizer.pt
validate!
last validate 0.
INFO 11-27 20:35:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:36:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.5_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.5_0.35_3_7/generated_contents/1
INFO 11-27 20:36:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-72', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-72', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:36:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.5_0.35_3_7 epoch 2

------------------------------------------------

0.02

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.5_0.35_3_7/generated_contents/2
INFO 11-27 20:36:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-108', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-108', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:37:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.5_0.35_3_7 epoch 3

------------------------------------------------

0.02

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.5_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-36
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-72
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_3_7/checkpoint-108
searching parameters: task121_30_20_40_0.6_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.6_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.6_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 20:37:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:37:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 20:40:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:40:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 277
expected_example_num: 600
selection_ratio: 0.46166666666666667
finetune_vicuna!
{'loss': 1.2468, 'learning_rate': 4.80952380952381e-05, 'epoch': 0.11}
{'loss': 0.5113, 'learning_rate': 4.6190476190476194e-05, 'epoch': 0.23}
{'loss': 0.7632, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.34}
{'loss': 0.4233, 'learning_rate': 4.2380952380952385e-05, 'epoch': 0.46}
{'loss': 0.3907, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4093, 'learning_rate': 3.857142857142858e-05, 'epoch': 0.69}
{'loss': 0.2596, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.518, 'learning_rate': 3.476190476190476e-05, 'epoch': 0.91}
{'loss': 0.311, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.03}
{'loss': 0.1357, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1392, 'learning_rate': 2.9047619047619052e-05, 'epoch': 1.26}
{'loss': 0.1288, 'learning_rate': 2.714285714285714e-05, 'epoch': 1.37}
{'loss': 0.2109, 'learning_rate': 2.523809523809524e-05, 'epoch': 1.49}
{'loss': 0.1629, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1662, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1805, 'learning_rate': 1.9523809523809524e-05, 'epoch': 1.83}
{'loss': 0.2097, 'learning_rate': 1.761904761904762e-05, 'epoch': 1.94}
{'loss': 0.0815, 'learning_rate': 1.5714285714285715e-05, 'epoch': 2.06}
{'loss': 0.0822, 'learning_rate': 1.3809523809523811e-05, 'epoch': 2.17}
{'loss': 0.084, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0754, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0555, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.51}
{'loss': 0.0535, 'learning_rate': 6.190476190476191e-06, 'epoch': 2.63}
{'loss': 0.0455, 'learning_rate': 4.285714285714286e-06, 'epoch': 2.74}
{'loss': 0.0636, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'loss': 0.0512, 'learning_rate': 4.761904761904763e-07, 'epoch': 2.97}
{'train_runtime': 280.6576, 'train_samples_per_second': 2.961, 'train_steps_per_second': 0.374, 'train_loss': 0.2583658896741413, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-35/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-105/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-70/optimizer.pt
validate!
last validate 0.
INFO 11-27 20:46:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-35', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-35', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:47:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.6_0.35_3_7/generated_contents/1
INFO 11-27 20:47:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-70', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-70', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:47:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.35_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.6_0.35_3_7/generated_contents/2
INFO 11-27 20:47:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-105', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-105', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:48:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.6_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-35
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-70
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_7/checkpoint-105
searching parameters: task121_10_15_45_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_15_45_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_15_45_0.5_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 20:48:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:48:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 20:49:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:49:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 54
expected_example_num: 150
selection_ratio: 0.36
finetune_vicuna!
{'loss': 1.6428, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4246, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2454, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1605, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.1129, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 179.899, 'train_samples_per_second': 0.901, 'train_steps_per_second': 0.117, 'train_loss': 0.4989600855679739, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-27 20:53:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:53:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.5_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_15_45_0.5_0.35_3_7/generated_contents/1
INFO 11-27 20:54:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:54:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.5_0.35_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_15_45_0.5_0.35_3_7/generated_contents/2
INFO 11-27 20:54:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:54:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_45_0.5_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_15_45_0.5_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_45_0.5_0.35_3_7/checkpoint-21
searching parameters: task121_30_10_40_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.5_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.5_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 20:55:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:55:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 20:57:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 20:57:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 117
expected_example_num: 300
selection_ratio: 0.39
finetune_vicuna!
{'loss': 1.2272, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.5942, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.3818, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2352, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.1491, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.143, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1914, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.1513, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0686, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0892, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0712, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 209.0588, 'train_samples_per_second': 1.679, 'train_steps_per_second': 0.215, 'train_loss': 0.2946025425361262, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:02:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:02:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.5_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.5_0.35_3_7/generated_contents/1
INFO 11-27 21:02:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:02:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.5_0.35_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.5_0.35_3_7/generated_contents/2
INFO 11-27 21:02:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:03:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.5_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.5_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.5_0.35_3_7/checkpoint-45
searching parameters: task121_20_10_45_0.4_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_10_45_0.4_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_10_45_0.4_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:03:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:03:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:04:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:05:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 80
expected_example_num: 200
selection_ratio: 0.4
finetune_vicuna!
{'loss': 1.0986, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.6689, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.311, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.201, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.2058, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1173, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0839, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 197.8896, 'train_samples_per_second': 1.213, 'train_steps_per_second': 0.152, 'train_loss': 0.3672692209482193, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:09:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:09:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.4_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_10_45_0.4_0.35_3_7/generated_contents/1
INFO 11-27 21:09:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:10:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.4_0.35_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_10_45_0.4_0.35_3_7/generated_contents/2
INFO 11-27 21:10:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:10:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.4_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_10_45_0.4_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.35_3_7/checkpoint-30
searching parameters: task121_30_10_40_0.6_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.6_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.6_0.3_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:10:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:11:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:12:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:12:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 144
expected_example_num: 300
selection_ratio: 0.48
finetune_vicuna!
{'loss': 1.2612, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.22}
{'loss': 0.6454, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.5164, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4012, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.361, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}
{'loss': 0.1977, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1782, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.56}
{'loss': 0.1195, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1087, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0675, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0501, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.44}
{'loss': 0.0629, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0615, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.89}
{'train_runtime': 235.3021, 'train_samples_per_second': 1.836, 'train_steps_per_second': 0.229, 'train_loss': 0.30221472321837034, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-54/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:18:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:18:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.6_0.3_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.6_0.3_3_7/generated_contents/1
INFO 11-27 21:18:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:18:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.6_0.3_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.6_0.3_3_7/generated_contents/2
INFO 11-27 21:18:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-54', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-54', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:19:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.6_0.3_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.6_0.3_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-36
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.6_0.3_3_7/checkpoint-54
searching parameters: task121_30_10_45_0.7_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_45_0.7_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_45_0.7_0.3_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:19:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:19:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:21:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:21:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 146
expected_example_num: 300
selection_ratio: 0.4866666666666667
finetune_vicuna!
{'loss': 1.4395, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.6898, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.497, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.4243, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.388, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.1867, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.1911, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.169, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.1701, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.1021, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0696, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.0691, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0605, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0417, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 230.4781, 'train_samples_per_second': 1.9, 'train_steps_per_second': 0.247, 'train_loss': 0.31610582618598354, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:26:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:27:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.3_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_45_0.7_0.3_3_7/generated_contents/1
INFO 11-27 21:27:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:27:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.3_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_45_0.7_0.3_3_7/generated_contents/2
INFO 11-27 21:27:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:28:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.3_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_45_0.7_0.3_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.3_3_7/checkpoint-57
searching parameters: task121_30_10_50_0.4_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_50_0.4_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_50_0.4_0.4_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:28:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:28:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:30:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:30:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 116
expected_example_num: 300
selection_ratio: 0.38666666666666666
finetune_vicuna!
{'loss': 1.1243, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.5491, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.4191, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.4808, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.1702, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1642, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1516, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.1089, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0791, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.086, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0559, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 214.2631, 'train_samples_per_second': 1.624, 'train_steps_per_second': 0.21, 'train_loss': 0.302349175264438, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:35:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:35:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.4_0.4_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_50_0.4_0.4_3_7/generated_contents/1
INFO 11-27 21:35:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:36:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.4_0.4_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_50_0.4_0.4_3_7/generated_contents/2
INFO 11-27 21:36:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:36:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.4_0.4_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_10_50_0.4_0.4_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.4_0.4_3_7/checkpoint-45
searching parameters: task121_20_15_40_0.8_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_15_40_0.8_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_15_40_0.8_0.4_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:36:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:37:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:38:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:38:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 132
expected_example_num: 300
selection_ratio: 0.44
finetune_vicuna!
{'loss': 1.276, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.6193, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.5734, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.4958, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.2486, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.1999, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.135, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.1626, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.053, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.0371, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.049, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.0639, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'train_runtime': 222.9364, 'train_samples_per_second': 1.776, 'train_steps_per_second': 0.229, 'train_loss': 0.3108506559156904, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:43:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:44:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.8_0.4_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_15_40_0.8_0.4_3_7/generated_contents/1
INFO 11-27 21:45:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:45:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.8_0.4_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_15_40_0.8_0.4_3_7/generated_contents/2
INFO 11-27 21:45:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:45:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.8_0.4_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_15_40_0.8_0.4_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-34
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.4_3_7/checkpoint-51
searching parameters: task121_30_20_45_0.7_0.35_3_7
searching parameters: task121_30_20_40_0.7_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.7_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.7_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:46:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:46:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:49:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:49:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 273
expected_example_num: 600
selection_ratio: 0.455
finetune_vicuna!
{'loss': 1.2861, 'learning_rate': 4.80952380952381e-05, 'epoch': 0.11}
{'loss': 0.4547, 'learning_rate': 4.6190476190476194e-05, 'epoch': 0.23}
{'loss': 0.5242, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.34}
{'loss': 0.4246, 'learning_rate': 4.2380952380952385e-05, 'epoch': 0.46}
{'loss': 0.5459, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4284, 'learning_rate': 3.857142857142858e-05, 'epoch': 0.69}
{'loss': 0.2981, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3423, 'learning_rate': 3.476190476190476e-05, 'epoch': 0.91}
{'loss': 0.2231, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.03}
{'loss': 0.1433, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2281, 'learning_rate': 2.9047619047619052e-05, 'epoch': 1.26}
{'loss': 0.1215, 'learning_rate': 2.714285714285714e-05, 'epoch': 1.37}
{'loss': 0.2243, 'learning_rate': 2.523809523809524e-05, 'epoch': 1.49}
{'loss': 0.1412, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.123, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1313, 'learning_rate': 1.9523809523809524e-05, 'epoch': 1.83}
{'loss': 0.1045, 'learning_rate': 1.761904761904762e-05, 'epoch': 1.94}
{'loss': 0.0618, 'learning_rate': 1.5714285714285715e-05, 'epoch': 2.06}
{'loss': 0.0612, 'learning_rate': 1.3809523809523811e-05, 'epoch': 2.17}
{'loss': 0.0547, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0528, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0385, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.51}
{'loss': 0.0532, 'learning_rate': 6.190476190476191e-06, 'epoch': 2.63}
{'loss': 0.0677, 'learning_rate': 4.285714285714286e-06, 'epoch': 2.74}
{'loss': 0.0724, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'loss': 0.0503, 'learning_rate': 4.761904761904763e-07, 'epoch': 2.97}
{'train_runtime': 291.6249, 'train_samples_per_second': 2.808, 'train_steps_per_second': 0.36, 'train_loss': 0.2403358592873528, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-35/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-105/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-70/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:56:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-35', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-35', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:56:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.7_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.7_0.35_3_7/generated_contents/1
INFO 11-27 21:56:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-70', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-70', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:56:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.7_0.35_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.7_0.35_3_7/generated_contents/2
INFO 11-27 21:57:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-105', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-105', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:57:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.7_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_40_0.7_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-35
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-70
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.7_0.35_3_7/checkpoint-105
searching parameters: task121_20_20_45_0.8_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_20_45_0.8_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_20_45_0.8_0.4_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:57:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:57:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:59:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:59:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 165
expected_example_num: 400
selection_ratio: 0.4125
finetune_vicuna!
{'loss': 1.4053, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.5777, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.5459, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.3787, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.4358, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.2083, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1832, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1729, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.1461, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1617, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.0904, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0617, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0401, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0514, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0631, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 241.3664, 'train_samples_per_second': 2.051, 'train_steps_per_second': 0.261, 'train_loss': 0.2894733046728467, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:05:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:05:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_45_0.8_0.4_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_20_45_0.8_0.4_3_7/generated_contents/1
INFO 11-27 22:05:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:05:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_45_0.8_0.4_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_20_45_0.8_0.4_3_7/generated_contents/2
INFO 11-27 22:06:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:06:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_45_0.8_0.4_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_20_45_0.8_0.4_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-21
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.4_3_7/checkpoint-63
searching parameters: task121_30_20_50_0.7_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_50_0.7_0.35_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_20_50_0.7_0.35_3_7/config.json
generate_and_write_inputs!
INFO 11-27 22:06:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:06:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:09:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:10:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 294
expected_example_num: 600
selection_ratio: 0.49
finetune_vicuna!
{'loss': 1.0367, 'learning_rate': 4.8198198198198205e-05, 'epoch': 0.11}
{'loss': 0.5509, 'learning_rate': 4.6396396396396394e-05, 'epoch': 0.22}
{'loss': 2.4615, 'learning_rate': 4.4594594594594596e-05, 'epoch': 0.32}
{'loss': 0.4648, 'learning_rate': 4.27927927927928e-05, 'epoch': 0.43}
{'loss': 0.3448, 'learning_rate': 4.099099099099099e-05, 'epoch': 0.54}
{'loss': 0.3514, 'learning_rate': 3.918918918918919e-05, 'epoch': 0.65}
{'loss': 0.3332, 'learning_rate': 3.738738738738739e-05, 'epoch': 0.76}
{'loss': 0.2579, 'learning_rate': 3.558558558558558e-05, 'epoch': 0.86}
{'loss': 0.4014, 'learning_rate': 3.3783783783783784e-05, 'epoch': 0.97}
{'loss': 0.1646, 'learning_rate': 3.198198198198199e-05, 'epoch': 1.08}
{'loss': 0.1794, 'learning_rate': 3.0180180180180183e-05, 'epoch': 1.19}
{'loss': 0.1399, 'learning_rate': 2.8378378378378378e-05, 'epoch': 1.3}
{'loss': 0.1926, 'learning_rate': 2.6576576576576577e-05, 'epoch': 1.41}
{'loss': 0.1501, 'learning_rate': 2.4774774774774777e-05, 'epoch': 1.51}
{'loss': 0.1334, 'learning_rate': 2.2972972972972976e-05, 'epoch': 1.62}
{'loss': 0.1233, 'learning_rate': 2.117117117117117e-05, 'epoch': 1.73}
{'loss': 0.1684, 'learning_rate': 1.936936936936937e-05, 'epoch': 1.84}
{'loss': 0.1135, 'learning_rate': 1.756756756756757e-05, 'epoch': 1.95}
{'loss': 0.1104, 'learning_rate': 1.5765765765765765e-05, 'epoch': 2.05}
{'loss': 0.0519, 'learning_rate': 1.3963963963963963e-05, 'epoch': 2.16}
{'loss': 0.0778, 'learning_rate': 1.2162162162162164e-05, 'epoch': 2.27}
{'loss': 0.0417, 'learning_rate': 1.0360360360360361e-05, 'epoch': 2.38}
{'loss': 0.0559, 'learning_rate': 8.558558558558558e-06, 'epoch': 2.49}
{'loss': 0.0635, 'learning_rate': 6.7567567567567575e-06, 'epoch': 2.59}
{'loss': 0.0794, 'learning_rate': 4.954954954954955e-06, 'epoch': 2.7}
{'loss': 0.0431, 'learning_rate': 3.153153153153153e-06, 'epoch': 2.81}
{'loss': 0.051, 'learning_rate': 1.3513513513513515e-06, 'epoch': 2.92}
{'train_runtime': 298.6421, 'train_samples_per_second': 2.953, 'train_steps_per_second': 0.372, 'train_loss': 0.29536411152766634, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-37/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-74/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-111/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:16:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-37', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-37', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:17:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_50_0.7_0.35_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_50_0.7_0.35_3_7/generated_contents/1
INFO 11-27 22:17:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-74', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-74', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:17:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_50_0.7_0.35_3_7 epoch 2

------------------------------------------------

0.02

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_50_0.7_0.35_3_7/generated_contents/2
INFO 11-27 22:17:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-111', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-111', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:17:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_50_0.7_0.35_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_30_20_50_0.7_0.35_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-37
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-74
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.7_0.35_3_7/checkpoint-111
searching parameters: task121_10_20_50_0.5_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.3_3_7/config.json
generate_and_write_inputs!
INFO 11-27 22:18:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:18:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:19:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:19:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 93
expected_example_num: 200
selection_ratio: 0.465
finetune_vicuna!
{'loss': 1.6206, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5183, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.369, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1608, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.169, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1939, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0752, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0944, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0654, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 211.9029, 'train_samples_per_second': 1.317, 'train_steps_per_second': 0.17, 'train_loss': 0.36295998344818753, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:24:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:24:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.5_0.3_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.3_3_7/generated_contents/1
INFO 11-27 22:24:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:25:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.5_0.3_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.3_3_7/generated_contents/2
INFO 11-27 22:25:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:25:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.5_0.3_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_20_50_0.5_0.3_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.5_0.3_3_7/checkpoint-36
searching parameters: task121_30_15_50_0.7_0.35_3_7
searching parameters: task121_30_15_50_0.5_0.35_3_7
searching parameters: task121_10_15_50_0.8_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_15_50_0.8_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_10_15_50_0.8_0.4_3_7/config.json
generate_and_write_inputs!
INFO 11-27 22:25:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:26:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:27:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:27:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 77
expected_example_num: 150
selection_ratio: 0.5133333333333333
finetune_vicuna!
{'loss': 1.2784, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 1.0232, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2582, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.2214, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1679, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0701, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0594, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 199.4774, 'train_samples_per_second': 1.158, 'train_steps_per_second': 0.15, 'train_loss': 0.4127502185602983, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:31:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:31:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.8_0.4_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_15_50_0.8_0.4_3_7/generated_contents/1
INFO 11-27 22:32:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:32:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.8_0.4_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_15_50_0.8_0.4_3_7/generated_contents/2
INFO 11-27 22:32:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:32:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.8_0.4_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_10_15_50_0.8_0.4_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.8_0.4_3_7/checkpoint-30
searching parameters: task121_20_15_50_0.4_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_15_50_0.4_0.3_3_7
/home/cyzhao/NI_task121_exp_7/task121_20_15_50_0.4_0.3_3_7/config.json
generate_and_write_inputs!
INFO 11-27 22:33:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:33:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:34:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:34:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 126
expected_example_num: 300
selection_ratio: 0.42
finetune_vicuna!
{'loss': 1.1909, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 3.0363, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.5917, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.4884, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.216, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.1799, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1722, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.2665, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0752, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.098, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.1267, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.0943, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 219.9348, 'train_samples_per_second': 1.719, 'train_steps_per_second': 0.218, 'train_loss': 0.5446825046092272, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:39:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:41:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_50_0.4_0.3_3_7 epoch 1

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_15_50_0.4_0.3_3_7/generated_contents/1
INFO 11-27 22:42:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:42:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_50_0.4_0.3_3_7 epoch 2

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_15_50_0.4_0.3_3_7/generated_contents/2
INFO 11-27 22:42:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:42:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_50_0.4_0.3_3_7 epoch 3

------------------------------------------------

0.0

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_7/task121_20_15_50_0.4_0.3_3_7/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.3_3_7/checkpoint-48
{'generation_epochs': 30, 'generation_batch_size': 15, 'generation_top_k': 50, 'generation_temperature': 0.5, 'min_frequency': 0.35, 'training_epochs': 3}
Already tested
