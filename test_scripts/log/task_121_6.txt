[2023-11-26 06:12:47,033] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_45_0.5_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.5_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.5_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:12:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:13:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:17:01,915] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_50_0.4_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.4_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.4_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:17:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:17:22 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:20:40,423] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_45_0.7_0.4_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/config.json
generate_and_write_inputs!
INFO 11-26 06:20:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:21:03 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:23:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:23:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 38
expected_example_num: 300
selection_ratio: 0.12666666666666668
finetune_vicuna!
{'loss': 0.8488, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2582, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1686, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 183.2427, 'train_samples_per_second': 0.622, 'train_steps_per_second': 0.082, 'train_loss': 0.35803285241127014, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:27:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:01 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 1

------------------------------------------------

0.47386203606812854

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/1
INFO 11-26 06:28:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 2

------------------------------------------------

0.6140774297053422

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/2
INFO 11-26 06:28:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:58 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 3

------------------------------------------------

0.6022900212123337

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15
searching parameters: task121_30_20_50_0.5_0.3_55_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.5_0.3_55_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.5_0.3_55_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:29:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:29:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:39:36,850] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_50_0.6_0.3_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/config.json
generate_and_write_inputs!
INFO 11-26 06:39:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:40:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:42:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:42:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 42
expected_example_num: 300
selection_ratio: 0.14
finetune_vicuna!
{'loss': 0.9131, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2706, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1809, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0709, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 189.7949, 'train_samples_per_second': 0.664, 'train_steps_per_second': 0.095, 'train_loss': 0.3254915161265267, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:46:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 1

------------------------------------------------

0.48281378766727984

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/1
INFO 11-26 06:47:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 2

------------------------------------------------

0.6159874308751637

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/2
INFO 11-26 06:47:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 3

------------------------------------------------

0.6087367413831614

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18
searching parameters: task121_30_10_40_0.7_0.35_50_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:48:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:50:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:51:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 57
expected_example_num: 300
selection_ratio: 0.19
finetune_vicuna!
{'loss': 0.8862, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.4745, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.182, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1536, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0834, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.1009, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 185.4357, 'train_samples_per_second': 0.922, 'train_steps_per_second': 0.129, 'train_loss': 0.31341806426644325, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:55:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:55:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 1

------------------------------------------------

0.5137478613418609

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/1
INFO 11-26 06:55:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 2

------------------------------------------------

0.5234433333658504

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/2
INFO 11-26 06:56:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 3

------------------------------------------------

0.5246105387327548

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24
searching parameters: task121_20_20_40_0.6_0.35_50_3_0.6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_50_3_0.6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_50_3_0.6/config.json
generate_and_write_inputs!
[2023-11-26 07:05:14,410] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_10_40_0.7_0.4_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/config.json
generate_and_write_inputs!
INFO 11-26 07:05:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:05:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:07:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:07:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 41
expected_example_num: 200
selection_ratio: 0.205
finetune_vicuna!
{'loss': 0.5583, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4243, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1881, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0618, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 186.8953, 'train_samples_per_second': 0.658, 'train_steps_per_second': 0.096, 'train_loss': 0.28146878878275555, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:11:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:11:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.7_0.4_50_3_0.5_6 epoch 1

------------------------------------------------

0.3858472483442352

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/generated_contents/1
INFO 11-26 07:12:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:12:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.7_0.4_50_3_0.5_6 epoch 2

------------------------------------------------

0.47052102660621464

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/generated_contents/2
INFO 11-26 07:12:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:13:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.7_0.4_50_3_0.5_6 epoch 3

------------------------------------------------

0.5339786138287099

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12
searching parameters: task121_30_15_40_0.7_0.35_55_3_0.7_6
/home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6
/home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/config.json
generate_and_write_inputs!
INFO 11-26 07:13:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:13:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:16:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:16:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 153
expected_example_num: 450
selection_ratio: 0.34
finetune_vicuna!
{'loss': 1.4867, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}
{'loss': 0.5709, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.3777, 'learning_rate': 4e-05, 'epoch': 0.6}
{'loss': 0.3137, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2483, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1991, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.1749, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}
{'loss': 0.1395, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1125, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.181, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.086, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}
{'loss': 0.0754, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0668, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}
{'loss': 0.0613, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0911, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 234.2722, 'train_samples_per_second': 1.959, 'train_steps_per_second': 0.256, 'train_loss': 0.2789895509680112, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:22:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:22:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.7_0.35_55_3_0.7_6 epoch 1

------------------------------------------------

0.5777966418898915

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/generated_contents/1
INFO 11-26 07:22:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:23:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.7_0.35_55_3_0.7_6 epoch 2

------------------------------------------------

0.5818269677488697

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/generated_contents/2
INFO 11-26 07:23:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:23:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.7_0.35_55_3_0.7_6 epoch 3

------------------------------------------------

0.5824677753361006

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40
searching parameters: task121_30_10_50_0.5_0.35_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/config.json
generate_and_write_inputs!
INFO 11-26 07:23:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:24:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:26:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:26:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 58
expected_example_num: 300
selection_ratio: 0.19333333333333333
finetune_vicuna!
{'loss': 0.7637, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.5959, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2278, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1217, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0697, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0648, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 191.5262, 'train_samples_per_second': 0.908, 'train_steps_per_second': 0.125, 'train_loss': 0.30728179092208546, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:30:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:30:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.35_50_3_0.5_6 epoch 1

------------------------------------------------

0.586048066600844

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/generated_contents/1
INFO 11-26 07:31:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:31:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.35_50_3_0.5_6 epoch 2

------------------------------------------------

0.6136896059990818

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/generated_contents/2
INFO 11-26 07:31:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:31:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.35_50_3_0.5_6 epoch 3

------------------------------------------------

0.5976522613035193

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24
searching parameters: task121_30_20_45_0.5_0.3_60_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/config.json
generate_and_write_inputs!
INFO 11-26 07:32:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:32:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:35:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:36:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 113
expected_example_num: 600
selection_ratio: 0.18833333333333332
finetune_vicuna!
{'loss': 0.8603, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.5866, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.3601, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2512, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.2324, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1084, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1568, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.1096, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0974, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.091, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0776, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 216.0001, 'train_samples_per_second': 1.569, 'train_steps_per_second': 0.208, 'train_loss': 0.26786362793710494, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:40:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:41:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.5_0.3_60_3_0.5_6 epoch 1

------------------------------------------------

0.5977986933767747

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/generated_contents/1
INFO 11-26 07:41:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:41:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.5_0.3_60_3_0.5_6 epoch 2

------------------------------------------------

0.5407027854901897

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/generated_contents/2
INFO 11-26 07:41:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:42:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.5_0.3_60_3_0.5_6 epoch 3

------------------------------------------------

0.5471179054261001

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45
searching parameters: task121_30_20_50_0.8_0.35_60_3_0.6_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.8_0.35_60_3_0.6_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.8_0.35_60_3_0.6_6/config.json
generate_and_write_inputs!
INFO 11-26 07:42:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:42:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 07:57:37,235] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_40_0.5_0.3_100_130_45_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.5_0.3_100_130_45_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.5_0.3_100_130_45_75_3_6/config.json
generate_and_write_inputs!
[2023-11-26 07:59:50,735] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_40_0.4_0.3_80_120_40_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 07:59:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:00:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:01:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:01:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 43
expected_example_num: 200
selection_ratio: 0.215
finetune_vicuna!
{'loss': 0.6489, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2245, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1032, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0598, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 188.6228, 'train_samples_per_second': 0.684, 'train_steps_per_second': 0.095, 'train_loss': 0.23762818177541098, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:05:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:06:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.4_0.3_80_120_40_80_3_6 epoch 1

------------------------------------------------

0.612704781327531

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/generated_contents/1
INFO 11-26 08:06:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:06:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.4_0.3_80_120_40_80_3_6 epoch 2

------------------------------------------------

0.6468893515818783

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/generated_contents/2
INFO 11-26 08:06:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:07:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.4_0.3_80_120_40_80_3_6 epoch 3

------------------------------------------------

0.6601258344586295

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12
searching parameters: task121_20_15_45_0.7_0.3_90_120_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_15_45_0.7_0.3_90_120_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_15_45_0.7_0.3_90_120_50_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:07:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:07:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 08:11:29,262] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_10_40_0.5_0.35_100_130_40_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:11:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:11:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:13:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:13:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 16
expected_example_num: 200
selection_ratio: 0.08
finetune_vicuna!
{'loss': 0.6141, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 172.0702, 'train_samples_per_second': 0.279, 'train_steps_per_second': 0.035, 'train_loss': 0.4542270402113597, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:17:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:17:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_100_130_40_85_3_6 epoch 1

------------------------------------------------

0.06293747840097973

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/generated_contents/1
INFO 11-26 08:17:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:18:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_100_130_40_85_3_6 epoch 2

------------------------------------------------

0.47881824502451115

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/generated_contents/2
INFO 11-26 08:18:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:18:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_100_130_40_85_3_6 epoch 3

------------------------------------------------

0.5459133829218742

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4
searching parameters: task121_10_20_45_0.4_0.3_100_120_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.4_0.3_100_120_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.4_0.3_100_120_40_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:18:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:19:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:21:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:21:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 0
expected_example_num: 200
selection_ratio: 0.0
finetune_vicuna!
[2023-11-26 08:29:49,159] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_15_40_0.6_0.3_80_120_50_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_15_40_0.6_0.3_80_120_50_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_15_40_0.6_0.3_80_120_50_85_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:29:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:30:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 08:31:26,527] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_45_0.7_0.4_100_130_45_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_100_130_45_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:31:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:31:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:34:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:34:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 103
expected_example_num: 300
selection_ratio: 0.3433333333333333
finetune_vicuna!
{'loss': 1.8005, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.6788, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.3866, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.2412, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.243, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1378, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1611, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0912, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0783, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 213.2116, 'train_samples_per_second': 1.449, 'train_steps_per_second': 0.183, 'train_loss': 0.3982352010714702, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:39:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:39:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_100_130_45_80_3_6 epoch 1

------------------------------------------------

0.5794013319149726

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/generated_contents/1
INFO 11-26 08:39:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:39:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_100_130_45_80_3_6 epoch 2

------------------------------------------------

0.5589139273472974

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/generated_contents/2
INFO 11-26 08:39:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:40:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_100_130_45_80_3_6 epoch 3

------------------------------------------------

0.5895949581768547

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-39 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_100_130_45_80_3_6/checkpoint-26
searching parameters: task121_20_20_40_0.6_0.35_90_120_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_90_120_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:40:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:40:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:43:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:43:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 42
expected_example_num: 400
selection_ratio: 0.105
finetune_vicuna!
{'loss': 1.5269, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.5197, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.336, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1619, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 187.5778, 'train_samples_per_second': 0.672, 'train_steps_per_second': 0.096, 'train_loss': 0.5780252400371764, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:47:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:47:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.6_0.35_90_120_50_75_3_6 epoch 1

------------------------------------------------

0.5834717169167425

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/generated_contents/1
INFO 11-26 08:48:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:48:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.6_0.35_90_120_50_75_3_6 epoch 2

------------------------------------------------

0.593677576303306

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/generated_contents/2
INFO 11-26 08:48:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:48:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.6_0.35_90_120_50_75_3_6 epoch 3

------------------------------------------------

0.5926791208242355

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.6_0.35_90_120_50_75_3_6/checkpoint-18
searching parameters: task121_10_10_45_0.4_0.3_90_130_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_45_0.4_0.3_90_130_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:49:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:49:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:50:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:51:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 2
expected_example_num: 100
selection_ratio: 0.02
finetune_vicuna!
{'train_runtime': 170.0364, 'train_samples_per_second': 0.035, 'train_steps_per_second': 0.018, 'train_loss': 1.00823974609375, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:54:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:54:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_45_0.4_0.3_90_130_40_75_3_6 epoch 1

------------------------------------------------

0.1547591227204042

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/generated_contents/1
INFO 11-26 08:55:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:55:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_45_0.4_0.3_90_130_40_75_3_6 epoch 2

------------------------------------------------

0.2639999150832952

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/generated_contents/2
INFO 11-26 08:56:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:56:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_45_0.4_0.3_90_130_40_75_3_6 epoch 3

------------------------------------------------

0.34286642885095714

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.3_90_130_40_75_3_6/checkpoint-3
searching parameters: task121_10_10_50_0.6_0.3_100_130_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_130_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:56:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:57:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:58:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:58:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 3
expected_example_num: 100
selection_ratio: 0.03
finetune_vicuna!
{'train_runtime': 167.064, 'train_samples_per_second': 0.054, 'train_steps_per_second': 0.018, 'train_loss': 0.27243473132451373, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:01:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:02:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.3_100_130_50_75_3_6 epoch 1

------------------------------------------------

0.6091560142427285

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/generated_contents/1
INFO 11-26 09:02:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:02:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.3_100_130_50_75_3_6 epoch 2

------------------------------------------------

0.641689420463575

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/generated_contents/2
INFO 11-26 09:02:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:03:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.3_100_130_50_75_3_6 epoch 3

------------------------------------------------

0.6434848686207766

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-3 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_130_50_75_3_6/checkpoint-2
searching parameters: task121_20_10_40_0.6_0.35_80_140_50_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.6_0.35_80_140_50_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/config.json
generate_and_write_inputs!
INFO 11-26 09:03:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:03:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:05:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:05:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 38
expected_example_num: 200
selection_ratio: 0.19
finetune_vicuna!
{'loss': 1.2544, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3118, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1823, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 187.4883, 'train_samples_per_second': 0.608, 'train_steps_per_second': 0.08, 'train_loss': 0.4919768770535787, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:09:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:09:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.6_0.35_80_140_50_85_3_6 epoch 1

------------------------------------------------

0.5467229527561951

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/generated_contents/1
INFO 11-26 09:10:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:10:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.6_0.35_80_140_50_85_3_6 epoch 2

------------------------------------------------

0.5666682784969523

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/generated_contents/2
INFO 11-26 09:10:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:11:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.6_0.35_80_140_50_85_3_6 epoch 3

------------------------------------------------

0.5938617784654961

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.6_0.35_80_140_50_85_3_6/checkpoint-15
searching parameters: task121_30_15_45_0.4_0.4_80_130_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_30_15_45_0.4_0.4_80_130_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 09:11:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:11:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:14:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:15:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 48
expected_example_num: 450
selection_ratio: 0.10666666666666667
finetune_vicuna!
{'loss': 0.7735, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4667, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1916, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1362, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 187.5741, 'train_samples_per_second': 0.768, 'train_steps_per_second': 0.096, 'train_loss': 0.35741164452499813, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:19:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:19:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.4_0.4_80_130_40_75_3_6 epoch 1

------------------------------------------------

0.4127274517856509

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/generated_contents/1
INFO 11-26 09:19:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:20:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.4_0.4_80_130_40_75_3_6 epoch 2

------------------------------------------------

0.5189240175035643

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/generated_contents/2
INFO 11-26 09:20:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:20:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.4_0.4_80_130_40_75_3_6 epoch 3

------------------------------------------------

0.5204816794974695

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.4_0.4_80_130_40_75_3_6/checkpoint-18
searching parameters: task121_30_20_40_0.8_0.3_100_120_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_40_0.8_0.3_100_120_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 09:20:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:21:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:24:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:24:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 29
expected_example_num: 600
selection_ratio: 0.04833333333333333
finetune_vicuna!
{'loss': 1.5806, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.389, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1754, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 177.9163, 'train_samples_per_second': 0.489, 'train_steps_per_second': 0.067, 'train_loss': 0.7150002419948578, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:28:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:28:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.8_0.3_100_120_40_75_3_6 epoch 1

------------------------------------------------

0.5179762460132825

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/generated_contents/1
INFO 11-26 09:28:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:28:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.8_0.3_100_120_40_75_3_6 epoch 2

------------------------------------------------

0.5863091225215855

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/generated_contents/2
INFO 11-26 09:29:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:29:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.8_0.3_100_120_40_75_3_6 epoch 3

------------------------------------------------

0.5857757451061981

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.8_0.3_100_120_40_75_3_6/checkpoint-12
searching parameters: task121_20_20_40_0.4_0.4_90_130_45_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.4_0.4_90_130_45_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/config.json
generate_and_write_inputs!
INFO 11-26 09:29:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:29:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:32:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:32:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 57
expected_example_num: 400
selection_ratio: 0.1425
finetune_vicuna!
{'loss': 1.2922, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.3085, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1316, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1474, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0704, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0569, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 192.7764, 'train_samples_per_second': 0.887, 'train_steps_per_second': 0.124, 'train_loss': 0.3344900341083606, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:36:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:36:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.4_0.4_90_130_45_85_3_6 epoch 1

------------------------------------------------

0.5852156617572596

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/generated_contents/1
INFO 11-26 09:37:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:37:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.4_0.4_90_130_45_85_3_6 epoch 2

------------------------------------------------

0.5678506995814844

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/generated_contents/2
INFO 11-26 09:37:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:37:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.4_0.4_90_130_45_85_3_6 epoch 3

------------------------------------------------

0.5761641796985598

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.4_0.4_90_130_45_85_3_6/checkpoint-24
searching parameters: task121_20_20_40_0.7_0.35_90_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.7_0.35_90_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 09:38:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:38:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:40:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:40:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 83
expected_example_num: 400
selection_ratio: 0.2075
finetune_vicuna!
{'loss': 1.1735, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.9212, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4496, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2112, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.2124, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.1623, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0992, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.1289, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 203.6143, 'train_samples_per_second': 1.223, 'train_steps_per_second': 0.162, 'train_loss': 0.4115513301256931, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:45:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:45:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.7_0.35_90_120_50_80_3_6 epoch 1

------------------------------------------------

0.6320691016846345

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/generated_contents/1
INFO 11-26 09:45:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:45:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.7_0.35_90_120_50_80_3_6 epoch 2

------------------------------------------------

0.6215828162917331

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/generated_contents/2
INFO 11-26 09:46:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:46:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_40_0.7_0.35_90_120_50_80_3_6 epoch 3

------------------------------------------------

0.631755756722277

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.7_0.35_90_120_50_80_3_6/checkpoint-33
searching parameters: task121_10_10_50_0.6_0.3_100_140_45_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_140_45_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 09:46:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:46:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:48:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:48:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 4
expected_example_num: 100
selection_ratio: 0.04
finetune_vicuna!
{'train_runtime': 165.5412, 'train_samples_per_second': 0.072, 'train_steps_per_second': 0.018, 'train_loss': 0.1819158395131429, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:51:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:52:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.3_100_140_45_75_3_6 epoch 1

------------------------------------------------

0.5948779034998338

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/generated_contents/1
INFO 11-26 09:52:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:52:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.3_100_140_45_75_3_6 epoch 2

------------------------------------------------

0.6258246578260187

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/generated_contents/2
INFO 11-26 09:52:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:52:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.3_100_140_45_75_3_6 epoch 3

------------------------------------------------

0.6193605269023899

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.3_100_140_45_75_3_6/checkpoint-3
searching parameters: task121_10_15_50_0.5_0.3_100_130_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.3_100_130_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 09:53:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:53:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:55:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:55:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 20
expected_example_num: 150
selection_ratio: 0.13333333333333333
finetune_vicuna!
{'loss': 0.8701, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2646, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 175.0868, 'train_samples_per_second': 0.343, 'train_steps_per_second': 0.051, 'train_loss': 0.5137175247073174, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:59:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:59:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.5_0.3_100_130_50_80_3_6 epoch 1

------------------------------------------------

0.49355783750405513

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/generated_contents/1
INFO 11-26 09:59:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:59:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.5_0.3_100_130_50_80_3_6 epoch 2

------------------------------------------------

0.5373333145435013

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/generated_contents/2
INFO 11-26 09:59:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:00:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.5_0.3_100_130_50_80_3_6 epoch 3

------------------------------------------------

0.5415636098756401

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.3_100_130_50_80_3_6/checkpoint-9
searching parameters: task121_20_20_50_0.7_0.35_90_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_50_0.7_0.35_90_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:00:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:00:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:02:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:02:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 83
expected_example_num: 400
selection_ratio: 0.2075
finetune_vicuna!
{'loss': 1.1711, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.9195, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4767, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2219, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.2143, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.1616, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.1023, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.1345, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 203.8584, 'train_samples_per_second': 1.221, 'train_steps_per_second': 0.162, 'train_loss': 0.4160873898954103, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:07:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:07:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_50_0.7_0.35_90_120_50_80_3_6 epoch 1

------------------------------------------------

0.6124488340598662

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/generated_contents/1
INFO 11-26 10:08:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:08:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_50_0.7_0.35_90_120_50_80_3_6 epoch 2

------------------------------------------------

0.6217689852191932

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/generated_contents/2
INFO 11-26 10:08:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:08:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_50_0.7_0.35_90_120_50_80_3_6 epoch 3

------------------------------------------------

0.6279384693388629

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.7_0.35_90_120_50_80_3_6/checkpoint-33
searching parameters: task121_10_20_50_0.7_0.35_100_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:09:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:09:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:10:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:10:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 10
expected_example_num: 200
selection_ratio: 0.05
finetune_vicuna!
{'loss': 0.8807, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 169.6598, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.035, 'train_loss': 0.6831897993882498, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:14:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:14:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.35_100_120_50_80_3_6 epoch 1

------------------------------------------------

0.661842560825152

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/generated_contents/1
INFO 11-26 10:14:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:15:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.35_100_120_50_80_3_6 epoch 2

------------------------------------------------

0.657379109924964

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/generated_contents/2
INFO 11-26 10:15:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:15:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.35_100_120_50_80_3_6 epoch 3

------------------------------------------------

0.6727728987936213

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-6 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_50_80_3_6/checkpoint-4
searching parameters: task121_10_10_50_0.8_0.3_100_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.8_0.3_100_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:16:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:16:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:17:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:17:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 29
expected_example_num: 100
selection_ratio: 0.29
finetune_vicuna!
{'loss': 1.2851, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2778, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1234, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 175.9187, 'train_samples_per_second': 0.495, 'train_steps_per_second': 0.068, 'train_loss': 0.5621013765533766, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:21:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:21:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.8_0.3_100_120_50_80_3_6 epoch 1

------------------------------------------------

0.5100189978198209

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/generated_contents/1
INFO 11-26 10:22:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:22:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.8_0.3_100_120_50_80_3_6 epoch 2

------------------------------------------------

0.49599708816706917

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/generated_contents/2
INFO 11-26 10:22:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:22:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.8_0.3_100_120_50_80_3_6 epoch 3

------------------------------------------------

0.49604642520067993

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.3_100_120_50_80_3_6/checkpoint-12
searching parameters: task121_10_15_50_0.5_0.35_100_140_50_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.35_100_140_50_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:23:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:23:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:24:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:25:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 27
expected_example_num: 150
selection_ratio: 0.18
finetune_vicuna!
{'loss': 0.9844, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1603, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0721, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 170.4691, 'train_samples_per_second': 0.475, 'train_steps_per_second': 0.07, 'train_loss': 0.4055941278735797, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:28:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:29:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.5_0.35_100_140_50_85_3_6 epoch 1

------------------------------------------------

0.5031310382981016

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/generated_contents/1
INFO 11-26 10:29:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:29:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.5_0.35_100_140_50_85_3_6 epoch 2

------------------------------------------------

0.48205316512812935

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/generated_contents/2
INFO 11-26 10:29:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:30:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.5_0.35_100_140_50_85_3_6 epoch 3

------------------------------------------------

0.49259861946014555

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.5_0.35_100_140_50_85_3_6/checkpoint-12
searching parameters: task121_10_20_50_0.7_0.3_100_130_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.3_100_130_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:30:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:30:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:32:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:32:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 24
expected_example_num: 200
selection_ratio: 0.12
finetune_vicuna!
{'loss': 0.7812, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1743, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 166.4661, 'train_samples_per_second': 0.433, 'train_steps_per_second': 0.054, 'train_loss': 0.43707286318143207, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:36:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:36:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.3_100_130_50_75_3_6 epoch 1

------------------------------------------------

0.36459587067188487

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/generated_contents/1
INFO 11-26 10:36:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:37:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.3_100_130_50_75_3_6 epoch 2

------------------------------------------------

0.5667467995362448

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/generated_contents/2
INFO 11-26 10:37:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:37:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.3_100_130_50_75_3_6 epoch 3

------------------------------------------------

0.5602463433073223

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_100_130_50_75_3_6/checkpoint-9
searching parameters: task121_10_10_50_0.6_0.35_80_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_80_120_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:37:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:38:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:39:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:39:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 15
expected_example_num: 100
selection_ratio: 0.15
finetune_vicuna!
{'loss': 1.0496, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 162.7415, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.037, 'train_loss': 0.7818319896856943, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:43:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:43:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.35_80_120_50_80_3_6 epoch 1

------------------------------------------------

0.3295798246658703

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/generated_contents/1
INFO 11-26 10:43:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:43:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.35_80_120_50_80_3_6 epoch 2

------------------------------------------------

0.5864634611497811

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/generated_contents/2
INFO 11-26 10:44:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:44:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.35_80_120_50_80_3_6 epoch 3

------------------------------------------------

0.605694483287402

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_80_120_50_80_3_6/checkpoint-6
searching parameters: task121_10_10_50_0.6_0.35_100_130_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_100_130_50_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:44:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:44:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:46:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:46:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 3
expected_example_num: 100
selection_ratio: 0.03
finetune_vicuna!
{'train_runtime': 159.2606, 'train_samples_per_second': 0.057, 'train_steps_per_second': 0.019, 'train_loss': 0.27372294664382935, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:49:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:50:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.35_100_130_50_80_3_6 epoch 1

------------------------------------------------

0.613661508748223

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/generated_contents/1
INFO 11-26 10:50:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:50:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.35_100_130_50_80_3_6 epoch 2

------------------------------------------------

0.642333144228829

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/generated_contents/2
INFO 11-26 10:50:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:51:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.6_0.35_100_130_50_80_3_6 epoch 3

------------------------------------------------

0.6444833105741687

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.6_0.35_100_130_50_80_3_6/checkpoint-3
searching parameters: task121_10_20_50_0.7_0.35_100_120_45_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_45_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 10:51:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:51:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 10:52:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:53:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 11
expected_example_num: 200
selection_ratio: 0.055
finetune_vicuna!
{'loss': 1.1476, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 160.8003, 'train_samples_per_second': 0.205, 'train_steps_per_second': 0.037, 'train_loss': 0.9411772886912028, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:56:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:56:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.35_100_120_45_80_3_6 epoch 1

------------------------------------------------

0.360046825816911

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/generated_contents/1
INFO 11-26 10:56:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:57:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.35_100_120_45_80_3_6 epoch 2

------------------------------------------------

0.6851774124060768

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/generated_contents/2
INFO 11-26 10:57:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:57:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.35_100_120_45_80_3_6 epoch 3

------------------------------------------------

0.6797983819348705

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-4 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.35_100_120_45_80_3_6/checkpoint-6
searching parameters: task121_10_20_50_0.7_0.35_100_120_45_80_3_6
{'generation_epochs': 10, 'generation_batch_size': 20, 'generation_top_k': 50, 'generation_temperature': 0.7, 'min_frequency': 0.35, 'min_input_length': 100, 'max_input_length': 120, 'min_output_length': 45, 'max_output_length': 80, 'training_epochs': 3}
test best ckpt.
INFO 11-26 10:58:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task121_exp_6', tokenizer='/data2/cyzhao/best_ckpt/NI_task121_exp_6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:58:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task121_exp_6

------------------------------------------------

0.5722580762420943

------------------------------------------------


The best ckpt on test set gain 0.5722580762420943
Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/best_ckpt_generated_content
