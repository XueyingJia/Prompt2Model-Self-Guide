[2023-11-26 06:12:47,033] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_45_0.5_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.5_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.5_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:12:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:13:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:17:01,915] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_50_0.4_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.4_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.4_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:17:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:17:22 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:20:40,423] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_45_0.7_0.4_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/config.json
generate_and_write_inputs!
INFO 11-26 06:20:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:21:03 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:23:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:23:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 38
expected_example_num: 300
selection_ratio: 0.12666666666666668
finetune_vicuna!
{'loss': 0.8488, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2582, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1686, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 183.2427, 'train_samples_per_second': 0.622, 'train_steps_per_second': 0.082, 'train_loss': 0.35803285241127014, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:27:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:01 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 1

------------------------------------------------

0.47386203606812854

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/1
INFO 11-26 06:28:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 2

------------------------------------------------

0.6140774297053422

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/2
INFO 11-26 06:28:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:58 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 3

------------------------------------------------

0.6022900212123337

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15
searching parameters: task121_30_20_50_0.5_0.3_55_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.5_0.3_55_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.5_0.3_55_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:29:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:29:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:39:36,850] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_50_0.6_0.3_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/config.json
generate_and_write_inputs!
INFO 11-26 06:39:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:40:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:42:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:42:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 42
expected_example_num: 300
selection_ratio: 0.14
finetune_vicuna!
{'loss': 0.9131, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2706, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1809, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0709, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 189.7949, 'train_samples_per_second': 0.664, 'train_steps_per_second': 0.095, 'train_loss': 0.3254915161265267, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:46:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 1

------------------------------------------------

0.48281378766727984

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/1
INFO 11-26 06:47:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 2

------------------------------------------------

0.6159874308751637

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/2
INFO 11-26 06:47:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 3

------------------------------------------------

0.6087367413831614

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18
searching parameters: task121_30_10_40_0.7_0.35_50_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:48:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:50:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:51:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 57
expected_example_num: 300
selection_ratio: 0.19
finetune_vicuna!
{'loss': 0.8862, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.4745, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.182, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1536, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0834, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.1009, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 185.4357, 'train_samples_per_second': 0.922, 'train_steps_per_second': 0.129, 'train_loss': 0.31341806426644325, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:55:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:55:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 1

------------------------------------------------

0.5137478613418609

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/1
INFO 11-26 06:55:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 2

------------------------------------------------

0.5234433333658504

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/2
INFO 11-26 06:56:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 3

------------------------------------------------

0.5246105387327548

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24
searching parameters: task121_20_20_40_0.6_0.35_50_3_0.6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_50_3_0.6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_50_3_0.6/config.json
generate_and_write_inputs!
[2023-11-26 07:05:14,410] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_10_40_0.7_0.4_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/config.json
generate_and_write_inputs!
INFO 11-26 07:05:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:05:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
