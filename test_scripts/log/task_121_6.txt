[2023-11-26 06:12:47,033] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_45_0.5_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.5_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.5_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:12:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:13:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:17:01,915] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_50_0.4_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.4_0.4_55_3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.4_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:17:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:17:22 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:20:40,423] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_45_0.7_0.4_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/config.json
generate_and_write_inputs!
INFO 11-26 06:20:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:21:03 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:23:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:23:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 38
expected_example_num: 300
selection_ratio: 0.12666666666666668
finetune_vicuna!
{'loss': 0.8488, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2582, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1686, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 183.2427, 'train_samples_per_second': 0.622, 'train_steps_per_second': 0.082, 'train_loss': 0.35803285241127014, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:27:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:01 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 1

------------------------------------------------

0.47386203606812854

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/1
INFO 11-26 06:28:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 2

------------------------------------------------

0.6140774297053422

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/2
INFO 11-26 06:28:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:28:58 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.7_0.4_60_3_0.3 epoch 3

------------------------------------------------

0.6022900212123337

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_45_0.7_0.4_60_3_0.3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-10 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.7_0.4_60_3_0.3/checkpoint-15
searching parameters: task121_30_20_50_0.5_0.3_55_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.5_0.3_55_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.5_0.3_55_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:29:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:29:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:39:36,850] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_50_0.6_0.3_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/config.json
generate_and_write_inputs!
INFO 11-26 06:39:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:40:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:42:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:42:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 42
expected_example_num: 300
selection_ratio: 0.14
finetune_vicuna!
{'loss': 0.9131, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2706, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1809, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0709, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 189.7949, 'train_samples_per_second': 0.664, 'train_steps_per_second': 0.095, 'train_loss': 0.3254915161265267, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:46:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 1

------------------------------------------------

0.48281378766727984

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/1
INFO 11-26 06:47:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 2

------------------------------------------------

0.6159874308751637

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/2
INFO 11-26 06:47:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.6_0.3_60_3_0.3 epoch 3

------------------------------------------------

0.6087367413831614

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.6_0.3_60_3_0.3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.6_0.3_60_3_0.3/checkpoint-18
searching parameters: task121_30_10_40_0.7_0.35_50_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5
/home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:48:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:50:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:51:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 57
expected_example_num: 300
selection_ratio: 0.19
finetune_vicuna!
{'loss': 0.8862, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.4745, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.182, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1536, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0834, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.1009, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 185.4357, 'train_samples_per_second': 0.922, 'train_steps_per_second': 0.129, 'train_loss': 0.31341806426644325, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:55:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:55:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 1

------------------------------------------------

0.5137478613418609

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/1
INFO 11-26 06:55:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 2

------------------------------------------------

0.5234433333658504

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/2
INFO 11-26 06:56:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_50_3_0.5 epoch 3

------------------------------------------------

0.5246105387327548

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_40_0.7_0.35_50_3_0.5/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_50_3_0.5/checkpoint-24
searching parameters: task121_20_20_40_0.6_0.35_50_3_0.6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_50_3_0.6
/home/cyzhao/NI_task121_exp_6/task121_20_20_40_0.6_0.35_50_3_0.6/config.json
generate_and_write_inputs!
[2023-11-26 07:05:14,410] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_10_40_0.7_0.4_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/config.json
generate_and_write_inputs!
INFO 11-26 07:05:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:05:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:07:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:07:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 41
expected_example_num: 200
selection_ratio: 0.205
finetune_vicuna!
{'loss': 0.5583, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4243, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1881, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0618, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 186.8953, 'train_samples_per_second': 0.658, 'train_steps_per_second': 0.096, 'train_loss': 0.28146878878275555, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:11:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:11:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.7_0.4_50_3_0.5_6 epoch 1

------------------------------------------------

0.3858472483442352

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/generated_contents/1
INFO 11-26 07:12:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:12:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.7_0.4_50_3_0.5_6 epoch 2

------------------------------------------------

0.47052102660621464

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/generated_contents/2
INFO 11-26 07:12:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:13:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.7_0.4_50_3_0.5_6 epoch 3

------------------------------------------------

0.5339786138287099

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.7_0.4_50_3_0.5_6/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-18 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.7_0.4_50_3_0.5_6/checkpoint-12
searching parameters: task121_30_15_40_0.7_0.35_55_3_0.7_6
/home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6
/home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/config.json
generate_and_write_inputs!
INFO 11-26 07:13:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:13:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:16:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:16:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 153
expected_example_num: 450
selection_ratio: 0.34
finetune_vicuna!
{'loss': 1.4867, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}
{'loss': 0.5709, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.3777, 'learning_rate': 4e-05, 'epoch': 0.6}
{'loss': 0.3137, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2483, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1991, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.1749, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}
{'loss': 0.1395, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1125, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.181, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.086, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}
{'loss': 0.0754, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0668, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}
{'loss': 0.0613, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0911, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 234.2722, 'train_samples_per_second': 1.959, 'train_steps_per_second': 0.256, 'train_loss': 0.2789895509680112, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:22:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:22:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.7_0.35_55_3_0.7_6 epoch 1

------------------------------------------------

0.5777966418898915

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/generated_contents/1
INFO 11-26 07:22:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:23:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.7_0.35_55_3_0.7_6 epoch 2

------------------------------------------------

0.5818269677488697

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/generated_contents/2
INFO 11-26 07:23:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:23:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.7_0.35_55_3_0.7_6 epoch 3

------------------------------------------------

0.5824677753361006

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_15_40_0.7_0.35_55_3_0.7_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-60 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.7_0.35_55_3_0.7_6/checkpoint-40
searching parameters: task121_30_10_50_0.5_0.35_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/config.json
generate_and_write_inputs!
INFO 11-26 07:23:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:24:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:26:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:26:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 58
expected_example_num: 300
selection_ratio: 0.19333333333333333
finetune_vicuna!
{'loss': 0.7637, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.5959, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2278, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1217, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0697, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0648, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 191.5262, 'train_samples_per_second': 0.908, 'train_steps_per_second': 0.125, 'train_loss': 0.30728179092208546, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:30:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:30:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.35_50_3_0.5_6 epoch 1

------------------------------------------------

0.586048066600844

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/generated_contents/1
INFO 11-26 07:31:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:31:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.35_50_3_0.5_6 epoch 2

------------------------------------------------

0.6136896059990818

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/generated_contents/2
INFO 11-26 07:31:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:31:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.35_50_3_0.5_6 epoch 3

------------------------------------------------

0.5976522613035193

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_10_50_0.5_0.35_50_3_0.5_6/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_6
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-16 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.35_50_3_0.5_6/checkpoint-24
searching parameters: task121_30_20_45_0.5_0.3_60_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/config.json
generate_and_write_inputs!
INFO 11-26 07:32:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:32:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:35:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:36:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 113
expected_example_num: 600
selection_ratio: 0.18833333333333332
finetune_vicuna!
{'loss': 0.8603, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.5866, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.3601, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2512, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.2324, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1084, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1568, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.1096, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0974, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.091, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0776, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 216.0001, 'train_samples_per_second': 1.569, 'train_steps_per_second': 0.208, 'train_loss': 0.26786362793710494, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:40:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:41:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.5_0.3_60_3_0.5_6 epoch 1

------------------------------------------------

0.5977986933767747

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/generated_contents/1
INFO 11-26 07:41:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:41:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.5_0.3_60_3_0.5_6 epoch 2

------------------------------------------------

0.5407027854901897

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/generated_contents/2
INFO 11-26 07:41:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:42:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_45_0.5_0.3_60_3_0.5_6 epoch 3

------------------------------------------------

0.5471179054261001

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_30_20_45_0.5_0.3_60_3_0.5_6/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_45_0.5_0.3_60_3_0.5_6/checkpoint-45
searching parameters: task121_30_20_50_0.8_0.35_60_3_0.6_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.8_0.35_60_3_0.6_6
/home/cyzhao/NI_task121_exp_6/task121_30_20_50_0.8_0.35_60_3_0.6_6/config.json
generate_and_write_inputs!
INFO 11-26 07:42:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:42:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 07:57:37,235] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_40_0.5_0.3_100_130_45_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.5_0.3_100_130_45_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.5_0.3_100_130_45_75_3_6/config.json
generate_and_write_inputs!
[2023-11-26 07:59:50,735] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_40_0.4_0.3_80_120_40_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/config.json
generate_and_write_inputs!
INFO 11-26 07:59:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:00:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:01:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:01:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 43
expected_example_num: 200
selection_ratio: 0.215
finetune_vicuna!
{'loss': 0.6489, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2245, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1032, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0598, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 188.6228, 'train_samples_per_second': 0.684, 'train_steps_per_second': 0.095, 'train_loss': 0.23762818177541098, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:05:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:06:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.4_0.3_80_120_40_80_3_6 epoch 1

------------------------------------------------

0.612704781327531

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/generated_contents/1
INFO 11-26 08:06:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:06:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.4_0.3_80_120_40_80_3_6 epoch 2

------------------------------------------------

0.6468893515818783

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/generated_contents/2
INFO 11-26 08:06:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:07:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.4_0.3_80_120_40_80_3_6 epoch 3

------------------------------------------------

0.6601258344586295

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-18 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.4_0.3_80_120_40_80_3_6/checkpoint-12
searching parameters: task121_20_15_45_0.7_0.3_90_120_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_15_45_0.7_0.3_90_120_50_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_15_45_0.7_0.3_90_120_50_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:07:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:07:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 08:11:29,262] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_10_40_0.5_0.35_100_130_40_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6
/home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:11:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:11:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:13:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:13:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 16
expected_example_num: 200
selection_ratio: 0.08
finetune_vicuna!
{'loss': 0.6141, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 172.0702, 'train_samples_per_second': 0.279, 'train_steps_per_second': 0.035, 'train_loss': 0.4542270402113597, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:17:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:17:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_100_130_40_85_3_6 epoch 1

------------------------------------------------

0.06293747840097973

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/generated_contents/1
INFO 11-26 08:17:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:18:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_100_130_40_85_3_6 epoch 2

------------------------------------------------

0.47881824502451115

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/generated_contents/2
INFO 11-26 08:18:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:18:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_100_130_40_85_3_6 epoch 3

------------------------------------------------

0.5459133829218742

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_6/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-6 /data2/cyzhao/best_ckpt/NI_task121_exp_6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_100_130_40_85_3_6/checkpoint-4
searching parameters: task121_10_20_45_0.4_0.3_100_120_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.4_0.3_100_120_40_75_3_6
/home/cyzhao/NI_task121_exp_6/task121_10_20_45_0.4_0.3_100_120_40_75_3_6/config.json
generate_and_write_inputs!
INFO 11-26 08:18:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:19:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:21:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:21:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 0
expected_example_num: 200
selection_ratio: 0.0
finetune_vicuna!
[2023-11-26 08:29:49,159] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
