[2023-11-26 02:53:08,270] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_50_0.5_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_20_50_0.5_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_20_50_0.5_0.3_55_3/config.json
generate_and_write_inputs!
INFO 11-26 02:53:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 02:53:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 02:54:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 02:55:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 03:02:14,774] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_20_45_0.75_0.4_55_3
/home/cyzhao/NI_task121_exp_3/task121_20_20_45_0.75_0.4_55_3
/home/cyzhao/NI_task121_exp_3/task121_20_20_45_0.75_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 03:02:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:02:36 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 03:20:42,743] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_10_45_0.85_0.35_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_55_3/config.json
generate_and_write_inputs!
INFO 11-26 03:20:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:21:03 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 03:22:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:22:23 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 53
expected_example_num: 100
selection_ratio: 0.53
finetune_vicuna!
{'loss': 0.6372, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.2527, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1214, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1108, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0623, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 191.3283, 'train_samples_per_second': 0.831, 'train_steps_per_second': 0.11, 'train_loss': 0.2281791748745101, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-26 03:26:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:26:57 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_55_3 epoch 1

------------------------------------------------

0.5808361059121462

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_55_3/generated_contents/1
INFO 11-26 03:27:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:27:23 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_55_3 epoch 2

------------------------------------------------

0.5956994759367834

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_55_3/generated_contents/2
INFO 11-26 03:27:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:27:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_55_3 epoch 3

------------------------------------------------

0.6031947954748782

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_55_3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-21 /data2/cyzhao/best_ckpt/NI_task121_exp_3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_55_3/checkpoint-14
searching parameters: task121_20_20_45_0.9_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_20_20_45_0.9_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_20_20_45_0.9_0.4_60_3/config.json
generate_and_write_inputs!
INFO 11-26 03:28:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:28:23 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 03:31:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:31:33 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 195
expected_example_num: 400
selection_ratio: 0.4875
finetune_vicuna!
{'loss': 1.0474, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.16}
{'loss': 0.5688, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.32}
{'loss': 0.4259, 'learning_rate': 4.2e-05, 'epoch': 0.48}
{'loss': 0.513, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.64}
{'loss': 0.3543, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.2671, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.96}
{'loss': 0.2486, 'learning_rate': 3.1333333333333334e-05, 'epoch': 1.12}
{'loss': 0.1516, 'learning_rate': 2.8666666666666668e-05, 'epoch': 1.28}
{'loss': 0.14, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.44}
{'loss': 0.1701, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1255, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.76}
{'loss': 0.1095, 'learning_rate': 1.8e-05, 'epoch': 1.92}
{'loss': 0.0997, 'learning_rate': 1.5333333333333334e-05, 'epoch': 2.08}
{'loss': 0.052, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.24}
{'loss': 0.0468, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0518, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.56}
{'loss': 0.0372, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.72}
{'loss': 0.0638, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.88}
{'train_runtime': 266.2731, 'train_samples_per_second': 2.197, 'train_steps_per_second': 0.282, 'train_loss': 0.2410207058986028, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-75/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-50/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-25/optimizer.pt
validate!
last validate 0.
INFO 11-26 03:37:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-25', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-25', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:37:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_20_45_0.9_0.4_60_3 epoch 1

------------------------------------------------

0.589888400468681

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_20_45_0.9_0.4_60_3/generated_contents/1
INFO 11-26 03:38:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-50', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-50', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:38:22 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_20_45_0.9_0.4_60_3 epoch 2

------------------------------------------------

0.5841963047249906

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_20_45_0.9_0.4_60_3/generated_contents/2
INFO 11-26 03:38:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-75', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-75', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:38:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_20_45_0.9_0.4_60_3 epoch 3

------------------------------------------------

0.5849763183211708

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_20_45_0.9_0.4_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-25
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-50
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.9_0.4_60_3/checkpoint-75
searching parameters: task121_10_10_50_0.85_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_50_0.85_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_50_0.85_0.3_55_3/config.json
generate_and_write_inputs!
INFO 11-26 03:39:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:39:20 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 03:40:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:40:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 54
expected_example_num: 100
selection_ratio: 0.54
finetune_vicuna!
{'loss': 0.6733, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.3058, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1269, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0492, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0671, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 193.7899, 'train_samples_per_second': 0.836, 'train_steps_per_second': 0.108, 'train_loss': 0.23519396196518624, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-26 03:44:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:45:13 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_50_0.85_0.3_55_3 epoch 1

------------------------------------------------

0.5160780722651679

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_50_0.85_0.3_55_3/generated_contents/1
INFO 11-26 03:45:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:45:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_50_0.85_0.3_55_3 epoch 2

------------------------------------------------

0.5269825046513259

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_50_0.85_0.3_55_3/generated_contents/2
INFO 11-26 03:45:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:46:05 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_50_0.85_0.3_55_3 epoch 3

------------------------------------------------

0.5396829130284

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_50_0.85_0.3_55_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.85_0.3_55_3/checkpoint-21
searching parameters: task121_20_10_50_0.85_0.4_55_3
/home/cyzhao/NI_task121_exp_3/task121_20_10_50_0.85_0.4_55_3
/home/cyzhao/NI_task121_exp_3/task121_20_10_50_0.85_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 03:46:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:46:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 03:48:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:48:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 91
expected_example_num: 200
selection_ratio: 0.455
finetune_vicuna!
{'loss': 0.539, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.3026, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2727, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1248, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1169, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1108, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.051, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0692, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0423, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 206.3063, 'train_samples_per_second': 1.323, 'train_steps_per_second': 0.174, 'train_loss': 0.18101587601833874, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 03:53:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:53:40 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_10_50_0.85_0.4_55_3 epoch 1

------------------------------------------------

0.5102281938840464

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_10_50_0.85_0.4_55_3/generated_contents/1
INFO 11-26 03:53:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:54:17 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_10_50_0.85_0.4_55_3 epoch 2

------------------------------------------------

0.5727291876953585

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_10_50_0.85_0.4_55_3/generated_contents/2
INFO 11-26 03:54:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:54:46 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_10_50_0.85_0.4_55_3 epoch 3

------------------------------------------------

0.5615228101610733

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_10_50_0.85_0.4_55_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.85_0.4_55_3/checkpoint-36
searching parameters: task121_20_15_40_0.95_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_20_15_40_0.95_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_20_15_40_0.95_0.3_55_3/config.json
generate_and_write_inputs!
INFO 11-26 03:55:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:55:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 03:57:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 03:58:02 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 172
expected_example_num: 300
selection_ratio: 0.5733333333333334
finetune_vicuna!
{'loss': 1.1134, 'learning_rate': 4.696969696969697e-05, 'epoch': 0.18}
{'loss': 0.5132, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.4669, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.55}
{'loss': 0.386, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.5107, 'learning_rate': 3.484848484848485e-05, 'epoch': 0.91}
{'loss': 0.2279, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.1569, 'learning_rate': 2.878787878787879e-05, 'epoch': 1.27}
{'loss': 0.1519, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1343, 'learning_rate': 2.272727272727273e-05, 'epoch': 1.64}
{'loss': 0.1538, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.0952, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0542, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0397, 'learning_rate': 1.0606060606060607e-05, 'epoch': 2.36}
{'loss': 0.0686, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0535, 'learning_rate': 4.5454545454545455e-06, 'epoch': 2.73}
{'loss': 0.0497, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 248.2281, 'train_samples_per_second': 2.079, 'train_steps_per_second': 0.266, 'train_loss': 0.2546561297142144, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-44/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-22/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-66/optimizer.pt
validate!
last validate 0.
INFO 11-26 04:03:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:03:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_15_40_0.95_0.3_55_3 epoch 1

------------------------------------------------

0.5888942691538113

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_15_40_0.95_0.3_55_3/generated_contents/1
INFO 11-26 04:04:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-44', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:04:28 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_15_40_0.95_0.3_55_3 epoch 2

------------------------------------------------

0.6004317330448715

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_15_40_0.95_0.3_55_3/generated_contents/2
INFO 11-26 04:04:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-66', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-66', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:04:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_15_40_0.95_0.3_55_3 epoch 3

------------------------------------------------

0.5790790786034848

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_15_40_0.95_0.3_55_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-44
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.95_0.3_55_3/checkpoint-66
searching parameters: task121_20_20_40_0.95_0.4_50_3
/home/cyzhao/NI_task121_exp_3/task121_20_20_40_0.95_0.4_50_3
/home/cyzhao/NI_task121_exp_3/task121_20_20_40_0.95_0.4_50_3/config.json
generate_and_write_inputs!
INFO 11-26 04:05:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:05:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 04:09:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:09:28 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 198
expected_example_num: 400
selection_ratio: 0.495
finetune_vicuna!
{'loss': 1.1757, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.16}
{'loss': 0.6653, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.32}
{'loss': 0.6057, 'learning_rate': 4.2e-05, 'epoch': 0.48}
{'loss': 0.4947, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.64}
{'loss': 0.404, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.4143, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.96}
{'loss': 0.2333, 'learning_rate': 3.1333333333333334e-05, 'epoch': 1.12}
{'loss': 0.1498, 'learning_rate': 2.8666666666666668e-05, 'epoch': 1.28}
{'loss': 0.1301, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.44}
{'loss': 0.1537, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1108, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.76}
{'loss': 0.1453, 'learning_rate': 1.8e-05, 'epoch': 1.92}
{'loss': 0.0989, 'learning_rate': 1.5333333333333334e-05, 'epoch': 2.08}
{'loss': 0.0397, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.24}
{'loss': 0.0313, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0354, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.56}
{'loss': 0.0369, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.72}
{'loss': 0.0389, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.88}
{'train_runtime': 262.1659, 'train_samples_per_second': 2.266, 'train_steps_per_second': 0.286, 'train_loss': 0.2665385973453522, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-75/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-50/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-25/optimizer.pt
validate!
last validate 0.
INFO 11-26 04:15:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-25', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-25', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:15:41 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_20_40_0.95_0.4_50_3 epoch 1

------------------------------------------------

0.5723509978922807

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_20_40_0.95_0.4_50_3/generated_contents/1
INFO 11-26 04:15:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-50', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-50', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:16:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_20_40_0.95_0.4_50_3 epoch 2

------------------------------------------------

0.5676243810940196

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_20_40_0.95_0.4_50_3/generated_contents/2
INFO 11-26 04:16:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-75', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-75', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:16:33 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_20_40_0.95_0.4_50_3 epoch 3

------------------------------------------------

0.5800602670665741

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_20_40_0.95_0.4_50_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-25
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-50
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_40_0.95_0.4_50_3/checkpoint-75
searching parameters: task121_20_15_50_0.85_0.4_50_3
/home/cyzhao/NI_task121_exp_3/task121_20_15_50_0.85_0.4_50_3
/home/cyzhao/NI_task121_exp_3/task121_20_15_50_0.85_0.4_50_3/config.json
generate_and_write_inputs!
INFO 11-26 04:16:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:17:04 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 04:19:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:19:35 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 165
expected_example_num: 300
selection_ratio: 0.55
finetune_vicuna!
{'loss': 0.8549, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.532, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.3339, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.5165, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.4293, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.194, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1519, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1489, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.1094, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1316, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.0941, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0645, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0593, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0646, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0623, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 238.102, 'train_samples_per_second': 2.079, 'train_steps_per_second': 0.265, 'train_loss': 0.2410548362467024, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-26 04:25:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:25:23 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_15_50_0.85_0.4_50_3 epoch 1

------------------------------------------------

0.5130854740990722

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_15_50_0.85_0.4_50_3/generated_contents/1
INFO 11-26 04:25:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:25:50 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_15_50_0.85_0.4_50_3 epoch 2

------------------------------------------------

0.5907738724759687

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_15_50_0.85_0.4_50_3/generated_contents/2
INFO 11-26 04:26:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:26:18 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_20_15_50_0.85_0.4_50_3 epoch 3

------------------------------------------------

0.5783890070453809

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_20_15_50_0.85_0.4_50_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-21
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.85_0.4_50_3/checkpoint-63
searching parameters: task121_10_20_40_0.85_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_20_40_0.85_0.3_55_3
/home/cyzhao/NI_task121_exp_3/task121_10_20_40_0.85_0.3_55_3/config.json
generate_and_write_inputs!
INFO 11-26 04:26:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:26:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 04:28:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:28:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 108
expected_example_num: 200
selection_ratio: 0.54
finetune_vicuna!
{'loss': 0.9348, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.29}
{'loss': 0.4901, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4494, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.86}
{'loss': 0.2644, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1476, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}
{'loss': 0.1178, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1129, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0531, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0489, 'learning_rate': 7.142857142857143e-06, 'epoch': 2.57}
{'loss': 0.0503, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 209.8723, 'train_samples_per_second': 1.544, 'train_steps_per_second': 0.2, 'train_loss': 0.2560817272890182, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-26 04:33:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:33:32 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_20_40_0.85_0.3_55_3 epoch 1

------------------------------------------------

0.5918890205543358

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_20_40_0.85_0.3_55_3/generated_contents/1
INFO 11-26 04:33:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:33:57 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_20_40_0.85_0.3_55_3 epoch 2

------------------------------------------------

0.5933876970757239

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_20_40_0.85_0.3_55_3/generated_contents/2
INFO 11-26 04:34:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:34:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_20_40_0.85_0.3_55_3 epoch 3

------------------------------------------------

0.5830403441312041

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_20_40_0.85_0.3_55_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-28
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.85_0.3_55_3/checkpoint-42
searching parameters: task121_30_10_40_0.85_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.85_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.85_0.4_60_3/config.json
generate_and_write_inputs!
INFO 11-26 04:34:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:35:01 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 04:38:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:38:32 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 162
expected_example_num: 300
selection_ratio: 0.54
finetune_vicuna!
{'loss': 0.4224, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.2119, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.3359, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.2019, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.3169, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.066, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0935, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0967, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.0679, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0703, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.0534, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0506, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0317, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0298, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0318, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 241.7692, 'train_samples_per_second': 2.01, 'train_steps_per_second': 0.261, 'train_loss': 0.13344161519928585, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-26 04:44:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:44:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_40_0.85_0.4_60_3 epoch 1

------------------------------------------------

0.6293105992450073

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.85_0.4_60_3/generated_contents/1
INFO 11-26 04:44:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:44:55 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_40_0.85_0.4_60_3 epoch 2

------------------------------------------------

0.6156818822642931

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.85_0.4_60_3/generated_contents/2
INFO 11-26 04:45:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:45:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_40_0.85_0.4_60_3 epoch 3

------------------------------------------------

0.5993385547728942

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.85_0.4_60_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-21 /data2/cyzhao/best_ckpt/NI_task121_exp_3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.85_0.4_60_3/checkpoint-63
searching parameters: task121_30_10_50_0.95_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_50_0.95_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_50_0.95_0.4_60_3/config.json
generate_and_write_inputs!
INFO 11-26 04:45:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:45:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 04:49:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:49:24 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 164
expected_example_num: 300
selection_ratio: 0.5466666666666666
finetune_vicuna!
{'loss': 0.7319, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.5939, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.3843, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.3753, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.482, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.3117, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.164, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1876, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.099, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1123, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.1232, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0632, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0515, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0413, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0469, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 237.574, 'train_samples_per_second': 2.071, 'train_steps_per_second': 0.265, 'train_loss': 0.24115551724320367, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-26 04:54:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:55:06 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_50_0.95_0.4_60_3 epoch 1

------------------------------------------------

0.5727006681023412

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_50_0.95_0.4_60_3/generated_contents/1
INFO 11-26 04:55:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:55:32 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_50_0.95_0.4_60_3 epoch 2

------------------------------------------------

0.5840852845602611

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_50_0.95_0.4_60_3/generated_contents/2
INFO 11-26 04:55:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:55:57 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_50_0.95_0.4_60_3 epoch 3

------------------------------------------------

0.5764260740875191

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_50_0.95_0.4_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-21
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.95_0.4_60_3/checkpoint-63
searching parameters: task121_30_10_40_0.9_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.9_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.9_0.35_60_3/config.json
generate_and_write_inputs!
INFO 11-26 04:56:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:56:27 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 04:59:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 04:59:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 149
expected_example_num: 300
selection_ratio: 0.49666666666666665
finetune_vicuna!
{'loss': 0.8539, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.4837, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.4173, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.3858, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.3258, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.1487, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.0995, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.1201, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.1525, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.0551, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0463, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.049, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0447, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.055, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 229.2545, 'train_samples_per_second': 1.95, 'train_steps_per_second': 0.249, 'train_loss': 0.2277561342507078, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-26 05:05:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:05:19 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_40_0.9_0.35_60_3 epoch 1

------------------------------------------------

0.5339812315876656

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.9_0.35_60_3/generated_contents/1
INFO 11-26 05:05:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:05:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_40_0.9_0.35_60_3 epoch 2

------------------------------------------------

0.5974197847223658

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.9_0.35_60_3/generated_contents/2
INFO 11-26 05:06:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:06:20 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_40_0.9_0.35_60_3 epoch 3

------------------------------------------------

0.5886172474517659

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_40_0.9_0.35_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.9_0.35_60_3/checkpoint-57
searching parameters: task121_10_10_45_0.85_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_60_3/config.json
generate_and_write_inputs!
INFO 11-26 05:06:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:06:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 05:07:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:08:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 49
expected_example_num: 100
selection_ratio: 0.49
finetune_vicuna!
{'loss': 0.5424, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4056, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1057, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0879, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0418, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 183.4247, 'train_samples_per_second': 0.801, 'train_steps_per_second': 0.114, 'train_loss': 0.22756192709008852, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-26 05:12:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:12:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_60_3 epoch 1

------------------------------------------------

0.6452097777807559

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_60_3/generated_contents/1
INFO 11-26 05:12:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:12:55 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_60_3 epoch 2

------------------------------------------------

0.6224098991912649

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_60_3/generated_contents/2
INFO 11-26 05:13:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:13:23 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_60_3 epoch 3

------------------------------------------------

0.630246165067023

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_60_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_3
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-7 /data2/cyzhao/best_ckpt/NI_task121_exp_3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_60_3/checkpoint-21
searching parameters: task121_30_10_45_0.85_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_45_0.85_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_10_45_0.85_0.35_60_3/config.json
generate_and_write_inputs!
INFO 11-26 05:13:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:13:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 05:16:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:16:44 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 145
expected_example_num: 300
selection_ratio: 0.48333333333333334
finetune_vicuna!
{'loss': 0.5695, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.3627, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.2628, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.2656, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.2571, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.139, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.085, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.0867, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.1071, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.0352, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0364, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.0546, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0499, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0545, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 231.1705, 'train_samples_per_second': 1.882, 'train_steps_per_second': 0.247, 'train_loss': 0.16794765230856443, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-26 05:22:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:22:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.85_0.35_60_3 epoch 1

------------------------------------------------

0.5447654364033905

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_45_0.85_0.35_60_3/generated_contents/1
INFO 11-26 05:22:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:22:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.85_0.35_60_3 epoch 2

------------------------------------------------

0.5346234562641928

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_45_0.85_0.35_60_3/generated_contents/2
INFO 11-26 05:23:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:23:19 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_10_45_0.85_0.35_60_3 epoch 3

------------------------------------------------

0.5375562095695308

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_10_45_0.85_0.35_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.85_0.35_60_3/checkpoint-57
searching parameters: task121_30_10_45_0.85_0.35_60_3
searching parameters: task121_10_10_40_0.85_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_40_0.85_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_40_0.85_0.35_60_3/config.json
generate_and_write_inputs!
INFO 11-26 05:23:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:23:50 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 05:24:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:25:13 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 48
expected_example_num: 100
selection_ratio: 0.48
finetune_vicuna!
{'loss': 0.5407, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.173, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1205, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0394, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 182.6408, 'train_samples_per_second': 0.788, 'train_steps_per_second': 0.099, 'train_loss': 0.1980116061038441, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 05:29:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:29:31 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_40_0.85_0.35_60_3 epoch 1

------------------------------------------------

0.6030555455071313

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_40_0.85_0.35_60_3/generated_contents/1
INFO 11-26 05:29:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:29:58 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_40_0.85_0.35_60_3 epoch 2

------------------------------------------------

0.5712316852845044

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_40_0.85_0.35_60_3/generated_contents/2
INFO 11-26 05:30:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:30:27 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_40_0.85_0.35_60_3 epoch 3

------------------------------------------------

0.5737368933409699

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_40_0.85_0.35_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_40_0.85_0.35_60_3/checkpoint-18
searching parameters: task121_30_15_45_0.9_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_15_45_0.9_0.4_60_3
/home/cyzhao/NI_task121_exp_3/task121_30_15_45_0.9_0.4_60_3/config.json
generate_and_write_inputs!
INFO 11-26 05:30:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:30:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 05:35:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:35:20 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 253
expected_example_num: 450
selection_ratio: 0.5622222222222222
finetune_vicuna!
{'loss': 0.9385, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.12}
{'loss': 0.6565, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 0.3553, 'learning_rate': 4.375e-05, 'epoch': 0.38}
{'loss': 0.3302, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.3438, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.62}
{'loss': 0.3914, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.3679, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.88}
{'loss': 0.2754, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1537, 'learning_rate': 3.125e-05, 'epoch': 1.12}
{'loss': 0.0974, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.1308, 'learning_rate': 2.7083333333333332e-05, 'epoch': 1.38}
{'loss': 0.1413, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1869, 'learning_rate': 2.2916666666666667e-05, 'epoch': 1.62}
{'loss': 0.1361, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.122, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}
{'loss': 0.1065, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.063, 'learning_rate': 1.4583333333333335e-05, 'epoch': 2.12}
{'loss': 0.058, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.0547, 'learning_rate': 1.0416666666666668e-05, 'epoch': 2.38}
{'loss': 0.0717, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0536, 'learning_rate': 6.25e-06, 'epoch': 2.62}
{'loss': 0.0634, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.0522, 'learning_rate': 2.0833333333333334e-06, 'epoch': 2.88}
{'loss': 0.0525, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 281.4171, 'train_samples_per_second': 2.697, 'train_steps_per_second': 0.341, 'train_loss': 0.2167809292053183, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-96/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-64/optimizer.pt
validate!
last validate 0.
INFO 11-26 05:42:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:42:37 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_15_45_0.9_0.4_60_3 epoch 1

------------------------------------------------

0.5675829104306049

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_15_45_0.9_0.4_60_3/generated_contents/1
INFO 11-26 05:42:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-64', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-64', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:43:06 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_15_45_0.9_0.4_60_3 epoch 2

------------------------------------------------

0.5553242608507866

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_15_45_0.9_0.4_60_3/generated_contents/2
INFO 11-26 05:43:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-96', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-96', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:43:35 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_15_45_0.9_0.4_60_3 epoch 3

------------------------------------------------

0.558080103356485

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_30_15_45_0.9_0.4_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-64
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.9_0.4_60_3/checkpoint-96
searching parameters: task121_10_10_45_0.85_0.35_50_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_50_3
/home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_50_3/config.json
generate_and_write_inputs!
INFO 11-26 05:43:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:44:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 05:45:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:45:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 50
expected_example_num: 100
selection_ratio: 0.5
finetune_vicuna!
{'loss': 0.6461, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4532, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2072, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1032, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0613, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 178.7119, 'train_samples_per_second': 0.839, 'train_steps_per_second': 0.118, 'train_loss': 0.28307490547498065, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-26 05:49:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:49:42 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_50_3 epoch 1

------------------------------------------------

0.5890405848069018

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_50_3/generated_contents/1
INFO 11-26 05:49:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:50:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_50_3 epoch 2

------------------------------------------------

0.6198331926136356

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_50_3/generated_contents/2
INFO 11-26 05:50:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:50:40 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_10_45_0.85_0.35_50_3 epoch 3

------------------------------------------------

0.6080920451531859

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_10_45_0.85_0.35_50_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.85_0.35_50_3/checkpoint-21
searching parameters: task121_30_10_40_0.85_0.4_60_3
searching parameters: task121_10_20_45_0.95_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_20_45_0.95_0.35_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_20_45_0.95_0.35_60_3/config.json
generate_and_write_inputs!
INFO 11-26 05:51:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:51:22 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 05:52:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:53:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 102
expected_example_num: 200
selection_ratio: 0.51
finetune_vicuna!
{'loss': 1.2987, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.647, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.5584, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.2808, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.156, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1553, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1255, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0529, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0647, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 203.1795, 'train_samples_per_second': 1.506, 'train_steps_per_second': 0.192, 'train_loss': 0.34534966754607666, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-26 05:57:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:58:10 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_20_45_0.95_0.35_60_3 epoch 1

------------------------------------------------

0.6038034592792004

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_20_45_0.95_0.35_60_3/generated_contents/1
INFO 11-26 05:58:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:58:40 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_20_45_0.95_0.35_60_3 epoch 2

------------------------------------------------

0.5762826134213394

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_20_45_0.95_0.35_60_3/generated_contents/2
INFO 11-26 05:58:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:59:09 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_20_45_0.95_0.35_60_3 epoch 3

------------------------------------------------

0.5839611358588148

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_20_45_0.95_0.35_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.95_0.35_60_3/checkpoint-39
searching parameters: task121_10_15_40_0.9_0.3_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_15_40_0.9_0.3_60_3
/home/cyzhao/NI_task121_exp_3/task121_10_15_40_0.9_0.3_60_3/config.json
generate_and_write_inputs!
INFO 11-26 05:59:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 05:59:42 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:01:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:01:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 72
expected_example_num: 150
selection_ratio: 0.48
finetune_vicuna!
{'loss': 0.8778, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.4503, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.226, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1907, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1334, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.1071, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 189.5846, 'train_samples_per_second': 1.139, 'train_steps_per_second': 0.142, 'train_loss': 0.30325546198421055, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:06:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:06:34 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_15_40_0.9_0.3_60_3 epoch 1

------------------------------------------------

0.5049963205646764

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_15_40_0.9_0.3_60_3/generated_contents/1
INFO 11-26 06:06:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:07:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_15_40_0.9_0.3_60_3 epoch 2

------------------------------------------------

0.5531735536802116

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_15_40_0.9_0.3_60_3/generated_contents/2
INFO 11-26 06:07:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:07:40 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_10_15_40_0.9_0.3_60_3 epoch 3

------------------------------------------------

0.5410169205851904

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/task121_10_15_40_0.9_0.3_60_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.9_0.3_60_3/checkpoint-27
{'generation_epochs': 10, 'generation_batch_size': 10, 'generation_top_k': 45, 'generation_temperature': 0.85, 'min_frequency': 0.35, 'min_input_length': 60, 'training_epochs': 3}
test best ckpt.
INFO 11-26 06:07:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task121_exp_3', tokenizer='/data2/cyzhao/best_ckpt/NI_task121_exp_3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:08:15 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task121_exp_3

------------------------------------------------

0.5655243671035022

------------------------------------------------


The best ckpt on test set gain 0.5655243671035022
Genrated contents are stored in /home/cyzhao/NI_task121_exp_3/best_ckpt_generated_content
