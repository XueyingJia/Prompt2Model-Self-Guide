[2023-11-26 06:11:55,637] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_10_40_0.7_0.3_55_3
/home/cyzhao/NI_task121_exp_4/task121_20_10_40_0.7_0.3_55_3
/home/cyzhao/NI_task121_exp_4/task121_20_10_40_0.7_0.3_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:12:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:12:17 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:15:36,430] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_20_45_0.6_0.4_55_3
/home/cyzhao/NI_task121_exp_4/task121_10_20_45_0.6_0.4_55_3
/home/cyzhao/NI_task121_exp_4/task121_10_20_45_0.6_0.4_55_3/config.json
generate_and_write_inputs!
INFO 11-26 06:15:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:15:57 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:17:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:17:55 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 44
expected_example_num: 200
selection_ratio: 0.22
finetune_deepseek!
[2023-11-26 06:20:15,118] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_20_50_0.6_0.35_60_3_0.5
/home/cyzhao/NI_task121_exp_4/task121_30_20_50_0.6_0.35_60_3_0.5
/home/cyzhao/NI_task121_exp_4/task121_30_20_50_0.6_0.35_60_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:20:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:20:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:24:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:25:10 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 103
expected_example_num: 600
selection_ratio: 0.17166666666666666
finetune_deepseek!
{'loss': 1.6605, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.6445, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.3731, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.2715, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.1953, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.2191, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1522, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.1004, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0921, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 211.9988, 'train_samples_per_second': 1.458, 'train_steps_per_second': 0.184, 'train_loss': 0.38894318235226166, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:30:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:30:16 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_20_50_0.6_0.35_60_3_0.5 epoch 1

------------------------------------------------

0.45402411869060105

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_20_50_0.6_0.35_60_3_0.5/generated_contents/1
INFO 11-26 06:30:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:30:50 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_20_50_0.6_0.35_60_3_0.5 epoch 2

------------------------------------------------

0.5031284856917501

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_20_50_0.6_0.35_60_3_0.5/generated_contents/2
INFO 11-26 06:31:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:31:24 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of task121_30_20_50_0.6_0.35_60_3_0.5 epoch 3

------------------------------------------------

0.5026146141033433

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_20_50_0.6_0.35_60_3_0.5/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-26 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_50_0.6_0.35_60_3_0.5/checkpoint-39
searching parameters: task121_30_15_50_0.4_0.4_55_3_0.8
/home/cyzhao/NI_task121_exp_4/task121_30_15_50_0.4_0.4_55_3_0.8
/home/cyzhao/NI_task121_exp_4/task121_30_15_50_0.4_0.4_55_3_0.8/config.json
generate_and_write_inputs!
INFO 11-26 06:31:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:31:54 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-26 06:39:07,726] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_15_40_0.8_0.3_50_3_0.6
/home/cyzhao/NI_task121_exp_4/task121_20_15_40_0.8_0.3_50_3_0.6
/home/cyzhao/NI_task121_exp_4/task121_20_15_40_0.8_0.3_50_3_0.6/config.json
generate_and_write_inputs!
INFO 11-26 06:39:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:39:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:41:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:41:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 72
expected_example_num: 300
selection_ratio: 0.24
finetune_deepseek!
{'loss': 0.8647, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.3942, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.1599, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1244, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1085, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0712, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 198.636, 'train_samples_per_second': 1.087, 'train_steps_per_second': 0.136, 'train_loss': 0.26077419298666493, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:46:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:46:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.8_0.3_50_3_0.6 epoch 1

------------------------------------------------

0.5421026601333654

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_40_0.8_0.3_50_3_0.6/generated_contents/1
INFO 11-26 06:46:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.8_0.3_50_3_0.6 epoch 2

------------------------------------------------

0.6062940120892479

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_40_0.8_0.3_50_3_0.6/generated_contents/2
INFO 11-26 06:47:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:47:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_40_0.8_0.3_50_3_0.6 epoch 3

------------------------------------------------

0.608897455088956

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_40_0.8_0.3_50_3_0.6/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-27 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_40_0.8_0.3_50_3_0.6/checkpoint-18
searching parameters: task121_30_15_40_0.8_0.4_50_3_0.5
/home/cyzhao/NI_task121_exp_4/task121_30_15_40_0.8_0.4_50_3_0.5
/home/cyzhao/NI_task121_exp_4/task121_30_15_40_0.8_0.4_50_3_0.5/config.json
generate_and_write_inputs!
INFO 11-26 06:47:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:48:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 06:51:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:51:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 91
expected_example_num: 450
selection_ratio: 0.20222222222222222
finetune_deepseek!
{'loss': 0.6491, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.3803, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.328, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1865, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1686, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1057, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0593, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0747, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0655, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 206.4295, 'train_samples_per_second': 1.322, 'train_steps_per_second': 0.174, 'train_loss': 0.22419920398129356, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 06:56:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.8_0.4_50_3_0.5 epoch 1

------------------------------------------------

0.37185994060102795

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_40_0.8_0.4_50_3_0.5/generated_contents/1
INFO 11-26 06:56:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:56:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.8_0.4_50_3_0.5 epoch 2

------------------------------------------------

0.602877753952785

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_40_0.8_0.4_50_3_0.5/generated_contents/2
INFO 11-26 06:57:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 06:57:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_40_0.8_0.4_50_3_0.5 epoch 3

------------------------------------------------

0.6065576219395836

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_40_0.8_0.4_50_3_0.5/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_40_0.8_0.4_50_3_0.5/checkpoint-36
searching parameters: task121_10_10_45_0.5_0.35_60_3_0.5
/home/cyzhao/NI_task121_exp_4/task121_10_10_45_0.5_0.35_60_3_0.5
/home/cyzhao/NI_task121_exp_4/task121_10_10_45_0.5_0.35_60_3_0.5/config.json
generate_and_write_inputs!
[2023-11-26 07:00:58,157] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_40_0.7_0.4_60_3_0.3_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.4_60_3_0.3_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.4_60_3_0.3_4/config.json
generate_and_write_inputs!
INFO 11-26 07:01:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:01:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:03:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:03:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 36
expected_example_num: 300
selection_ratio: 0.12
finetune_deepseek!
{'loss': 0.8937, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.237, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1186, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 178.6493, 'train_samples_per_second': 0.605, 'train_steps_per_second': 0.084, 'train_loss': 0.3496215949455897, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:07:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:08:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.4_60_3_0.3_4 epoch 1

------------------------------------------------

0.5423022205444414

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.4_60_3_0.3_4/generated_contents/1
INFO 11-26 07:08:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:08:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.4_60_3_0.3_4 epoch 2

------------------------------------------------

0.6141911806292408

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.4_60_3_0.3_4/generated_contents/2
INFO 11-26 07:08:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:09:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.4_60_3_0.3_4 epoch 3

------------------------------------------------

0.6178494005507443

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.4_60_3_0.3_4/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-15 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.4_60_3_0.3_4/checkpoint-10
searching parameters: task121_30_10_45_0.6_0.4_55_3_0.7_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.4_55_3_0.7_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.4_55_3_0.7_4/config.json
generate_and_write_inputs!
INFO 11-26 07:09:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:09:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:11:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:12:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 110
expected_example_num: 300
selection_ratio: 0.36666666666666664
finetune_deepseek!
{'loss': 0.5749, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.29}
{'loss': 0.293, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.2834, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.86}
{'loss': 0.1577, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1096, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}
{'loss': 0.1269, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1284, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0535, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0479, 'learning_rate': 7.142857142857143e-06, 'epoch': 2.57}
{'loss': 0.057, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 215.8745, 'train_samples_per_second': 1.529, 'train_steps_per_second': 0.195, 'train_loss': 0.17710505248535247, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:17:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:17:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.6_0.4_55_3_0.7_4 epoch 1

------------------------------------------------

0.6046287024145763

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.4_55_3_0.7_4/generated_contents/1
INFO 11-26 07:17:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:17:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.6_0.4_55_3_0.7_4 epoch 2

------------------------------------------------

0.5913644774313593

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.4_55_3_0.7_4/generated_contents/2
INFO 11-26 07:18:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:18:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.6_0.4_55_3_0.7_4 epoch 3

------------------------------------------------

0.6037800573525852

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.4_55_3_0.7_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-28
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.4_55_3_0.7_4/checkpoint-42
searching parameters: task121_30_10_45_0.6_0.35_60_3_0.4_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.35_60_3_0.4_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.35_60_3_0.4_4/config.json
generate_and_write_inputs!
INFO 11-26 07:18:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:18:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:21:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:21:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 51
expected_example_num: 300
selection_ratio: 0.17
finetune_deepseek!
{'loss': 1.4683, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4063, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2684, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1621, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.1011, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 185.1053, 'train_samples_per_second': 0.827, 'train_steps_per_second': 0.113, 'train_loss': 0.46381084798347383, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:25:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:25:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.6_0.35_60_3_0.4_4 epoch 1

------------------------------------------------

0.48397557608360914

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.35_60_3_0.4_4/generated_contents/1
INFO 11-26 07:25:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:25:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.6_0.35_60_3_0.4_4 epoch 2

------------------------------------------------

0.4711831925998554

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.35_60_3_0.4_4/generated_contents/2
INFO 11-26 07:26:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:26:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_45_0.6_0.35_60_3_0.4_4 epoch 3

------------------------------------------------

0.49760193243474554

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_45_0.6_0.35_60_3_0.4_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_45_0.6_0.35_60_3_0.4_4/checkpoint-21
searching parameters: task121_30_10_50_0.5_0.3_50_3_0.3_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_50_0.5_0.3_50_3_0.3_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_50_0.5_0.3_50_3_0.3_4/config.json
generate_and_write_inputs!
INFO 11-26 07:26:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:26:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:29:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:29:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 39
expected_example_num: 300
selection_ratio: 0.13
finetune_deepseek!
{'loss': 0.7979, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.5606, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1308, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 180.5723, 'train_samples_per_second': 0.648, 'train_steps_per_second': 0.083, 'train_loss': 0.418443763256073, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:33:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:33:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.3_50_3_0.3_4 epoch 1

------------------------------------------------

0.5613307081929166

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_50_0.5_0.3_50_3_0.3_4/generated_contents/1
INFO 11-26 07:33:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:34:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.3_50_3_0.3_4 epoch 2

------------------------------------------------

0.605166162535566

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_50_0.5_0.3_50_3_0.3_4/generated_contents/2
INFO 11-26 07:34:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:34:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_50_0.5_0.3_50_3_0.3_4 epoch 3

------------------------------------------------

0.6164771787596115

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_50_0.5_0.3_50_3_0.3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_50_0.5_0.3_50_3_0.3_4/checkpoint-15
searching parameters: task121_20_20_45_0.8_0.3_55_3_0.3_4
/home/cyzhao/NI_task121_exp_4/task121_20_20_45_0.8_0.3_55_3_0.3_4
/home/cyzhao/NI_task121_exp_4/task121_20_20_45_0.8_0.3_55_3_0.3_4/config.json
generate_and_write_inputs!
INFO 11-26 07:34:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:35:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 07:38:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:38:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 69
expected_example_num: 400
selection_ratio: 0.1725
finetune_deepseek!
{'loss': 1.4181, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.6071, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.3875, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2209, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1976, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.1039, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 193.9763, 'train_samples_per_second': 1.067, 'train_steps_per_second': 0.139, 'train_loss': 0.44666686764469854, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 07:42:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:42:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_45_0.8_0.3_55_3_0.3_4 epoch 1

------------------------------------------------

0.6213617869697178

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_20_45_0.8_0.3_55_3_0.3_4/generated_contents/1
INFO 11-26 07:43:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:43:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_45_0.8_0.3_55_3_0.3_4 epoch 2

------------------------------------------------

0.5890860965727894

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_20_45_0.8_0.3_55_3_0.3_4/generated_contents/2
INFO 11-26 07:43:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:43:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_45_0.8_0.3_55_3_0.3_4 epoch 3

------------------------------------------------

0.586960968634517

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_20_45_0.8_0.3_55_3_0.3_4/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_4
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-9 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_45_0.8_0.3_55_3_0.3_4/checkpoint-27
searching parameters: task121_30_15_45_0.5_0.3_60_3_0.5_4
/home/cyzhao/NI_task121_exp_4/task121_30_15_45_0.5_0.3_60_3_0.5_4
/home/cyzhao/NI_task121_exp_4/task121_30_15_45_0.5_0.3_60_3_0.5_4/config.json
generate_and_write_inputs!
INFO 11-26 07:44:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:44:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 07:57:16,739] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_15_50_0.7_0.3_100_120_50_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_15_50_0.7_0.3_100_120_50_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_15_50_0.7_0.3_100_120_50_80_3_4/config.json
generate_and_write_inputs!
[2023-11-26 07:59:04,339] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_10_50_0.6_0.35_80_140_40_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.6_0.35_80_140_40_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/config.json
generate_and_write_inputs!
INFO 11-26 07:59:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 07:59:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:01:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:01:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 61
expected_example_num: 200
selection_ratio: 0.305
finetune_deepseek!
{'loss': 1.2201, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.6663, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2181, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.1487, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0755, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0749, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 198.39, 'train_samples_per_second': 0.922, 'train_steps_per_second': 0.121, 'train_loss': 0.4006098123888175, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:05:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:06:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_50_0.6_0.35_80_140_40_80_3_4 epoch 1

------------------------------------------------

0.5583305446111145

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/generated_contents/1
INFO 11-26 08:06:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:06:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_50_0.6_0.35_80_140_40_80_3_4 epoch 2

------------------------------------------------

0.5832981459862331

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/generated_contents/2
INFO 11-26 08:07:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:07:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_50_0.6_0.35_80_140_40_80_3_4 epoch 3

------------------------------------------------

0.5837577286215977

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-24 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.6_0.35_80_140_40_80_3_4/checkpoint-16
searching parameters: task121_20_15_40_0.4_0.3_90_120_45_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_15_40_0.4_0.3_90_120_45_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_15_40_0.4_0.3_90_120_45_80_3_4/config.json
generate_and_write_inputs!
INFO 11-26 08:07:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:07:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
[2023-11-26 08:11:09,204] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_20_15_45_0.6_0.3_90_130_50_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_15_45_0.6_0.3_90_130_50_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/config.json
generate_and_write_inputs!
INFO 11-26 08:11:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:11:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:13:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:13:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 61
expected_example_num: 300
selection_ratio: 0.20333333333333334
finetune_deepseek!
{'loss': 1.6762, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.706, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.3187, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2564, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.18, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0981, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 196.6746, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.122, 'train_loss': 0.5392325110733509, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:18:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:18:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.3_90_130_50_85_3_4 epoch 1

------------------------------------------------

0.6092716995166894

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/generated_contents/1
INFO 11-26 08:18:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:18:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.3_90_130_50_85_3_4 epoch 2

------------------------------------------------

0.6026138443100427

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/generated_contents/2
INFO 11-26 08:18:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:19:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_45_0.6_0.3_90_130_50_85_3_4 epoch 3

------------------------------------------------

0.607864667346494

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-8 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_45_0.6_0.3_90_130_50_85_3_4/checkpoint-24
searching parameters: task121_30_15_45_0.6_0.35_100_120_50_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_15_45_0.6_0.35_100_120_50_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/config.json
generate_and_write_inputs!
INFO 11-26 08:19:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:19:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:24:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:24:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 5
expected_example_num: 450
selection_ratio: 0.011111111111111112
finetune_deepseek!
{'train_runtime': 160.9927, 'train_samples_per_second': 0.093, 'train_steps_per_second': 0.019, 'train_loss': 0.6810835202534994, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:28:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:28:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.6_0.35_100_120_50_75_3_4 epoch 1

------------------------------------------------

0.3782560681679407

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/generated_contents/1
INFO 11-26 08:28:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:28:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.6_0.35_100_120_50_75_3_4 epoch 2

------------------------------------------------

0.4657184516453523

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/generated_contents/2
INFO 11-26 08:29:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:29:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.6_0.35_100_120_50_75_3_4 epoch 3

------------------------------------------------

0.4574359025559908

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.6_0.35_100_120_50_75_3_4/checkpoint-3
searching parameters: task121_20_10_50_0.7_0.35_100_140_40_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.7_0.35_100_140_40_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/config.json
generate_and_write_inputs!
INFO 11-26 08:29:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:29:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:31:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:31:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 48
expected_example_num: 200
selection_ratio: 0.24
finetune_deepseek!
{'loss': 1.33, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4028, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2686, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1402, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 186.0126, 'train_samples_per_second': 0.774, 'train_steps_per_second': 0.097, 'train_loss': 0.488376721739769, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:35:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:35:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_50_0.7_0.35_100_140_40_80_3_4 epoch 1

------------------------------------------------

0.6178260446415895

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/generated_contents/1
INFO 11-26 08:36:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:36:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_50_0.7_0.35_100_140_40_80_3_4 epoch 2

------------------------------------------------

0.5131005270675401

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/generated_contents/2
INFO 11-26 08:36:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:36:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_50_0.7_0.35_100_140_40_80_3_4 epoch 3

------------------------------------------------

0.4999374316400612

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_4
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-6 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_50_0.7_0.35_100_140_40_80_3_4/checkpoint-18
searching parameters: task121_10_20_45_0.5_0.4_100_120_50_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_20_45_0.5_0.4_100_120_50_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/config.json
generate_and_write_inputs!
INFO 11-26 08:36:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:37:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:38:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:39:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 24
expected_example_num: 200
selection_ratio: 0.12
finetune_deepseek!
{'loss': 0.6066, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1325, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 171.0317, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.053, 'train_loss': 0.3440074308051003, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:42:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:42:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_45_0.5_0.4_100_120_50_85_3_4 epoch 1

------------------------------------------------

0.6016374718234266

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/generated_contents/1
INFO 11-26 08:43:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:43:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_45_0.5_0.4_100_120_50_85_3_4 epoch 2

------------------------------------------------

0.6203154570700558

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/generated_contents/2
INFO 11-26 08:43:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:43:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_45_0.5_0.4_100_120_50_85_3_4 epoch 3

------------------------------------------------

0.6282539708067533

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_4
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-9 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_45_0.5_0.4_100_120_50_85_3_4/checkpoint-6
searching parameters: task121_30_20_40_0.5_0.35_90_130_40_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_20_40_0.5_0.35_90_130_40_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/config.json
generate_and_write_inputs!
INFO 11-26 08:44:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:44:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:47:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:47:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 109
expected_example_num: 600
selection_ratio: 0.18166666666666667
finetune_deepseek!
{'loss': 1.1697, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.29}
{'loss': 0.3897, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4223, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.86}
{'loss': 0.2144, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1976, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}
{'loss': 0.1918, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1215, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1191, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.1181, 'learning_rate': 7.142857142857143e-06, 'epoch': 2.57}
{'loss': 0.0802, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 216.3766, 'train_samples_per_second': 1.511, 'train_steps_per_second': 0.194, 'train_loss': 0.2927174781050001, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:52:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:52:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.5_0.35_90_130_40_75_3_4 epoch 1

------------------------------------------------

0.5996503250451118

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/generated_contents/1
INFO 11-26 08:52:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:52:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.5_0.35_90_130_40_75_3_4 epoch 2

------------------------------------------------

0.6107723384338937

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/generated_contents/2
INFO 11-26 08:53:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:53:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.5_0.35_90_130_40_75_3_4 epoch 3

------------------------------------------------

0.6140619950626838

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-28
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.5_0.35_90_130_40_75_3_4/checkpoint-42
searching parameters: task121_20_15_50_0.4_0.4_80_120_45_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_15_50_0.4_0.4_80_120_45_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/config.json
generate_and_write_inputs!
INFO 11-26 08:53:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:53:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 08:56:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 08:56:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 19
expected_example_num: 300
selection_ratio: 0.06333333333333334
finetune_deepseek!
{'loss': 1.3026, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2565, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 171.0089, 'train_samples_per_second': 0.333, 'train_steps_per_second': 0.053, 'train_loss': 0.7116091168589063, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 08:59:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:00:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_50_0.4_0.4_80_120_45_85_3_4 epoch 1

------------------------------------------------

0.4483713328105638

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/generated_contents/1
INFO 11-26 09:00:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:00:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_50_0.4_0.4_80_120_45_85_3_4 epoch 2

------------------------------------------------

0.5281158631698606

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/generated_contents/2
INFO 11-26 09:00:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:01:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_15_50_0.4_0.4_80_120_45_85_3_4 epoch 3

------------------------------------------------

0.5295984132863911

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_15_50_0.4_0.4_80_120_45_85_3_4/checkpoint-9
searching parameters: task121_10_15_40_0.8_0.3_90_130_45_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.8_0.3_90_130_45_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:01:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:01:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:02:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:03:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 47
expected_example_num: 150
selection_ratio: 0.31333333333333335
finetune_deepseek!
{'loss': 1.0595, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4365, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1909, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1098, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 184.2764, 'train_samples_per_second': 0.765, 'train_steps_per_second': 0.098, 'train_loss': 0.41070305638843113, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:07:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:07:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_40_0.8_0.3_90_130_45_85_3_4 epoch 1

------------------------------------------------

0.28269290563290406

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/generated_contents/1
INFO 11-26 09:07:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:07:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_40_0.8_0.3_90_130_45_85_3_4 epoch 2

------------------------------------------------

0.6584026182859659

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/generated_contents/2
INFO 11-26 09:08:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:08:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_40_0.8_0.3_90_130_45_85_3_4 epoch 3

------------------------------------------------

0.6678225067329332

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_4
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-18 /data2/cyzhao/best_ckpt/NI_task121_exp_4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.8_0.3_90_130_45_85_3_4/checkpoint-12
searching parameters: task121_20_10_45_0.4_0.3_100_140_45_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_10_45_0.4_0.3_100_140_45_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:08:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:08:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:10:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:11:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 13
expected_example_num: 200
selection_ratio: 0.065
finetune_deepseek!
{'loss': 0.8794, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 166.7178, 'train_samples_per_second': 0.234, 'train_steps_per_second': 0.036, 'train_loss': 0.6731291015942892, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:14:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:15:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.4_0.3_100_140_45_80_3_4 epoch 1

------------------------------------------------

0.40597919263189153

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/generated_contents/1
INFO 11-26 09:15:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:15:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.4_0.3_100_140_45_80_3_4 epoch 2

------------------------------------------------

0.5705857426485258

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/generated_contents/2
INFO 11-26 09:15:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:15:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.4_0.3_100_140_45_80_3_4 epoch 3

------------------------------------------------

0.5511245277663991

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.4_0.3_100_140_45_80_3_4/checkpoint-6
searching parameters: task121_10_10_50_0.8_0.4_80_130_40_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_10_50_0.8_0.4_80_130_40_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:16:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:16:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:17:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:17:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 21
expected_example_num: 100
selection_ratio: 0.21
finetune_deepseek!
{'loss': 1.1004, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.193, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 179.6094, 'train_samples_per_second': 0.351, 'train_steps_per_second': 0.05, 'train_loss': 0.5893943938944075, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:21:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:21:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.8_0.4_80_130_40_85_3_4 epoch 1

------------------------------------------------

0.5238864902594258

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/generated_contents/1
INFO 11-26 09:22:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:22:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.8_0.4_80_130_40_85_3_4 epoch 2

------------------------------------------------

0.44132839288312203

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/generated_contents/2
INFO 11-26 09:22:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:22:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_50_0.8_0.4_80_130_40_85_3_4 epoch 3

------------------------------------------------

0.4717652892545636

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_50_0.8_0.4_80_130_40_85_3_4/checkpoint-9
searching parameters: task121_30_15_50_0.6_0.4_90_130_45_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_15_50_0.6_0.4_90_130_45_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:23:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:23:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:26:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:27:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 92
expected_example_num: 450
selection_ratio: 0.20444444444444446
finetune_deepseek!
{'loss': 1.6422, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.7139, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.5383, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2481, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2199, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2318, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.129, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0918, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.1221, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 205.8318, 'train_samples_per_second': 1.341, 'train_steps_per_second': 0.175, 'train_loss': 0.437454024122821, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:31:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:31:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_50_0.6_0.4_90_130_45_75_3_4 epoch 1

------------------------------------------------

0.477014271189628

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/generated_contents/1
INFO 11-26 09:32:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:32:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_50_0.6_0.4_90_130_45_75_3_4 epoch 2

------------------------------------------------

0.5951609287675086

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/generated_contents/2
INFO 11-26 09:32:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:32:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_50_0.6_0.4_90_130_45_75_3_4 epoch 3

------------------------------------------------

0.6226679043259842

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.6_0.4_90_130_45_75_3_4/checkpoint-36
searching parameters: task121_10_15_40_0.8_0.3_90_130_45_85_3_4
searching parameters: task121_10_15_40_0.8_0.3_90_130_45_85_3_4
searching parameters: task121_10_15_40_0.8_0.3_90_130_45_85_3_4
searching parameters: task121_10_15_40_0.8_0.3_90_130_45_85_3_4
searching parameters: task121_10_20_40_0.8_0.3_90_140_45_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_45_85_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:33:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:33:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:34:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:34:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 60
expected_example_num: 200
selection_ratio: 0.3
finetune_deepseek!
{'loss': 1.0858, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.6071, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2384, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2138, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0962, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0974, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 193.336, 'train_samples_per_second': 0.931, 'train_steps_per_second': 0.124, 'train_loss': 0.3897883743047714, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:38:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:39:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.8_0.3_90_140_45_85_3_4 epoch 1

------------------------------------------------

0.6291394490173275

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/generated_contents/1
INFO 11-26 09:39:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:39:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.8_0.3_90_140_45_85_3_4 epoch 2

------------------------------------------------

0.5831877892042622

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/generated_contents/2
INFO 11-26 09:39:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:40:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.8_0.3_90_140_45_85_3_4 epoch 3

------------------------------------------------

0.5744670123978058

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_45_85_3_4/checkpoint-24
searching parameters: task121_10_15_40_0.7_0.3_80_130_45_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.7_0.3_80_130_45_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:40:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:40:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:42:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:42:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 7
expected_example_num: 150
selection_ratio: 0.04666666666666667
finetune_deepseek!
{'train_runtime': 171.8855, 'train_samples_per_second': 0.122, 'train_steps_per_second': 0.017, 'train_loss': 0.6799513498942057, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:46:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:46:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_40_0.7_0.3_80_130_45_80_3_4 epoch 1

------------------------------------------------

0.5606436075611484

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/generated_contents/1
INFO 11-26 09:46:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:46:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_40_0.7_0.3_80_130_45_80_3_4 epoch 2

------------------------------------------------

0.6572885537134878

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/generated_contents/2
INFO 11-26 09:47:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:47:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_40_0.7_0.3_80_130_45_80_3_4 epoch 3

------------------------------------------------

0.6542594701301604

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_40_0.7_0.3_80_130_45_80_3_4/checkpoint-3
searching parameters: task121_10_15_40_0.8_0.3_90_130_45_85_3_4
searching parameters: task121_10_15_40_0.8_0.3_90_130_45_85_3_4
searching parameters: task121_10_20_40_0.8_0.3_90_140_40_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_40_80_3_4
/home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:47:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:47:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:48:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:49:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 60
expected_example_num: 200
selection_ratio: 0.3
finetune_deepseek!
{'loss': 1.085, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.6119, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2342, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2184, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1037, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0979, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 195.7526, 'train_samples_per_second': 0.92, 'train_steps_per_second': 0.123, 'train_loss': 0.3918635882437229, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-26 09:53:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:53:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.8_0.3_90_140_40_80_3_4 epoch 1

------------------------------------------------

0.6396002380964285

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/generated_contents/1
INFO 11-26 09:53:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:54:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.8_0.3_90_140_40_80_3_4 epoch 2

------------------------------------------------

0.5954695539952969

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/generated_contents/2
INFO 11-26 09:54:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:54:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_40_0.8_0.3_90_140_40_80_3_4 epoch 3

------------------------------------------------

0.581254269472725

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_40_0.8_0.3_90_140_40_80_3_4/checkpoint-24
searching parameters: task121_30_10_40_0.7_0.35_80_120_50_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.35_80_120_50_75_3_4
/home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/config.json
generate_and_write_inputs!
INFO 11-26 09:54:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:55:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-26 09:56:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 09:57:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 68
expected_example_num: 300
selection_ratio: 0.22666666666666666
finetune_deepseek!
{'loss': 1.7497, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.9004, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.4283, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2722, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.2489, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.1499, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 196.7041, 'train_samples_per_second': 1.037, 'train_steps_per_second': 0.137, 'train_loss': 0.5862944898781953, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-26 10:01:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:01:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_80_120_50_75_3_4 epoch 1

------------------------------------------------

0.559722105064974

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/generated_contents/1
INFO 11-26 10:02:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:02:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_80_120_50_75_3_4 epoch 2

------------------------------------------------

0.58838193913341

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/generated_contents/2
INFO 11-26 10:02:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:02:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_10_40_0.7_0.35_80_120_50_75_3_4 epoch 3

------------------------------------------------

0.5865117122493817

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_10_40_0.7_0.35_80_120_50_75_3_4/checkpoint-27
{'generation_epochs': 10, 'generation_batch_size': 15, 'generation_top_k': 40, 'generation_temperature': 0.8, 'min_frequency': 0.3, 'min_input_length': 90, 'max_input_length': 130, 'min_output_length': 45, 'max_output_length': 85, 'training_epochs': 3}
test best ckpt.
INFO 11-26 10:02:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task121_exp_4', tokenizer='/data2/cyzhao/best_ckpt/NI_task121_exp_4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-26 10:03:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task121_exp_4

------------------------------------------------

0.601703519108207

------------------------------------------------


The best ckpt on test set gain 0.601703519108207
Genrated contents are stored in /home/cyzhao/NI_task121_exp_4/best_ckpt_generated_content
