[2023-11-24 11:42:22,959] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NI_task1529
searching parameters: NI_task1529_50_40_45_0.9_0.35_120_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_50_40_45_0.9_0.35_120_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_50_40_45_0.9_0.35_120_4/config.json
generate_and_write_inputs!
INFO 11-24 11:42:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:42:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
[2023-11-24 11:43:44,007] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NI_task1529
searching parameters: NI_task1529_15_20_45_0.4_0.3_120_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_45_0.4_0.3_120_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_45_0.4_0.3_120_4/config.json
generate_and_write_inputs!
INFO 11-24 11:43:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:44:05 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
WARNING 11-24 11:48:03 scheduler.py:146] Input prompt (4633 tokens) is too long and exceeds limit of 4096
WARNING 11-24 11:50:15 scheduler.py:146] Input prompt (4558 tokens) is too long and exceeds limit of 4096
WARNING 11-24 11:50:16 scheduler.py:146] Input prompt (4568 tokens) is too long and exceeds limit of 4096
WARNING 11-24 11:50:16 scheduler.py:146] Input prompt (4541 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-24 11:50:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 11:51:04 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 108
expected_example_num: 300
selection_ratio: 0.36
finetune_vicuna!
{'loss': 0.1656, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.29}
{'loss': 0.2299, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.57}
{'loss': 0.0981, 'learning_rate': 3.928571428571429e-05, 'epoch': 0.86}
{'loss': 0.0484, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.14}
{'loss': 0.0533, 'learning_rate': 3.2142857142857144e-05, 'epoch': 1.43}
{'loss': 0.0436, 'learning_rate': 2.857142857142857e-05, 'epoch': 1.71}
{'loss': 0.1044, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.0272, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.29}
{'loss': 0.0358, 'learning_rate': 1.785714285714286e-05, 'epoch': 2.57}
{'loss': 0.0354, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.86}
{'loss': 0.021, 'learning_rate': 1.0714285714285714e-05, 'epoch': 3.14}
{'loss': 0.0176, 'learning_rate': 7.142857142857143e-06, 'epoch': 3.43}
{'loss': 0.0331, 'learning_rate': 3.5714285714285714e-06, 'epoch': 3.71}
{'loss': 0.0122, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 352.2993, 'train_samples_per_second': 1.226, 'train_steps_per_second': 0.159, 'train_loss': 0.0660997335133808, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-56/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-24 12:01:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:01:54 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_45_0.4_0.3_120_4 epoch 1

------------------------------------------------

0.46

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_45_0.4_0.3_120_4/generated_contents/1
INFO 11-24 12:02:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:02:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_45_0.4_0.3_120_4 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_45_0.4_0.3_120_4/generated_contents/2
INFO 11-24 12:03:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:03:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_45_0.4_0.3_120_4 epoch 3

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_45_0.4_0.3_120_4/generated_contents/3
INFO 11-24 12:04:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-56', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-56', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:04:54 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_45_0.4_0.3_120_4 epoch 4

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_45_0.4_0.3_120_4/generated_contents/4
mv /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-28 /data2/cyzhao/best_ckpt/NI_task1529_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_45_0.4_0.3_120_4/checkpoint-56
searching parameters: NI_task1529_10_20_40_0.9_0.4_130_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_40_0.9_0.4_130_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_40_0.9_0.4_130_4/config.json
generate_and_write_inputs!
INFO 11-24 12:05:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:06:04 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 12:07:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:07:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 148
expected_example_num: 200
selection_ratio: 0.74
finetune_vicuna!
{'loss': 2.3536, 'learning_rate': 4.736842105263158e-05, 'epoch': 0.21}
{'loss': 1.8703, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.42}
{'loss': 1.635, 'learning_rate': 4.210526315789474e-05, 'epoch': 0.63}
{'loss': 0.5851, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.84}
{'loss': 0.6063, 'learning_rate': 3.6842105263157895e-05, 'epoch': 1.05}
{'loss': 0.6063, 'learning_rate': 3.421052631578947e-05, 'epoch': 1.26}
{'loss': 0.3226, 'learning_rate': 3.157894736842105e-05, 'epoch': 1.47}
{'loss': 0.4934, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.68}
{'loss': 0.3093, 'learning_rate': 2.6315789473684212e-05, 'epoch': 1.89}
{'loss': 0.2525, 'learning_rate': 2.368421052631579e-05, 'epoch': 2.11}
{'loss': 0.2034, 'learning_rate': 2.105263157894737e-05, 'epoch': 2.32}
{'loss': 0.36, 'learning_rate': 1.8421052631578947e-05, 'epoch': 2.53}
{'loss': 0.2183, 'learning_rate': 1.5789473684210526e-05, 'epoch': 2.74}
{'loss': 0.195, 'learning_rate': 1.3157894736842106e-05, 'epoch': 2.95}
{'loss': 0.2188, 'learning_rate': 1.0526315789473684e-05, 'epoch': 3.16}
{'loss': 0.2053, 'learning_rate': 7.894736842105263e-06, 'epoch': 3.37}
{'loss': 0.1413, 'learning_rate': 5.263157894736842e-06, 'epoch': 3.58}
{'loss': 0.2271, 'learning_rate': 2.631578947368421e-06, 'epoch': 3.79}
{'loss': 0.1203, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 284.2363, 'train_samples_per_second': 2.083, 'train_steps_per_second': 0.267, 'train_loss': 0.5749482241900343, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-76/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-24 12:13:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:13:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_20_40_0.9_0.4_130_4 epoch 1

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_40_0.9_0.4_130_4/generated_contents/1
INFO 11-24 12:14:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:14:51 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_20_40_0.9_0.4_130_4 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_40_0.9_0.4_130_4/generated_contents/2
INFO 11-24 12:15:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:15:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_20_40_0.9_0.4_130_4 epoch 3

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_40_0.9_0.4_130_4/generated_contents/3
INFO 11-24 12:16:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-76', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-76', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:16:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_20_40_0.9_0.4_130_4 epoch 4

------------------------------------------------

0.546

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_40_0.9_0.4_130_4/generated_contents/4
rm -rf /data2/cyzhao/best_ckpt/NI_task1529_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-76 /data2/cyzhao/best_ckpt/NI_task1529_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_40_0.9_0.4_130_4/checkpoint-57
searching parameters: NI_task1529_10_10_40_0.8_0.5_130_3
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_10_40_0.8_0.5_130_3
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_10_40_0.8_0.5_130_3/config.json
generate_and_write_inputs!
INFO 11-24 12:17:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:17:55 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 12:21:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:21:32 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 53
expected_example_num: 100
selection_ratio: 0.53
finetune_vicuna!
{'loss': 0.0869, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.3649, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.2322, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.039, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0797, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 206.151, 'train_samples_per_second': 0.771, 'train_steps_per_second': 0.102, 'train_loss': 0.1570988709018344, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-24 12:26:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:26:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_10_40_0.8_0.5_130_3 epoch 1

------------------------------------------------

0.46

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_10_40_0.8_0.5_130_3/generated_contents/1
INFO 11-24 12:27:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:27:44 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_10_40_0.8_0.5_130_3 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_10_40_0.8_0.5_130_3/generated_contents/2
INFO 11-24 12:28:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:28:45 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_10_40_0.8_0.5_130_3 epoch 3

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_10_40_0.8_0.5_130_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_10_40_0.8_0.5_130_3/checkpoint-21
searching parameters: NI_task1529_20_10_50_0.4_0.5_120_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_50_0.4_0.5_120_4
/home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_50_0.4_0.5_120_4/config.json
generate_and_write_inputs!
INFO 11-24 12:29:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:29:45 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 12:31:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:31:45 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 32
expected_example_num: 200
selection_ratio: 0.16
finetune_vicuna!
{'loss': 3.3702, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.0}
{'loss': 0.4663, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.1031, 'learning_rate': 1.25e-05, 'epoch': 3.0}
{'loss': 0.0484, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 229.3959, 'train_samples_per_second': 0.558, 'train_steps_per_second': 0.07, 'train_loss': 0.9970083851367235, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-24 12:36:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:36:46 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_50_0.4_0.5_120_4 epoch 1

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_50_0.4_0.5_120_4/generated_contents/1
INFO 11-24 12:41:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:41:18 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_50_0.4_0.5_120_4 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_50_0.4_0.5_120_4/generated_contents/2
INFO 11-24 12:41:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:42:15 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_50_0.4_0.5_120_4 epoch 3

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_50_0.4_0.5_120_4/generated_contents/3
INFO 11-24 12:42:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:43:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_50_0.4_0.5_120_4 epoch 4

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_50_0.4_0.5_120_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_50_0.4_0.5_120_4/checkpoint-16
searching parameters: NI_task1529_10_20_50_0.3_0.35_120_3
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_50_0.3_0.35_120_3
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_50_0.3_0.35_120_3/config.json
generate_and_write_inputs!
INFO 11-24 12:43:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:44:13 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
WARNING 11-24 12:46:20 scheduler.py:146] Input prompt (4336 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-24 12:48:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:48:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 41
expected_example_num: 200
selection_ratio: 0.205
finetune_vicuna!
{'loss': 0.3869, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.2488, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.074, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0506, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 198.1403, 'train_samples_per_second': 0.621, 'train_steps_per_second': 0.091, 'train_loss': 0.2532824261320962, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-24 12:54:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:54:28 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_20_50_0.3_0.35_120_3 epoch 1

------------------------------------------------

0.204

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_50_0.3_0.35_120_3/generated_contents/1
INFO 11-24 12:55:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:55:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_20_50_0.3_0.35_120_3 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_50_0.3_0.35_120_3/generated_contents/2
INFO 11-24 12:56:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:56:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_10_20_50_0.3_0.35_120_3 epoch 3

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_10_20_50_0.3_0.35_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_10_20_50_0.3_0.35_120_3/checkpoint-18
searching parameters: NI_task1529_15_15_40_0.4_0.4_130_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_15_15_40_0.4_0.4_130_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_15_15_40_0.4_0.4_130_5/config.json
generate_and_write_inputs!
INFO 11-24 12:57:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 12:57:57 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 13:02:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:03:00 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 60
expected_example_num: 225
selection_ratio: 0.26666666666666666
finetune_vicuna!
{'loss': 0.1447, 'learning_rate': 4.5e-05, 'epoch': 0.5}
{'loss': 0.4254, 'learning_rate': 4e-05, 'epoch': 1.0}
{'loss': 0.054, 'learning_rate': 3.5e-05, 'epoch': 1.5}
{'loss': 0.1187, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.0416, 'learning_rate': 2.5e-05, 'epoch': 2.5}
{'loss': 0.0884, 'learning_rate': 2e-05, 'epoch': 3.0}
{'loss': 0.0505, 'learning_rate': 1.5e-05, 'epoch': 3.5}
{'loss': 0.0962, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0157, 'learning_rate': 5e-06, 'epoch': 4.5}
{'loss': 0.0284, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 341.9721, 'train_samples_per_second': 0.877, 'train_steps_per_second': 0.117, 'train_loss': 0.10633881352841854, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-40/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-24 13:10:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:10:50 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_15_40_0.4_0.4_130_5 epoch 1

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_15_40_0.4_0.4_130_5/generated_contents/1
INFO 11-24 13:11:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:11:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_15_40_0.4_0.4_130_5 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_15_40_0.4_0.4_130_5/generated_contents/2
INFO 11-24 13:12:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:12:44 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_15_40_0.4_0.4_130_5 epoch 3

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_15_40_0.4_0.4_130_5/generated_contents/3
INFO 11-24 13:13:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:13:42 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_15_40_0.4_0.4_130_5 epoch 4

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_15_40_0.4_0.4_130_5/generated_contents/4
INFO 11-24 13:14:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-40', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-40', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:14:42 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_15_40_0.4_0.4_130_5 epoch 5

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_15_40_0.4_0.4_130_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_15_40_0.4_0.4_130_5/checkpoint-40
searching parameters: NI_task1529_15_20_40_0.9_0.5_120_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_40_0.9_0.5_120_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_40_0.9_0.5_120_5/config.json
generate_and_write_inputs!
INFO 11-24 13:15:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:15:50 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 13:18:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:18:30 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 221
expected_example_num: 300
selection_ratio: 0.7366666666666667
finetune_vicuna!
{'loss': 0.779, 'learning_rate': 4.8571428571428576e-05, 'epoch': 0.14}
{'loss': 0.7587, 'learning_rate': 4.714285714285714e-05, 'epoch': 0.29}
{'loss': 0.8282, 'learning_rate': 4.5714285714285716e-05, 'epoch': 0.43}
{'loss': 0.4067, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.57}
{'loss': 0.3467, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.71}
{'loss': 0.1788, 'learning_rate': 4.1428571428571437e-05, 'epoch': 0.86}
{'loss': 0.4188, 'learning_rate': 4e-05, 'epoch': 1.0}
{'loss': 0.6597, 'learning_rate': 3.857142857142858e-05, 'epoch': 1.14}
{'loss': 0.2015, 'learning_rate': 3.7142857142857143e-05, 'epoch': 1.29}
{'loss': 0.2383, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.43}
{'loss': 0.2339, 'learning_rate': 3.428571428571429e-05, 'epoch': 1.57}
{'loss': 0.1615, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.71}
{'loss': 0.2267, 'learning_rate': 3.142857142857143e-05, 'epoch': 1.86}
{'loss': 0.2021, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.1079, 'learning_rate': 2.857142857142857e-05, 'epoch': 2.14}
{'loss': 0.1442, 'learning_rate': 2.714285714285714e-05, 'epoch': 2.29}
{'loss': 0.1314, 'learning_rate': 2.5714285714285714e-05, 'epoch': 2.43}
{'loss': 0.2012, 'learning_rate': 2.4285714285714288e-05, 'epoch': 2.57}
{'loss': 0.1923, 'learning_rate': 2.2857142857142858e-05, 'epoch': 2.71}
{'loss': 0.1867, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.86}
{'loss': 0.1269, 'learning_rate': 2e-05, 'epoch': 3.0}
{'loss': 0.1237, 'learning_rate': 1.8571428571428572e-05, 'epoch': 3.14}
{'loss': 0.0892, 'learning_rate': 1.7142857142857145e-05, 'epoch': 3.29}
{'loss': 0.0762, 'learning_rate': 1.5714285714285715e-05, 'epoch': 3.43}
{'loss': 0.0939, 'learning_rate': 1.4285714285714285e-05, 'epoch': 3.57}
{'loss': 0.1027, 'learning_rate': 1.2857142857142857e-05, 'epoch': 3.71}
{'loss': 0.0898, 'learning_rate': 1.1428571428571429e-05, 'epoch': 3.86}
{'loss': 0.0639, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0675, 'learning_rate': 8.571428571428573e-06, 'epoch': 4.14}
{'loss': 0.0825, 'learning_rate': 7.142857142857143e-06, 'epoch': 4.29}
{'loss': 0.0313, 'learning_rate': 5.7142857142857145e-06, 'epoch': 4.43}
{'loss': 0.0294, 'learning_rate': 4.285714285714286e-06, 'epoch': 4.57}
{'loss': 0.0496, 'learning_rate': 2.8571428571428573e-06, 'epoch': 4.71}
{'loss': 0.0151, 'learning_rate': 1.4285714285714286e-06, 'epoch': 4.86}
{'loss': 0.058, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 441.7433, 'train_samples_per_second': 2.501, 'train_steps_per_second': 0.317, 'train_loss': 0.22011441245142901, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-56/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-140/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-112/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-84/optimizer.pt
validate!
last validate 0.
INFO 11-24 13:27:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:27:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_40_0.9_0.5_120_5 epoch 1

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_40_0.9_0.5_120_5/generated_contents/1
INFO 11-24 13:28:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-56', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-56', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:28:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_40_0.9_0.5_120_5 epoch 2

------------------------------------------------

0.485

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_40_0.9_0.5_120_5/generated_contents/2
INFO 11-24 13:29:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-84', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-84', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:29:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_40_0.9_0.5_120_5 epoch 3

------------------------------------------------

0.565

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_40_0.9_0.5_120_5/generated_contents/3
INFO 11-24 13:30:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-112', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-112', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:30:50 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_40_0.9_0.5_120_5 epoch 4

------------------------------------------------

0.569

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_40_0.9_0.5_120_5/generated_contents/4
INFO 11-24 13:31:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-140', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-140', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:31:46 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_15_20_40_0.9_0.5_120_5 epoch 5

------------------------------------------------

0.589

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_15_20_40_0.9_0.5_120_5/generated_contents/5
rm -rf /data2/cyzhao/best_ckpt/NI_task1529_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-140 /data2/cyzhao/best_ckpt/NI_task1529_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-28
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-56
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-84
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_15_20_40_0.9_0.5_120_5/checkpoint-112
searching parameters: NI_task1529_20_10_45_0.9_0.35_115_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_45_0.9_0.35_115_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_45_0.9_0.35_115_5/config.json
generate_and_write_inputs!
INFO 11-24 13:32:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:32:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 13:35:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:36:09 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 141
expected_example_num: 200
selection_ratio: 0.705
finetune_vicuna!
{'loss': 2.6468, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.22}
{'loss': 1.489, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.44}
{'loss': 0.3638, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.67}
{'loss': 0.3586, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.89}
{'loss': 0.4077, 'learning_rate': 3.888888888888889e-05, 'epoch': 1.11}
{'loss': 0.2843, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}
{'loss': 0.3089, 'learning_rate': 3.444444444444445e-05, 'epoch': 1.56}
{'loss': 0.1515, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.78}
{'loss': 0.1627, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.1158, 'learning_rate': 2.777777777777778e-05, 'epoch': 2.22}
{'loss': 0.095, 'learning_rate': 2.5555555555555554e-05, 'epoch': 2.44}
{'loss': 0.1345, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}
{'loss': 0.128, 'learning_rate': 2.111111111111111e-05, 'epoch': 2.89}
{'loss': 0.0736, 'learning_rate': 1.888888888888889e-05, 'epoch': 3.11}
{'loss': 0.0639, 'learning_rate': 1.6666666666666667e-05, 'epoch': 3.33}
{'loss': 0.0763, 'learning_rate': 1.4444444444444444e-05, 'epoch': 3.56}
{'loss': 0.0727, 'learning_rate': 1.2222222222222222e-05, 'epoch': 3.78}
{'loss': 0.1226, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0718, 'learning_rate': 7.777777777777777e-06, 'epoch': 4.22}
{'loss': 0.0207, 'learning_rate': 5.555555555555556e-06, 'epoch': 4.44}
{'loss': 0.0768, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.67}
{'loss': 0.034, 'learning_rate': 1.1111111111111112e-06, 'epoch': 4.89}
{'train_runtime': 378.4534, 'train_samples_per_second': 1.863, 'train_steps_per_second': 0.238, 'train_loss': 0.32279166320545805, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-54/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-72/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-18/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-90/optimizer.pt
validate!
last validate 0.
INFO 11-24 13:43:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:44:13 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_45_0.9_0.35_115_5 epoch 1

------------------------------------------------

0.539

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_45_0.9_0.35_115_5/generated_contents/1
INFO 11-24 13:44:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:45:10 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_45_0.9_0.35_115_5 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_45_0.9_0.35_115_5/generated_contents/2
INFO 11-24 13:45:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-54', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-54', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:46:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_45_0.9_0.35_115_5 epoch 3

------------------------------------------------

0.534

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_45_0.9_0.35_115_5/generated_contents/3
INFO 11-24 13:46:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-72', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-72', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:47:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_45_0.9_0.35_115_5 epoch 4

------------------------------------------------

0.545

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_45_0.9_0.35_115_5/generated_contents/4
INFO 11-24 13:47:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-90', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:48:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_10_45_0.9_0.35_115_5 epoch 5

------------------------------------------------

0.522

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_10_45_0.9_0.35_115_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-36
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-54
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-72
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_10_45_0.9_0.35_115_5/checkpoint-90
searching parameters: NI_task1529_20_15_40_0.9_0.35_130_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_20_15_40_0.9_0.35_130_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_20_15_40_0.9_0.35_130_5/config.json
generate_and_write_inputs!
INFO 11-24 13:48:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:49:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 13:52:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 13:52:54 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 205
expected_example_num: 300
selection_ratio: 0.6833333333333333
finetune_vicuna!
{'loss': 2.2866, 'learning_rate': 4.846153846153846e-05, 'epoch': 0.15}
{'loss': 3.8695, 'learning_rate': 4.692307692307693e-05, 'epoch': 0.31}
{'loss': 0.4557, 'learning_rate': 4.538461538461539e-05, 'epoch': 0.46}
{'loss': 0.6037, 'learning_rate': 4.384615384615385e-05, 'epoch': 0.62}
{'loss': 0.2332, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.77}
{'loss': 0.3251, 'learning_rate': 4.0769230769230773e-05, 'epoch': 0.92}
{'loss': 0.3748, 'learning_rate': 3.923076923076923e-05, 'epoch': 1.08}
{'loss': 0.1936, 'learning_rate': 3.769230769230769e-05, 'epoch': 1.23}
{'loss': 0.1795, 'learning_rate': 3.615384615384615e-05, 'epoch': 1.38}
{'loss': 0.2274, 'learning_rate': 3.461538461538462e-05, 'epoch': 1.54}
{'loss': 0.1913, 'learning_rate': 3.307692307692308e-05, 'epoch': 1.69}
{'loss': 0.2651, 'learning_rate': 3.153846153846154e-05, 'epoch': 1.85}
{'loss': 0.1685, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.1647, 'learning_rate': 2.846153846153846e-05, 'epoch': 2.15}
{'loss': 0.2632, 'learning_rate': 2.6923076923076923e-05, 'epoch': 2.31}
{'loss': 0.1479, 'learning_rate': 2.5384615384615383e-05, 'epoch': 2.46}
{'loss': 0.0997, 'learning_rate': 2.384615384615385e-05, 'epoch': 2.62}
{'loss': 0.1141, 'learning_rate': 2.230769230769231e-05, 'epoch': 2.77}
{'loss': 0.1215, 'learning_rate': 2.0769230769230772e-05, 'epoch': 2.92}
{'loss': 0.0649, 'learning_rate': 1.923076923076923e-05, 'epoch': 3.08}
{'loss': 0.0763, 'learning_rate': 1.7692307692307694e-05, 'epoch': 3.23}
{'loss': 0.1357, 'learning_rate': 1.6153846153846154e-05, 'epoch': 3.38}
{'loss': 0.1171, 'learning_rate': 1.4615384615384617e-05, 'epoch': 3.54}
{'loss': 0.0597, 'learning_rate': 1.3076923076923078e-05, 'epoch': 3.69}
{'loss': 0.0502, 'learning_rate': 1.153846153846154e-05, 'epoch': 3.85}
{'loss': 0.1008, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0665, 'learning_rate': 8.461538461538462e-06, 'epoch': 4.15}
{'loss': 0.041, 'learning_rate': 6.923076923076923e-06, 'epoch': 4.31}
{'loss': 0.0534, 'learning_rate': 5.3846153846153855e-06, 'epoch': 4.46}
{'loss': 0.0607, 'learning_rate': 3.846153846153847e-06, 'epoch': 4.62}
{'loss': 0.0914, 'learning_rate': 2.307692307692308e-06, 'epoch': 4.77}
{'loss': 0.0216, 'learning_rate': 7.692307692307694e-07, 'epoch': 4.92}
{'train_runtime': 442.281, 'train_samples_per_second': 2.318, 'train_steps_per_second': 0.294, 'train_loss': 0.3458308560343889, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-78/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-52/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-130/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-104/optimizer.pt
validate!
last validate 0.
INFO 11-24 14:02:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 14:02:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_15_40_0.9_0.35_130_5 epoch 1

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_15_40_0.9_0.35_130_5/generated_contents/1
INFO 11-24 14:03:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-52', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-52', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 14:03:24 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_15_40_0.9_0.35_130_5 epoch 2

------------------------------------------------

0.54

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_15_40_0.9_0.35_130_5/generated_contents/2
INFO 11-24 14:04:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-78', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 14:04:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_15_40_0.9_0.35_130_5 epoch 3

------------------------------------------------

0.539

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_15_40_0.9_0.35_130_5/generated_contents/3
INFO 11-24 14:05:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-104', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-104', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 14:05:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_15_40_0.9_0.35_130_5 epoch 4

------------------------------------------------

0.537

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_15_40_0.9_0.35_130_5/generated_contents/4
INFO 11-24 14:06:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-130', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-130', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 14:06:18 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task1529_20_15_40_0.9_0.35_130_5 epoch 5

------------------------------------------------

0.581

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1529_exp_1/NI_task1529_20_15_40_0.9_0.35_130_5/generated_contents/5
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-52
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-78
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-104
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task1529_20_15_40_0.9_0.35_130_5/checkpoint-130
searching parameters: NI_task1529_10_15_50_0.9_0.35_130_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_15_50_0.9_0.35_130_5
/home/cyzhao/NI_task1529_exp_1/NI_task1529_10_15_50_0.9_0.35_130_5/config.json
generate_and_write_inputs!
INFO 11-24 14:07:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 14:07:22 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 14:08:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 14:08:58 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
