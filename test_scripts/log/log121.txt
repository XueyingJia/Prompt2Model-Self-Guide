[2023-11-27 21:04:51,127] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_30_10_40_0.4_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.4_0.4_3_7
/home/cyzhao/NI_task121_exp_7/task121_30_10_40_0.4_0.4_3_7/config.json
generate_and_write_inputs!
INFO 11-27 21:04:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
[2023-11-27 21:05:41,142] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task121
searching parameters: task121_10_10_45_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_10_45_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_10_45_0.4_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:05:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:06:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:06:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:06:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 39
expected_example_num: 100
selection_ratio: 0.39
finetune_vicuna!
{'loss': 1.1507, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3733, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1704, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 183.1166, 'train_samples_per_second': 0.639, 'train_steps_per_second': 0.082, 'train_loss': 0.47170167167981464, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:10:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:11:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_45_0.4_0.35_3_8 epoch 1

------------------------------------------------

0.5995525260759426

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_10_45_0.4_0.35_3_8/generated_contents/1
INFO 11-27 21:11:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:11:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_45_0.4_0.35_3_8 epoch 2

------------------------------------------------

0.6204876791479207

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_10_45_0.4_0.35_3_8/generated_contents/2
INFO 11-27 21:11:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:11:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_10_45_0.4_0.35_3_8 epoch 3

------------------------------------------------

0.6230855465962044

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_10_45_0.4_0.35_3_8/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-15 /data2/cyzhao/best_ckpt/NI_task121_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_10_45_0.4_0.35_3_8/checkpoint-10
searching parameters: task121_10_20_50_0.8_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.8_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.8_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:12:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:12:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:13:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:13:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 92
expected_example_num: 200
selection_ratio: 0.46
finetune_vicuna!
{'loss': 1.2929, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 1.6962, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.5474, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2782, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2234, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.221, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1248, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0624, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0908, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 211.7309, 'train_samples_per_second': 1.304, 'train_steps_per_second': 0.17, 'train_loss': 0.5041463693810834, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:18:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:18:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.8_0.35_3_8 epoch 1

------------------------------------------------

0.5549770739207324

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.8_0.35_3_8/generated_contents/1
INFO 11-27 21:18:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:19:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.8_0.35_3_8 epoch 2

------------------------------------------------

0.5467375981773206

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.8_0.35_3_8/generated_contents/2
INFO 11-27 21:19:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:19:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.8_0.35_3_8 epoch 3

------------------------------------------------

0.5670919013724599

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.8_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.8_0.35_3_8/checkpoint-36
searching parameters: task121_30_20_40_0.6_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_30_20_40_0.6_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_30_20_40_0.6_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:20:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:20:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:23:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:23:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 275
expected_example_num: 600
selection_ratio: 0.4583333333333333
finetune_vicuna!
{'loss': 1.454, 'learning_rate': 4.80952380952381e-05, 'epoch': 0.11}
{'loss': 0.5774, 'learning_rate': 4.6190476190476194e-05, 'epoch': 0.23}
{'loss': 0.4665, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.34}
{'loss': 0.4088, 'learning_rate': 4.2380952380952385e-05, 'epoch': 0.46}
{'loss': 0.3492, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.47, 'learning_rate': 3.857142857142858e-05, 'epoch': 0.69}
{'loss': 0.2941, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3822, 'learning_rate': 3.476190476190476e-05, 'epoch': 0.91}
{'loss': 0.2864, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.03}
{'loss': 0.1512, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.147, 'learning_rate': 2.9047619047619052e-05, 'epoch': 1.26}
{'loss': 0.1322, 'learning_rate': 2.714285714285714e-05, 'epoch': 1.37}
{'loss': 0.1975, 'learning_rate': 2.523809523809524e-05, 'epoch': 1.49}
{'loss': 0.1559, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1313, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1336, 'learning_rate': 1.9523809523809524e-05, 'epoch': 1.83}
{'loss': 0.114, 'learning_rate': 1.761904761904762e-05, 'epoch': 1.94}
{'loss': 0.0782, 'learning_rate': 1.5714285714285715e-05, 'epoch': 2.06}
{'loss': 0.0584, 'learning_rate': 1.3809523809523811e-05, 'epoch': 2.17}
{'loss': 0.0502, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0495, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0554, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.51}
{'loss': 0.0799, 'learning_rate': 6.190476190476191e-06, 'epoch': 2.63}
{'loss': 0.0606, 'learning_rate': 4.285714285714286e-06, 'epoch': 2.74}
{'loss': 0.0909, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'loss': 0.068, 'learning_rate': 4.761904761904763e-07, 'epoch': 2.97}
{'train_runtime': 288.1329, 'train_samples_per_second': 2.863, 'train_steps_per_second': 0.364, 'train_loss': 0.24689755382991974, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-35/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-105/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-70/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:30:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-35', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-35', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:30:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.35_3_8 epoch 1

------------------------------------------------

0.6218769528881709

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_20_40_0.6_0.35_3_8/generated_contents/1
INFO 11-27 21:30:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-70', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-70', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:30:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.35_3_8 epoch 2

------------------------------------------------

0.6213518025660034

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_20_40_0.6_0.35_3_8/generated_contents/2
INFO 11-27 21:30:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-105', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-105', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:31:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_20_40_0.6_0.35_3_8 epoch 3

------------------------------------------------

0.6033070904253418

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_20_40_0.6_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-35
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-70
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_20_40_0.6_0.35_3_8/checkpoint-105
searching parameters: task121_20_10_45_0.6_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.6_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.6_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:31:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:31:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:32:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:33:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 94
expected_example_num: 200
selection_ratio: 0.47
finetune_vicuna!
{'loss': 1.3646, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.6179, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.52, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1733, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.233, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2103, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0813, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0903, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0943, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 203.4754, 'train_samples_per_second': 1.386, 'train_steps_per_second': 0.177, 'train_loss': 0.376092163225015, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:37:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:37:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.6_0.4_3_8 epoch 1

------------------------------------------------

0.6311086972770359

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.6_0.4_3_8/generated_contents/1
INFO 11-27 21:37:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:38:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.6_0.4_3_8 epoch 2

------------------------------------------------

0.6014751369701652

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.6_0.4_3_8/generated_contents/2
INFO 11-27 21:38:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:38:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.6_0.4_3_8 epoch 3

------------------------------------------------

0.6169365879312414

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.6_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_8
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.6_0.4_3_8/checkpoint-36
searching parameters: task121_10_20_50_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.4_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:38:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:39:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:39:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:40:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 83
expected_example_num: 200
selection_ratio: 0.415
finetune_vicuna!
{'loss': 0.9509, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.5069, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4106, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2103, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1532, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.1438, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.1037, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0763, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 196.7554, 'train_samples_per_second': 1.266, 'train_steps_per_second': 0.168, 'train_loss': 0.31040989743037656, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:44:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:44:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.4_0.35_3_8 epoch 1

------------------------------------------------

0.5655608986721077

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.4_0.35_3_8/generated_contents/1
INFO 11-27 21:44:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:45:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.4_0.35_3_8 epoch 2

------------------------------------------------

0.6160593655637826

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.4_0.35_3_8/generated_contents/2
INFO 11-27 21:45:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:45:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.4_0.35_3_8 epoch 3

------------------------------------------------

0.6154736010350887

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.4_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.4_0.35_3_8/checkpoint-33
searching parameters: task121_10_15_50_0.6_0.3_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_15_50_0.6_0.3_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_15_50_0.6_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:45:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:46:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:47:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:47:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 78
expected_example_num: 150
selection_ratio: 0.52
finetune_vicuna!
{'loss': 1.481, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.8996, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.4299, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.2891, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.301, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1436, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.1555, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 195.1332, 'train_samples_per_second': 1.199, 'train_steps_per_second': 0.154, 'train_loss': 0.5073174109061559, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:51:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:51:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.6_0.3_3_8 epoch 1

------------------------------------------------

0.6247428574430028

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_15_50_0.6_0.3_3_8/generated_contents/1
INFO 11-27 21:51:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:52:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.6_0.3_3_8 epoch 2

------------------------------------------------

0.5893870826295788

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_15_50_0.6_0.3_3_8/generated_contents/2
INFO 11-27 21:52:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:52:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_15_50_0.6_0.3_3_8 epoch 3

------------------------------------------------

0.5992177003704606

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_15_50_0.6_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_15_50_0.6_0.3_3_8/checkpoint-30
searching parameters: task121_20_10_40_0.5_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:52:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:53:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:54:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:54:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 85
expected_example_num: 200
selection_ratio: 0.425
finetune_vicuna!
{'loss': 1.8372, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.4771, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4503, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2029, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1791, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.1489, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.1056, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0598, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 200.3989, 'train_samples_per_second': 1.272, 'train_steps_per_second': 0.165, 'train_loss': 0.423405056198438, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:58:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:58:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_3_8 epoch 1

------------------------------------------------

0.6316582717814534

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.35_3_8/generated_contents/1
INFO 11-27 21:58:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:59:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_3_8 epoch 2

------------------------------------------------

0.5832253784694165

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.35_3_8/generated_contents/2
INFO 11-27 21:59:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:59:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.35_3_8 epoch 3

------------------------------------------------

0.5788099016199023

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_8
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-11 /data2/cyzhao/best_ckpt/NI_task121_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.35_3_8/checkpoint-33
searching parameters: task121_10_20_50_0.7_0.3_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.7_0.3_3_8
/home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.7_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-27 21:59:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:00:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:01:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:01:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 89
expected_example_num: 200
selection_ratio: 0.445
finetune_vicuna!
{'loss': 1.3506, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.6707, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.7023, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.32, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2035, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2068, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0851, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0924, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0808, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 199.5027, 'train_samples_per_second': 1.338, 'train_steps_per_second': 0.18, 'train_loss': 0.4124627262353897, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:05:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:06:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.3_3_8 epoch 1

------------------------------------------------

0.6111340066404599

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.7_0.3_3_8/generated_contents/1
INFO 11-27 22:06:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:06:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.3_3_8 epoch 2

------------------------------------------------

0.6161546667069814

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.7_0.3_3_8/generated_contents/2
INFO 11-27 22:06:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:06:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_10_20_50_0.7_0.3_3_8 epoch 3

------------------------------------------------

0.6113812899650634

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_10_20_50_0.7_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_10_20_50_0.7_0.3_3_8/checkpoint-36
searching parameters: task121_20_20_50_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_20_50_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_20_50_0.4_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-27 22:07:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:07:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:08:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:08:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 148
expected_example_num: 400
selection_ratio: 0.37
finetune_vicuna!
{'loss': 1.0878, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 1.7416, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.5437, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.2679, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.3412, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.1473, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.2017, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.1692, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.1797, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.1173, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.1024, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.0725, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0569, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0902, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 224.8162, 'train_samples_per_second': 1.975, 'train_steps_per_second': 0.254, 'train_loss': 0.3604619086050151, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:13:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:14:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_50_0.4_0.35_3_8 epoch 1

------------------------------------------------

0.5288283469228279

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_20_50_0.4_0.35_3_8/generated_contents/1
INFO 11-27 22:14:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:14:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_50_0.4_0.35_3_8 epoch 2

------------------------------------------------

0.5487094065916474

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_20_50_0.4_0.35_3_8/generated_contents/2
INFO 11-27 22:14:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:15:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_20_50_0.4_0.35_3_8 epoch 3

------------------------------------------------

0.5502421075101361

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_20_50_0.4_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_20_50_0.4_0.35_3_8/checkpoint-57
searching parameters: task121_30_15_50_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_30_15_50_0.4_0.35_3_8
/home/cyzhao/NI_task121_exp_8/task121_30_15_50_0.4_0.35_3_8/config.json
generate_and_write_inputs!
INFO 11-27 22:15:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:15:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:17:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:17:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 165
expected_example_num: 450
selection_ratio: 0.36666666666666664
finetune_vicuna!
{'loss': 1.4928, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 0.5345, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.5483, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.346, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.3003, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.2082, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1366, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2085, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.1402, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1492, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.0941, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0769, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0703, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0791, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0478, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 234.1035, 'train_samples_per_second': 2.114, 'train_steps_per_second': 0.269, 'train_loss': 0.2847166934183666, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:22:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:22:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_50_0.4_0.35_3_8 epoch 1

------------------------------------------------

0.5928658387807652

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_15_50_0.4_0.35_3_8/generated_contents/1
INFO 11-27 22:23:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:23:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_50_0.4_0.35_3_8 epoch 2

------------------------------------------------

0.5872765731796057

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_15_50_0.4_0.35_3_8/generated_contents/2
INFO 11-27 22:23:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:23:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_50_0.4_0.35_3_8 epoch 3

------------------------------------------------

0.5853625926065444

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_15_50_0.4_0.35_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-21
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_50_0.4_0.35_3_8/checkpoint-63
searching parameters: task121_20_10_40_0.5_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-27 22:24:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:24:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:27:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:28:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 92
expected_example_num: 200
selection_ratio: 0.46
finetune_vicuna!
{'loss': 0.9756, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.6228, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.8969, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.354, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2595, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1975, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1194, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.1325, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.1032, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 202.6937, 'train_samples_per_second': 1.362, 'train_steps_per_second': 0.178, 'train_loss': 0.40680837134520215, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:32:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:32:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.4_3_8 epoch 1

------------------------------------------------

0.5736140177219865

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.4_3_8/generated_contents/1
INFO 11-27 22:32:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:33:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.4_3_8 epoch 2

------------------------------------------------

0.5849695481655679

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.4_3_8/generated_contents/2
INFO 11-27 22:33:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:33:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_40_0.5_0.4_3_8 epoch 3

------------------------------------------------

0.5858142532606722

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_40_0.5_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_40_0.5_0.4_3_8/checkpoint-36
searching parameters: task121_20_10_45_0.5_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-27 22:33:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:34:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:35:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:35:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 92
expected_example_num: 200
selection_ratio: 0.46
finetune_vicuna!
{'loss': 0.9769, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5796, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.3409, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1691, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1491, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1493, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0639, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0656, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0635, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 201.8636, 'train_samples_per_second': 1.367, 'train_steps_per_second': 0.178, 'train_loss': 0.2842053348819415, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:39:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:42:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.5_0.4_3_8 epoch 1

------------------------------------------------

0.6319753044415607

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.4_3_8/generated_contents/1
INFO 11-27 22:42:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:42:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.5_0.4_3_8 epoch 2

------------------------------------------------

0.5788607154292964

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.4_3_8/generated_contents/2
INFO 11-27 22:42:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:43:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.5_0.4_3_8 epoch 3

------------------------------------------------

0.5936080515953032

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task121_exp_8
mv /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task121_exp_8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.4_3_8/checkpoint-36
searching parameters: task121_20_10_40_0.5_0.4_3_8
searching parameters: task121_20_10_45_0.5_0.4_3_8
searching parameters: task121_20_10_45_0.5_0.4_3_8
searching parameters: task121_20_10_45_0.5_0.4_3_8
searching parameters: task121_30_15_45_0.8_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_30_15_45_0.8_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_30_15_45_0.8_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-27 22:43:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:43:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:47:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:47:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 207
expected_example_num: 450
selection_ratio: 0.46
finetune_vicuna!
{'loss': 1.4893, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.15}
{'loss': 0.6764, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.4897, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.46}
{'loss': 0.4248, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.3002, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.77}
{'loss': 0.3123, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.2637, 'learning_rate': 3.205128205128206e-05, 'epoch': 1.08}
{'loss': 0.1697, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.1876, 'learning_rate': 2.6923076923076923e-05, 'epoch': 1.38}
{'loss': 0.1207, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1065, 'learning_rate': 2.1794871794871795e-05, 'epoch': 1.69}
{'loss': 0.1441, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1001, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0651, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.048, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.31}
{'loss': 0.0448, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0375, 'learning_rate': 6.41025641025641e-06, 'epoch': 2.62}
{'loss': 0.0344, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'loss': 0.0307, 'learning_rate': 1.282051282051282e-06, 'epoch': 2.92}
{'train_runtime': 258.1355, 'train_samples_per_second': 2.406, 'train_steps_per_second': 0.302, 'train_loss': 0.2605146638666972, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-78/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-52/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:53:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:53:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.8_0.4_3_8 epoch 1

------------------------------------------------

0.6099616643914153

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_15_45_0.8_0.4_3_8/generated_contents/1
INFO 11-27 22:54:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-52', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-52', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:54:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.8_0.4_3_8 epoch 2

------------------------------------------------

0.5975138002345644

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_15_45_0.8_0.4_3_8/generated_contents/2
INFO 11-27 22:54:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-78', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:54:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_30_15_45_0.8_0.4_3_8 epoch 3

------------------------------------------------

0.595683673165356

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_30_15_45_0.8_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-52
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_30_15_45_0.8_0.4_3_8/checkpoint-78
searching parameters: task121_20_10_45_0.7_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.7_0.4_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.7_0.4_3_8/config.json
generate_and_write_inputs!
INFO 11-27 22:55:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:55:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:56:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:56:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 88
expected_example_num: 200
selection_ratio: 0.44
finetune_vicuna!
{'loss': 1.1455, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.7934, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.6195, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2199, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1936, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.1989, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.1284, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0966, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 202.1886, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.163, 'train_loss': 0.41592937978831207, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-27 23:01:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:01:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.7_0.4_3_8 epoch 1

------------------------------------------------

0.598813309022347

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.7_0.4_3_8/generated_contents/1
INFO 11-27 23:01:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:02:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.7_0.4_3_8 epoch 2

------------------------------------------------

0.5862925372976349

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.7_0.4_3_8/generated_contents/2
INFO 11-27 23:02:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:02:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.7_0.4_3_8 epoch 3

------------------------------------------------

0.5862223283605829

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.7_0.4_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.7_0.4_3_8/checkpoint-33
searching parameters: task121_20_10_45_0.5_0.4_3_8
searching parameters: task121_20_10_45_0.5_0.3_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.3_3_8
/home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.3_3_8/config.json
generate_and_write_inputs!
INFO 11-27 23:02:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:03:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 23:04:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:04:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 94
expected_example_num: 200
selection_ratio: 0.47
finetune_vicuna!
{'loss': 1.0453, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.6177, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.6795, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2987, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2272, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1884, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1201, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0917, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.1448, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 206.3897, 'train_samples_per_second': 1.366, 'train_steps_per_second': 0.174, 'train_loss': 0.379253674712446, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 23:09:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:09:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.5_0.3_3_8 epoch 1

------------------------------------------------

0.5652863012058349

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.3_3_8/generated_contents/1
INFO 11-27 23:09:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:09:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.5_0.3_3_8 epoch 2

------------------------------------------------

0.6029259437912728

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.3_3_8/generated_contents/2
INFO 11-27 23:09:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:10:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task121_20_10_45_0.5_0.3_3_8 epoch 3

------------------------------------------------

0.5826366179468752

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/task121_20_10_45_0.5_0.3_3_8/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task121_20_10_45_0.5_0.3_3_8/checkpoint-36
{'generation_epochs': 20, 'generation_batch_size': 10, 'generation_top_k': 45, 'generation_temperature': 0.5, 'min_frequency': 0.4, 'training_epochs': 3}
test best ckpt.
INFO 11-27 23:10:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task121_exp_8', tokenizer='/data2/cyzhao/best_ckpt/NI_task121_exp_8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:10:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task121_exp_8

------------------------------------------------

0.545579930757252

------------------------------------------------


The best ckpt on test set gain 0.545579930757252
Genrated contents are stored in /home/cyzhao/NI_task121_exp_8/best_ckpt_generated_content
