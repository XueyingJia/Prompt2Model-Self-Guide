[2023-11-27 17:02:27,639] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1195
searching parameters: task1195_10_10_40_0.5_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_40_0.5_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_40_0.5_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:02:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:02:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:03:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:03:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 19
expected_example_num: 100
selection_ratio: 0.19
finetune_vicuna!
{'loss': 0.7967, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0607, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 162.107, 'train_samples_per_second': 0.352, 'train_steps_per_second': 0.056, 'train_loss': 0.3880445741944843, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:06:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:07:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_40_0.5_0.4_3_1 epoch 1

------------------------------------------------

0.057

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_40_0.5_0.4_3_1/generated_contents/1
INFO 11-27 17:07:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:08:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_40_0.5_0.4_3_1 epoch 2

------------------------------------------------

0.091

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_40_0.5_0.4_3_1/generated_contents/2
INFO 11-27 17:08:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:08:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_40_0.5_0.4_3_1 epoch 3

------------------------------------------------

0.11

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_40_0.5_0.4_3_1/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-9 /data2/cyzhao/best_ckpt/NI_task1195_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_40_0.5_0.4_3_1/checkpoint-6
searching parameters: task1195_20_10_45_0.6_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_10_45_0.6_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_10_45_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:09:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:09:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:10:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:10:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 49
expected_example_num: 200
selection_ratio: 0.245
finetune_vicuna!
{'loss': 0.3749, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4152, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1049, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0306, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0376, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 165.775, 'train_samples_per_second': 0.887, 'train_steps_per_second': 0.127, 'train_loss': 0.18713571344103133, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:14:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:14:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_45_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.04

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_10_45_0.6_0.4_3_1/generated_contents/1
INFO 11-27 17:15:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:15:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_45_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.09

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_10_45_0.6_0.4_3_1/generated_contents/2
INFO 11-27 17:16:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:16:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_45_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.085

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_10_45_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_45_0.6_0.4_3_1/checkpoint-21
searching parameters: task1195_20_10_50_0.4_0.3_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_10_50_0.4_0.3_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_10_50_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:17:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:17:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:17:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:18:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 19
expected_example_num: 200
selection_ratio: 0.095
finetune_vicuna!
{'loss': 0.2354, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0396, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 163.4346, 'train_samples_per_second': 0.349, 'train_steps_per_second': 0.055, 'train_loss': 0.12235682631014949, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:21:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:21:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_50_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.048

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_10_50_0.4_0.3_3_1/generated_contents/1
INFO 11-27 17:22:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:22:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_50_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.129

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_10_50_0.4_0.3_3_1/generated_contents/2
INFO 11-27 17:23:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:23:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_50_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.157

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_10_50_0.4_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1195_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-9 /data2/cyzhao/best_ckpt/NI_task1195_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_50_0.4_0.3_3_1/checkpoint-6
searching parameters: task1195_20_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_15_50_0.7_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:24:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:24:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:25:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:25:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 99
expected_example_num: 300
selection_ratio: 0.33
finetune_vicuna!
{'loss': 0.3117, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.2278, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.1255, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.127, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.0394, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.0342, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.0494, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0084, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0031, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 183.9085, 'train_samples_per_second': 1.615, 'train_steps_per_second': 0.212, 'train_loss': 0.09578972830413243, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:29:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:30:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_50_0.7_0.35_3_1 epoch 1

------------------------------------------------

0.267

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_15_50_0.7_0.35_3_1/generated_contents/1
INFO 11-27 17:30:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:31:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_50_0.7_0.35_3_1 epoch 2

------------------------------------------------

0.068

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_15_50_0.7_0.35_3_1/generated_contents/2
INFO 11-27 17:31:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:31:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_50_0.7_0.35_3_1 epoch 3

------------------------------------------------

0.098

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_15_50_0.7_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1195_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-13 /data2/cyzhao/best_ckpt/NI_task1195_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.7_0.35_3_1/checkpoint-39
searching parameters: task1195_30_10_40_0.4_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_30_10_40_0.4_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_30_10_40_0.4_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:32:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:32:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:33:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:33:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 19
expected_example_num: 300
selection_ratio: 0.06333333333333334
finetune_vicuna!
{'loss': 0.2598, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0002, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 168.3202, 'train_samples_per_second': 0.339, 'train_steps_per_second': 0.053, 'train_loss': 0.11553860260109003, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:37:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:37:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.4_0.35_3_1 epoch 1

------------------------------------------------

0.162

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_10_40_0.4_0.35_3_1/generated_contents/1
INFO 11-27 17:38:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:38:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.4_0.35_3_1 epoch 2

------------------------------------------------

0.247

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_10_40_0.4_0.35_3_1/generated_contents/2
INFO 11-27 17:39:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:39:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.4_0.35_3_1 epoch 3

------------------------------------------------

0.24

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_10_40_0.4_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.35_3_1/checkpoint-9
searching parameters: task1195_10_10_45_0.5_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.5_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.5_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:40:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:40:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:40:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:41:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 30
expected_example_num: 100
selection_ratio: 0.3
finetune_vicuna!
{'loss': 1.0017, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2306, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0624, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 163.5886, 'train_samples_per_second': 0.55, 'train_steps_per_second': 0.073, 'train_loss': 0.4315638268987338, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:44:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:44:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.5_0.4_3_1 epoch 1

------------------------------------------------

0.029

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.5_0.4_3_1/generated_contents/1
INFO 11-27 17:45:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:45:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.5_0.4_3_1 epoch 2

------------------------------------------------

0.071

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.5_0.4_3_1/generated_contents/2
INFO 11-27 17:46:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:46:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.5_0.4_3_1 epoch 3

------------------------------------------------

0.071

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.5_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.5_0.4_3_1/checkpoint-12
searching parameters: task1195_10_20_45_0.6_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_20_45_0.6_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_20_45_0.6_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:46:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:47:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:47:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:48:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 53
expected_example_num: 200
selection_ratio: 0.265
finetune_vicuna!
{'loss': 0.166, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.2405, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0318, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0034, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0231, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 170.9256, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.123, 'train_loss': 0.08860634482975695, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:51:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:52:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.6_0.35_3_1 epoch 1

------------------------------------------------

0.232

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_20_45_0.6_0.35_3_1/generated_contents/1
INFO 11-27 17:52:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:52:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.6_0.35_3_1 epoch 2

------------------------------------------------

0.197

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_20_45_0.6_0.35_3_1/generated_contents/2
INFO 11-27 17:53:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:53:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.6_0.35_3_1 epoch 3

------------------------------------------------

0.21

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_20_45_0.6_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.35_3_1/checkpoint-21
searching parameters: task1195_20_15_40_0.5_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_15_40_0.5_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_20_15_40_0.5_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-27 17:54:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:54:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 17:55:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:55:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 63
expected_example_num: 300
selection_ratio: 0.21
finetune_vicuna!
{'loss': 0.5653, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.4999, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1025, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.0895, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.037, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0566, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 169.9855, 'train_samples_per_second': 1.112, 'train_steps_per_second': 0.141, 'train_loss': 0.2250939067453146, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 17:59:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 17:59:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_40_0.5_0.4_3_1 epoch 1

------------------------------------------------

0.216

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_15_40_0.5_0.4_3_1/generated_contents/1
INFO 11-27 18:00:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:00:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_40_0.5_0.4_3_1 epoch 2

------------------------------------------------

0.209

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_15_40_0.5_0.4_3_1/generated_contents/2
INFO 11-27 18:01:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:01:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_40_0.5_0.4_3_1 epoch 3

------------------------------------------------

0.18

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_20_15_40_0.5_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_40_0.5_0.4_3_1/checkpoint-24
searching parameters: task1195_30_20_45_0.6_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_30_20_45_0.6_0.4_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_30_20_45_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-27 18:01:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:02:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 18:03:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:03:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 157
expected_example_num: 600
selection_ratio: 0.26166666666666666
finetune_vicuna!
{'loss': 0.3507, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}
{'loss': 0.2611, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.2247, 'learning_rate': 4e-05, 'epoch': 0.6}
{'loss': 0.0736, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.0345, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0023, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.0981, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}
{'loss': 0.0388, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.0174, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.0522, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0116, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}
{'loss': 0.0121, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0166, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}
{'loss': 0.0143, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0497, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 188.3482, 'train_samples_per_second': 2.501, 'train_steps_per_second': 0.319, 'train_loss': 0.08384489176484446, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-40/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-20/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-60/optimizer.pt
validate!
last validate 0.
INFO 11-27 18:08:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:08:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_45_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.12

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_20_45_0.6_0.4_3_1/generated_contents/1
INFO 11-27 18:09:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-40', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-40', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:09:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_45_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.154

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_20_45_0.6_0.4_3_1/generated_contents/2
INFO 11-27 18:09:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-60', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-60', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:10:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_45_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.167

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_20_45_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-40
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_45_0.6_0.4_3_1/checkpoint-60
searching parameters: task1195_10_10_45_0.6_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-27 18:10:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:11:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 18:11:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:11:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 30
expected_example_num: 100
selection_ratio: 0.3
finetune_vicuna!
{'loss': 0.2985, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1336, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0227, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 161.6053, 'train_samples_per_second': 0.557, 'train_steps_per_second': 0.074, 'train_loss': 0.15163479807476202, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-27 18:15:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:15:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.6_0.35_3_1 epoch 1

------------------------------------------------

0.094

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.35_3_1/generated_contents/1
INFO 11-27 18:16:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:16:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.6_0.35_3_1 epoch 2

------------------------------------------------

0.273

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.35_3_1/generated_contents/2
INFO 11-27 18:17:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:17:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.6_0.35_3_1 epoch 3

------------------------------------------------

0.248

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1195_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-8 /data2/cyzhao/best_ckpt/NI_task1195_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.35_3_1/checkpoint-12
searching parameters: task1195_10_15_45_0.8_0.3_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_15_45_0.8_0.3_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_15_45_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-27 18:17:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:18:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 18:18:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:19:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 47
expected_example_num: 150
selection_ratio: 0.31333333333333335
finetune_vicuna!
{'loss': 0.6609, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.0813, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.13, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.043, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 162.3862, 'train_samples_per_second': 0.868, 'train_steps_per_second': 0.111, 'train_loss': 0.20394795977820954, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 18:22:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:22:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_45_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.096

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_15_45_0.8_0.3_3_1/generated_contents/1
INFO 11-27 18:23:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:23:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_45_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.09

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_15_45_0.8_0.3_3_1/generated_contents/2
INFO 11-27 18:24:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:24:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_45_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.098

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_15_45_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.8_0.3_3_1/checkpoint-18
searching parameters: task1195_20_15_50_0.7_0.35_3_1
searching parameters: task1195_10_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_15_50_0.7_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-27 18:25:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:25:45 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 18:26:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:26:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 43
expected_example_num: 150
selection_ratio: 0.2866666666666667
finetune_vicuna!
{'loss': 0.3999, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1619, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0785, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0359, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 161.3089, 'train_samples_per_second': 0.8, 'train_steps_per_second': 0.112, 'train_loss': 0.15242057459221947, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 18:30:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:30:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_50_0.7_0.35_3_1 epoch 1

------------------------------------------------

0.1

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_15_50_0.7_0.35_3_1/generated_contents/1
INFO 11-27 18:30:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:31:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_50_0.7_0.35_3_1 epoch 2

------------------------------------------------

0.165

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_15_50_0.7_0.35_3_1/generated_contents/2
INFO 11-27 18:31:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:31:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_50_0.7_0.35_3_1 epoch 3

------------------------------------------------

0.17

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_15_50_0.7_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_50_0.7_0.35_3_1/checkpoint-18
searching parameters: task1195_20_15_50_0.7_0.35_3_1
searching parameters: task1195_30_20_50_0.8_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_30_20_50_0.8_0.35_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_30_20_50_0.8_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-27 18:32:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:32:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 18:34:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:34:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 217
expected_example_num: 600
selection_ratio: 0.3616666666666667
finetune_vicuna!
{'loss': 0.247, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.14}
{'loss': 0.2505, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.29}
{'loss': 0.1818, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.43}
{'loss': 0.0539, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.1635, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.71}
{'loss': 0.1002, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.86}
{'loss': 0.0362, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0223, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0382, 'learning_rate': 2.857142857142857e-05, 'epoch': 1.29}
{'loss': 0.0299, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}
{'loss': 0.0425, 'learning_rate': 2.380952380952381e-05, 'epoch': 1.57}
{'loss': 0.0243, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0283, 'learning_rate': 1.9047619047619046e-05, 'epoch': 1.86}
{'loss': 0.0306, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0234, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.14}
{'loss': 0.0103, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0049, 'learning_rate': 9.523809523809523e-06, 'epoch': 2.43}
{'loss': 0.0049, 'learning_rate': 7.142857142857143e-06, 'epoch': 2.57}
{'loss': 0.0041, 'learning_rate': 4.7619047619047615e-06, 'epoch': 2.71}
{'loss': 0.0164, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'loss': 0.0074, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 201.9363, 'train_samples_per_second': 3.224, 'train_steps_per_second': 0.416, 'train_loss': 0.06287701509981639, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-56/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-84/optimizer.pt
validate!
last validate 0.
INFO 11-27 18:39:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:39:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_50_0.8_0.35_3_1 epoch 1

------------------------------------------------

0.122

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_20_50_0.8_0.35_3_1/generated_contents/1
INFO 11-27 18:40:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-56', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-56', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:40:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_50_0.8_0.35_3_1 epoch 2

------------------------------------------------

0.097

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_20_50_0.8_0.35_3_1/generated_contents/2
INFO 11-27 18:41:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-84', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-84', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:41:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_50_0.8_0.35_3_1 epoch 3

------------------------------------------------

0.109

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_30_20_50_0.8_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-28
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-56
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.8_0.35_3_1/checkpoint-84
searching parameters: task1195_10_10_45_0.6_0.35_3_1
searching parameters: task1195_10_10_45_0.6_0.3_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.3_3_1
/home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-27 18:42:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:42:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 18:42:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:43:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 30
expected_example_num: 100
selection_ratio: 0.3
finetune_vicuna!
{'loss': 0.2985, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1336, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0227, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 160.5235, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.075, 'train_loss': 0.15163479807476202, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-27 18:46:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:46:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.6_0.3_3_1 epoch 1

------------------------------------------------

0.094

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.3_3_1/generated_contents/1
INFO 11-27 18:47:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:47:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.6_0.3_3_1 epoch 2

------------------------------------------------

0.273

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.3_3_1/generated_contents/2
INFO 11-27 18:48:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:48:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_45_0.6_0.3_3_1 epoch 3

------------------------------------------------

0.248

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/task1195_10_10_45_0.6_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_45_0.6_0.3_3_1/checkpoint-12
searching parameters: task1195_10_10_45_0.6_0.35_3_1
searching parameters: task1195_10_10_45_0.6_0.35_3_1
searching parameters: task1195_10_10_45_0.6_0.35_3_1
{'generation_epochs': 10, 'generation_batch_size': 10, 'generation_top_k': 45, 'generation_temperature': 0.6, 'min_frequency': 0.35, 'training_epochs': 3}
test best ckpt.
INFO 11-27 18:49:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task1195_exp_1', tokenizer='/data2/cyzhao/best_ckpt/NI_task1195_exp_1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 18:49:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task1195_exp_1

------------------------------------------------

0.247

------------------------------------------------


The best ckpt on test set gain 0.247
Genrated contents are stored in /home/cyzhao/NI_task1195_exp_1/best_ckpt_generated_content
[2023-11-27 21:02:14,291] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1195
searching parameters: task1195_30_20_50_0.5_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_20_50_0.5_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_20_50_0.5_0.3_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:02:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:02:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:04:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:04:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 121
expected_example_num: 600
selection_ratio: 0.20166666666666666
finetune_vicuna!
{'loss': 0.3828, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 0.0858, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.288, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.2504, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0797, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.0347, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.0419, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.1061, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0277, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.0466, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0131, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.0215, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 190.6552, 'train_samples_per_second': 1.904, 'train_steps_per_second': 0.252, 'train_loss': 0.11485460051335394, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:08:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:08:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_50_0.5_0.3_3_2 epoch 1

------------------------------------------------

0.8055572748223074

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_20_50_0.5_0.3_3_2/generated_contents/1
INFO 11-27 21:09:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:09:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_50_0.5_0.3_3_2 epoch 2

------------------------------------------------

0.8171193965896884

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_20_50_0.5_0.3_3_2/generated_contents/2
INFO 11-27 21:10:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:10:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_50_0.5_0.3_3_2 epoch 3

------------------------------------------------

0.8178712103858511

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_20_50_0.5_0.3_3_2/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-48 /data2/cyzhao/best_ckpt/NI_task1195_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_50_0.5_0.3_3_2/checkpoint-32
searching parameters: task1195_10_15_45_0.7_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_15_45_0.7_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_15_45_0.7_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:11:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:11:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:11:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:12:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 46
expected_example_num: 150
selection_ratio: 0.30666666666666664
finetune_vicuna!
{'loss': 0.4251, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1966, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0557, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0132, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 173.9251, 'train_samples_per_second': 0.793, 'train_steps_per_second': 0.103, 'train_loss': 0.15399285949145755, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:15:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:15:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_45_0.7_0.4_3_2 epoch 1

------------------------------------------------

0.8077915887213857

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_15_45_0.7_0.4_3_2/generated_contents/1
INFO 11-27 21:16:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:16:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_45_0.7_0.4_3_2 epoch 2

------------------------------------------------

0.7665587684008106

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_15_45_0.7_0.4_3_2/generated_contents/2
INFO 11-27 21:17:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:17:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_15_45_0.7_0.4_3_2 epoch 3

------------------------------------------------

0.8099000208283487

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_15_45_0.7_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_15_45_0.7_0.4_3_2/checkpoint-18
searching parameters: task1195_10_10_50_0.6_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_10_50_0.6_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_10_50_0.6_0.3_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:18:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:18:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:18:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:19:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 26
expected_example_num: 100
selection_ratio: 0.26
finetune_vicuna!
{'loss': 0.5942, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1617, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0173, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 165.6038, 'train_samples_per_second': 0.471, 'train_steps_per_second': 0.072, 'train_loss': 0.25775095634162426, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:22:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:22:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_50_0.6_0.3_3_2 epoch 1

------------------------------------------------

0.4180978824349833

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_10_50_0.6_0.3_3_2/generated_contents/1
INFO 11-27 21:23:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:23:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_50_0.6_0.3_3_2 epoch 2

------------------------------------------------

0.8158984348519006

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_10_50_0.6_0.3_3_2/generated_contents/2
INFO 11-27 21:24:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:24:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_10_50_0.6_0.3_3_2 epoch 3

------------------------------------------------

0.8074272147668095

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_10_50_0.6_0.3_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_10_50_0.6_0.3_3_2/checkpoint-12
searching parameters: task1195_20_10_40_0.4_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_20_10_40_0.4_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_20_10_40_0.4_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:24:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:25:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:25:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:26:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 15
expected_example_num: 200
selection_ratio: 0.075
finetune_vicuna!
{'loss': 0.4716, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 164.534, 'train_samples_per_second': 0.273, 'train_steps_per_second': 0.036, 'train_loss': 0.33998431265354156, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:29:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:29:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_40_0.4_0.4_3_2 epoch 1

------------------------------------------------

0.41770124954934784

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_10_40_0.4_0.4_3_2/generated_contents/1
INFO 11-27 21:30:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:30:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_40_0.4_0.4_3_2 epoch 2

------------------------------------------------

0.823347142766184

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_10_40_0.4_0.4_3_2/generated_contents/2
INFO 11-27 21:31:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:31:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_10_40_0.4_0.4_3_2 epoch 3

------------------------------------------------

0.8202048429499234

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_10_40_0.4_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1195_exp_2
mv /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-4 /data2/cyzhao/best_ckpt/NI_task1195_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_10_40_0.4_0.4_3_2/checkpoint-6
searching parameters: task1195_30_10_45_0.8_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.35_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:32:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:32:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:33:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:33:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 106
expected_example_num: 300
selection_ratio: 0.35333333333333333
finetune_vicuna!
{'loss': 0.2636, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.29}
{'loss': 0.227, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.0937, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.86}
{'loss': 0.0384, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0338, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}
{'loss': 0.0417, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0441, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0089, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0039, 'learning_rate': 7.142857142857143e-06, 'epoch': 2.57}
{'loss': 0.0105, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 185.228, 'train_samples_per_second': 1.717, 'train_steps_per_second': 0.227, 'train_loss': 0.07291656371255938, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:37:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:38:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.8_0.35_3_2 epoch 1

------------------------------------------------

0.8101183578081828

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.35_3_2/generated_contents/1
INFO 11-27 21:38:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:38:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.8_0.35_3_2 epoch 2

------------------------------------------------

0.8257129517952172

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.35_3_2/generated_contents/2
INFO 11-27 21:39:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:39:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.8_0.35_3_2 epoch 3

------------------------------------------------

0.8260347642402415

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.35_3_2/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1195_exp_2
mv /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-42 /data2/cyzhao/best_ckpt/NI_task1195_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.35_3_2/checkpoint-28
searching parameters: task1195_20_15_45_0.5_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_20_15_45_0.5_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_20_15_45_0.5_0.35_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:40:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:40:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:41:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:41:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 58
expected_example_num: 300
selection_ratio: 0.19333333333333333
finetune_vicuna!
{'loss': 0.9691, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.3512, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0729, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2447, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0706, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0432, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 173.3872, 'train_samples_per_second': 1.004, 'train_steps_per_second': 0.138, 'train_loss': 0.29193759709596634, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:45:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:45:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_45_0.5_0.35_3_2 epoch 1

------------------------------------------------

0.8065655038176175

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_15_45_0.5_0.35_3_2/generated_contents/1
INFO 11-27 21:46:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:46:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_45_0.5_0.35_3_2 epoch 2

------------------------------------------------

0.817690897569498

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_15_45_0.5_0.35_3_2/generated_contents/2
INFO 11-27 21:46:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:47:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_45_0.5_0.35_3_2 epoch 3

------------------------------------------------

0.824151307925273

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_15_45_0.5_0.35_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_45_0.5_0.35_3_2/checkpoint-24
searching parameters: task1195_30_10_45_0.8_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:47:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:47:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:48:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:49:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 111
expected_example_num: 300
selection_ratio: 0.37
finetune_vicuna!
{'loss': 0.2652, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.29}
{'loss': 0.982, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.1083, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.86}
{'loss': 0.0844, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0515, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}
{'loss': 0.0608, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0822, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0186, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.019, 'learning_rate': 7.142857142857143e-06, 'epoch': 2.57}
{'loss': 0.0066, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 181.9962, 'train_samples_per_second': 1.83, 'train_steps_per_second': 0.231, 'train_loss': 0.16086794019100212, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-27 21:53:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:53:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.8_0.4_3_2 epoch 1

------------------------------------------------

0.8219349500640157

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.4_3_2/generated_contents/1
INFO 11-27 21:54:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:54:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.8_0.4_3_2 epoch 2

------------------------------------------------

0.8391627523079938

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.4_3_2/generated_contents/2
INFO 11-27 21:54:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:55:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.8_0.4_3_2 epoch 3

------------------------------------------------

0.8456111961805377

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.8_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1195_exp_2
mv /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-42 /data2/cyzhao/best_ckpt/NI_task1195_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.8_0.4_3_2/checkpoint-28
searching parameters: task1195_10_20_45_0.5_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.35_3_2/config.json
generate_and_write_inputs!
INFO 11-27 21:55:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:56:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 21:56:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 21:56:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 42
expected_example_num: 200
selection_ratio: 0.21
finetune_vicuna!
{'loss': 0.2484, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1168, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0405, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0077, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 171.3107, 'train_samples_per_second': 0.736, 'train_steps_per_second': 0.105, 'train_loss': 0.09276907704770565, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:00:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:00:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.5_0.35_3_2 epoch 1

------------------------------------------------

0.8263691341556271

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.35_3_2/generated_contents/1
INFO 11-27 22:01:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:01:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.5_0.35_3_2 epoch 2

------------------------------------------------

0.8052094574422163

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.35_3_2/generated_contents/2
INFO 11-27 22:01:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:02:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.5_0.35_3_2 epoch 3

------------------------------------------------

0.8073251998720941

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.35_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.35_3_2/checkpoint-18
searching parameters: task1195_10_20_45_0.5_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:02:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:03:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:03:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:03:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 42
expected_example_num: 200
selection_ratio: 0.21
finetune_vicuna!
{'loss': 0.2484, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1168, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0405, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0077, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 173.3712, 'train_samples_per_second': 0.727, 'train_steps_per_second': 0.104, 'train_loss': 0.09276907704770565, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:07:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:07:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.5_0.4_3_2 epoch 1

------------------------------------------------

0.8263691341556271

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.4_3_2/generated_contents/1
INFO 11-27 22:08:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:08:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.5_0.4_3_2 epoch 2

------------------------------------------------

0.8052094574422163

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.4_3_2/generated_contents/2
INFO 11-27 22:08:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:09:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.5_0.4_3_2 epoch 3

------------------------------------------------

0.8073251998720941

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.5_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.5_0.4_3_2/checkpoint-18
searching parameters: task1195_30_20_40_0.5_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_20_40_0.5_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_20_40_0.5_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:09:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:10:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:11:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:11:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 121
expected_example_num: 600
selection_ratio: 0.20166666666666666
finetune_vicuna!
{'loss': 0.3828, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}
{'loss': 0.0858, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.288, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}
{'loss': 0.2504, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.079, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}
{'loss': 0.0347, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.0419, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}
{'loss': 0.1066, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0276, 'learning_rate': 1.25e-05, 'epoch': 2.25}
{'loss': 0.0466, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0137, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}
{'loss': 0.0215, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 184.9786, 'train_samples_per_second': 1.962, 'train_steps_per_second': 0.259, 'train_loss': 0.11487289572445054, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:15:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:15:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_40_0.5_0.4_3_2 epoch 1

------------------------------------------------

0.8055572748223074

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_20_40_0.5_0.4_3_2/generated_contents/1
INFO 11-27 22:16:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:16:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_40_0.5_0.4_3_2 epoch 2

------------------------------------------------

0.8175622889943857

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_20_40_0.5_0.4_3_2/generated_contents/2
INFO 11-27 22:17:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:17:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_20_40_0.5_0.4_3_2 epoch 3

------------------------------------------------

0.818274823131421

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_20_40_0.5_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_20_40_0.5_0.4_3_2/checkpoint-48
searching parameters: task1195_30_10_40_0.8_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.8_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.8_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:18:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:18:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:19:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:19:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 96
expected_example_num: 300
selection_ratio: 0.32
finetune_vicuna!
{'loss': 0.1965, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.2515, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1713, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0586, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0355, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.0742, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0021, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0308, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0202, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 191.3285, 'train_samples_per_second': 1.505, 'train_steps_per_second': 0.188, 'train_loss': 0.09343171060188776, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:24:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:24:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.8_0.4_3_2 epoch 1

------------------------------------------------

0.8225524261366406

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.8_0.4_3_2/generated_contents/1
INFO 11-27 22:25:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:25:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.8_0.4_3_2 epoch 2

------------------------------------------------

0.7755998701778201

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.8_0.4_3_2/generated_contents/2
INFO 11-27 22:26:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:26:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.8_0.4_3_2 epoch 3

------------------------------------------------

0.8118493411380208

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.8_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.8_0.4_3_2/checkpoint-36
searching parameters: task1195_10_20_45_0.8_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.8_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.8_0.35_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:27:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:27:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:28:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:28:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 72
expected_example_num: 200
selection_ratio: 0.36
finetune_vicuna!
{'loss': 0.3909, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.1912, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.0296, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0574, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.0262, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.014, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 175.4492, 'train_samples_per_second': 1.231, 'train_steps_per_second': 0.154, 'train_loss': 0.1059833632575141, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:32:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:32:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.8_0.35_3_2 epoch 1

------------------------------------------------

0.8408980973877932

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.8_0.35_3_2/generated_contents/1
INFO 11-27 22:33:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:33:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.8_0.35_3_2 epoch 2

------------------------------------------------

0.8381600432660732

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.8_0.35_3_2/generated_contents/2
INFO 11-27 22:33:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:34:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.8_0.35_3_2 epoch 3

------------------------------------------------

0.8330041374116434

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.8_0.35_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.8_0.35_3_2/checkpoint-27
searching parameters: task1195_10_20_45_0.8_0.35_3_2
searching parameters: task1195_30_10_45_0.8_0.35_3_2
searching parameters: task1195_20_15_50_0.8_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_20_15_50_0.8_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_20_15_50_0.8_0.3_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:34:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:35:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:36:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:36:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 82
expected_example_num: 300
selection_ratio: 0.2733333333333333
finetune_vicuna!
{'loss': 0.1847, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.1625, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.3598, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.0653, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.2066, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.0301, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0907, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0215, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 184.3911, 'train_samples_per_second': 1.334, 'train_steps_per_second': 0.179, 'train_loss': 0.1380464540738048, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:40:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:40:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_50_0.8_0.3_3_2 epoch 1

------------------------------------------------

0.8248381142630492

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_15_50_0.8_0.3_3_2/generated_contents/1
INFO 11-27 22:41:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:41:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_50_0.8_0.3_3_2 epoch 2

------------------------------------------------

0.8107219192892653

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_15_50_0.8_0.3_3_2/generated_contents/2
INFO 11-27 22:42:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:42:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_20_15_50_0.8_0.3_3_2 epoch 3

------------------------------------------------

0.8133334723042506

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_20_15_50_0.8_0.3_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_20_15_50_0.8_0.3_3_2/checkpoint-33
searching parameters: task1195_10_20_45_0.6_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.6_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.6_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:42:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:43:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:43:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:44:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 55
expected_example_num: 200
selection_ratio: 0.275
finetune_vicuna!
{'loss': 0.3653, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.4487, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0412, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0473, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0121, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 171.8254, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.122, 'train_loss': 0.17433435579628817, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:47:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:48:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.6_0.4_3_2 epoch 1

------------------------------------------------

0.7936810160541774

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.6_0.4_3_2/generated_contents/1
INFO 11-27 22:48:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:48:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.6_0.4_3_2 epoch 2

------------------------------------------------

0.8278164360887476

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.6_0.4_3_2/generated_contents/2
INFO 11-27 22:49:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:49:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_10_20_45_0.6_0.4_3_2 epoch 3

------------------------------------------------

0.8317416062920854

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_10_20_45_0.6_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_10_20_45_0.6_0.4_3_2/checkpoint-21
searching parameters: task1195_30_10_45_0.7_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.35_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:50:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:50:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 22:51:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:52:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 99
expected_example_num: 300
selection_ratio: 0.33
finetune_vicuna!
{'loss': 0.4301, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.1657, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.2259, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.0405, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.0321, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.0253, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.0345, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0123, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0338, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 181.6952, 'train_samples_per_second': 1.635, 'train_steps_per_second': 0.215, 'train_loss': 0.10343563079069822, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-27 22:56:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:56:29 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.7_0.35_3_2 epoch 1

------------------------------------------------

0.8453305019221808

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.35_3_2/generated_contents/1
INFO 11-27 22:57:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:57:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.7_0.35_3_2 epoch 2

------------------------------------------------

0.8310866253405034

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.35_3_2/generated_contents/2
INFO 11-27 22:57:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:58:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.7_0.35_3_2 epoch 3

------------------------------------------------

0.831520710734456

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.35_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.35_3_2/checkpoint-39
searching parameters: task1195_30_10_45_0.7_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.4_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.4_3_2/config.json
generate_and_write_inputs!
INFO 11-27 22:58:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 22:59:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 23:00:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:00:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 87
expected_example_num: 300
selection_ratio: 0.29
finetune_vicuna!
{'loss': 0.4575, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.189, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.1557, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.0956, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.0759, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.078, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0455, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0757, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 178.3942, 'train_samples_per_second': 1.463, 'train_steps_per_second': 0.185, 'train_loss': 0.14304985478520393, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-27 23:04:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:04:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.7_0.4_3_2 epoch 1

------------------------------------------------

0.764690642776494

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.4_3_2/generated_contents/1
INFO 11-27 23:05:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:05:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.7_0.4_3_2 epoch 2

------------------------------------------------

0.7991517106701752

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.4_3_2/generated_contents/2
INFO 11-27 23:05:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:06:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_45_0.7_0.4_3_2 epoch 3

------------------------------------------------

0.7994415828486741

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_45_0.7_0.4_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_45_0.7_0.4_3_2/checkpoint-33
searching parameters: task1195_30_10_50_0.7_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_50_0.7_0.35_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_50_0.7_0.35_3_2/config.json
generate_and_write_inputs!
INFO 11-27 23:06:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:07:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 23:07:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:08:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 91
expected_example_num: 300
selection_ratio: 0.30333333333333334
finetune_vicuna!
{'loss': 0.58, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.1149, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4152, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0731, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.062, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.1361, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0262, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0302, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0079, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 176.4172, 'train_samples_per_second': 1.547, 'train_steps_per_second': 0.204, 'train_loss': 0.1606267805521687, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-27 23:12:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:12:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_50_0.7_0.35_3_2 epoch 1

------------------------------------------------

0.8041420163463076

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_50_0.7_0.35_3_2/generated_contents/1
INFO 11-27 23:12:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:13:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_50_0.7_0.35_3_2 epoch 2

------------------------------------------------

0.8132177387481058

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_50_0.7_0.35_3_2/generated_contents/2
INFO 11-27 23:13:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:14:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_50_0.7_0.35_3_2 epoch 3

------------------------------------------------

0.8105185721870429

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_50_0.7_0.35_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_50_0.7_0.35_3_2/checkpoint-36
searching parameters: task1195_30_10_40_0.4_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.4_0.3_3_2
/home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.4_0.3_3_2/config.json
generate_and_write_inputs!
INFO 11-27 23:14:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:15:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-27 23:16:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:16:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 41
expected_example_num: 300
selection_ratio: 0.13666666666666666
finetune_vicuna!
{'loss': 0.7402, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1971, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0557, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0381, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 161.6356, 'train_samples_per_second': 0.761, 'train_steps_per_second': 0.111, 'train_loss': 0.23863496879736582, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-27 23:19:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:19:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.4_0.3_3_2 epoch 1

------------------------------------------------

0.8057340568452729

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.4_0.3_3_2/generated_contents/1
INFO 11-27 23:20:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:20:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.4_0.3_3_2 epoch 2

------------------------------------------------

0.80065519996416

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.4_0.3_3_2/generated_contents/2
INFO 11-27 23:21:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:21:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1195_30_10_40_0.4_0.3_3_2 epoch 3

------------------------------------------------

0.8056362298895708

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/task1195_30_10_40_0.4_0.3_3_2/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1195_30_10_40_0.4_0.3_3_2/checkpoint-18
{'generation_epochs': 30, 'generation_batch_size': 10, 'generation_top_k': 45, 'generation_temperature': 0.8, 'min_frequency': 0.4, 'training_epochs': 3}
test best ckpt.
INFO 11-27 23:22:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task1195_exp_2', tokenizer='/data2/cyzhao/best_ckpt/NI_task1195_exp_2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-27 23:22:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task1195_exp_2

------------------------------------------------

0.8414600291446583

------------------------------------------------


The best ckpt on test set gain 0.8414600291446583
Genrated contents are stored in /home/cyzhao/NI_task1195_exp_2/best_ckpt_generated_content
