[2023-11-24 05:29:54,005] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NI_task738
searching parameters: NI_task738_20_15_45_0.8_0.5_115_4
/home/cyzhao/NI_task738_exp_1/NI_task738_20_15_45_0.8_0.5_115_4
/home/cyzhao/NI_task738_exp_1/NI_task738_20_15_45_0.8_0.5_115_4/config.json
generate_and_write_inputs!
INFO 11-24 05:30:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 05:30:17 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 05:34:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 05:34:19 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 248
expected_example_num: 300
selection_ratio: 0.8266666666666667
finetune_vicuna!
{'loss': 2.0244, 'learning_rate': 4.8387096774193554e-05, 'epoch': 0.13}
{'loss': 0.1831, 'learning_rate': 4.67741935483871e-05, 'epoch': 0.26}
{'loss': 0.872, 'learning_rate': 4.516129032258064e-05, 'epoch': 0.39}
{'loss': 0.5534, 'learning_rate': 4.3548387096774194e-05, 'epoch': 0.52}
{'loss': 0.3397, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.65}
{'loss': 0.1853, 'learning_rate': 4.032258064516129e-05, 'epoch': 0.77}
{'loss': 0.1735, 'learning_rate': 3.870967741935484e-05, 'epoch': 0.9}
{'loss': 0.2737, 'learning_rate': 3.7096774193548386e-05, 'epoch': 1.03}
{'loss': 0.1731, 'learning_rate': 3.548387096774194e-05, 'epoch': 1.16}
{'loss': 0.0948, 'learning_rate': 3.387096774193548e-05, 'epoch': 1.29}
{'loss': 0.0808, 'learning_rate': 3.2258064516129034e-05, 'epoch': 1.42}
{'loss': 0.0479, 'learning_rate': 3.0645161290322585e-05, 'epoch': 1.55}
{'loss': 0.0881, 'learning_rate': 2.9032258064516133e-05, 'epoch': 1.68}
{'loss': 0.23, 'learning_rate': 2.7419354838709678e-05, 'epoch': 1.81}
{'loss': 0.0866, 'learning_rate': 2.5806451612903226e-05, 'epoch': 1.94}
{'loss': 0.1438, 'learning_rate': 2.4193548387096777e-05, 'epoch': 2.06}
{'loss': 0.0379, 'learning_rate': 2.258064516129032e-05, 'epoch': 2.19}
{'loss': 0.0365, 'learning_rate': 2.0967741935483873e-05, 'epoch': 2.32}
{'loss': 0.0575, 'learning_rate': 1.935483870967742e-05, 'epoch': 2.45}
{'loss': 0.0603, 'learning_rate': 1.774193548387097e-05, 'epoch': 2.58}
{'loss': 0.085, 'learning_rate': 1.6129032258064517e-05, 'epoch': 2.71}
{'loss': 0.0245, 'learning_rate': 1.4516129032258066e-05, 'epoch': 2.84}
{'loss': 0.0692, 'learning_rate': 1.2903225806451613e-05, 'epoch': 2.97}
{'loss': 0.0349, 'learning_rate': 1.129032258064516e-05, 'epoch': 3.1}
{'loss': 0.0397, 'learning_rate': 9.67741935483871e-06, 'epoch': 3.23}
{'loss': 0.0183, 'learning_rate': 8.064516129032258e-06, 'epoch': 3.35}
{'loss': 0.0267, 'learning_rate': 6.451612903225806e-06, 'epoch': 3.48}
{'loss': 0.0066, 'learning_rate': 4.838709677419355e-06, 'epoch': 3.61}
{'loss': 0.0176, 'learning_rate': 3.225806451612903e-06, 'epoch': 3.74}
{'loss': 0.0729, 'learning_rate': 1.6129032258064516e-06, 'epoch': 3.87}
{'loss': 0.0083, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 336.3609, 'train_samples_per_second': 2.949, 'train_steps_per_second': 0.369, 'train_loss': 0.19826374655108778, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-124/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-31/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-62/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-93/optimizer.pt
validate!
last validate 0.
INFO 11-24 05:41:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-31', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-31', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 05:41:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_20_15_45_0.8_0.5_115_4 epoch 1

------------------------------------------------

0.548

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_20_15_45_0.8_0.5_115_4/generated_contents/1
INFO 11-24 05:42:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-62', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-62', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 05:42:42 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_20_15_45_0.8_0.5_115_4 epoch 2

------------------------------------------------

0.66

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_20_15_45_0.8_0.5_115_4/generated_contents/2
INFO 11-24 05:43:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-93', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-93', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 05:43:25 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_20_15_45_0.8_0.5_115_4 epoch 3

------------------------------------------------

0.823

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_20_15_45_0.8_0.5_115_4/generated_contents/3
INFO 11-24 05:43:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-124', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-124', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 05:44:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_20_15_45_0.8_0.5_115_4 epoch 4

------------------------------------------------

0.815

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_20_15_45_0.8_0.5_115_4/generated_contents/4
mv /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-93 /data2/cyzhao/best_ckpt/NI_task738_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-31
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-62
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_20_15_45_0.8_0.5_115_4/checkpoint-124
searching parameters: NI_task738_50_40_40_1.0_0.3_115_5
/home/cyzhao/NI_task738_exp_1/NI_task738_50_40_40_1.0_0.3_115_5
/home/cyzhao/NI_task738_exp_1/NI_task738_50_40_40_1.0_0.3_115_5/config.json
generate_and_write_inputs!
INFO 11-24 05:44:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 05:45:00 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 06:04:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:04:28 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 1516
expected_example_num: 2000
selection_ratio: 0.758
finetune_vicuna!
{'loss': 2.0839, 'learning_rate': 4.978947368421053e-05, 'epoch': 0.02}
{'loss': 0.5568, 'learning_rate': 4.9578947368421055e-05, 'epoch': 0.04}
{'loss': 0.8884, 'learning_rate': 4.936842105263158e-05, 'epoch': 0.06}
{'loss': 0.7147, 'learning_rate': 4.915789473684211e-05, 'epoch': 0.08}
{'loss': 0.455, 'learning_rate': 4.8947368421052635e-05, 'epoch': 0.11}
{'loss': 0.2792, 'learning_rate': 4.873684210526316e-05, 'epoch': 0.13}
{'loss': 0.3431, 'learning_rate': 4.852631578947369e-05, 'epoch': 0.15}
{'loss': 0.2604, 'learning_rate': 4.8315789473684215e-05, 'epoch': 0.17}
{'loss': 0.2665, 'learning_rate': 4.8105263157894735e-05, 'epoch': 0.19}
{'loss': 0.3828, 'learning_rate': 4.789473684210526e-05, 'epoch': 0.21}
{'loss': 0.566, 'learning_rate': 4.7684210526315794e-05, 'epoch': 0.23}
{'loss': 0.3757, 'learning_rate': 4.747368421052632e-05, 'epoch': 0.25}
{'loss': 0.2794, 'learning_rate': 4.726315789473684e-05, 'epoch': 0.27}
{'loss': 0.4209, 'learning_rate': 4.705263157894737e-05, 'epoch': 0.29}
{'loss': 0.2467, 'learning_rate': 4.68421052631579e-05, 'epoch': 0.32}
{'loss': 0.3655, 'learning_rate': 4.663157894736842e-05, 'epoch': 0.34}
{'loss': 0.4002, 'learning_rate': 4.642105263157895e-05, 'epoch': 0.36}
{'loss': 0.2828, 'learning_rate': 4.6210526315789473e-05, 'epoch': 0.38}
{'loss': 0.2645, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.4}
{'loss': 0.3408, 'learning_rate': 4.5789473684210527e-05, 'epoch': 0.42}
{'loss': 0.2471, 'learning_rate': 4.557894736842105e-05, 'epoch': 0.44}
{'loss': 0.3995, 'learning_rate': 4.536842105263158e-05, 'epoch': 0.46}
{'loss': 0.1488, 'learning_rate': 4.515789473684211e-05, 'epoch': 0.48}
{'loss': 0.4379, 'learning_rate': 4.494736842105263e-05, 'epoch': 0.51}
{'loss': 0.2866, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.53}
{'loss': 0.2855, 'learning_rate': 4.4526315789473686e-05, 'epoch': 0.55}
{'loss': 0.4286, 'learning_rate': 4.431578947368421e-05, 'epoch': 0.57}
{'loss': 0.3097, 'learning_rate': 4.410526315789474e-05, 'epoch': 0.59}
{'loss': 0.3094, 'learning_rate': 4.3894736842105266e-05, 'epoch': 0.61}
{'loss': 0.2445, 'learning_rate': 4.368421052631579e-05, 'epoch': 0.63}
{'loss': 0.3922, 'learning_rate': 4.347368421052632e-05, 'epoch': 0.65}
{'loss': 0.1844, 'learning_rate': 4.3263157894736845e-05, 'epoch': 0.67}
{'loss': 0.1591, 'learning_rate': 4.305263157894737e-05, 'epoch': 0.69}
{'loss': 0.1146, 'learning_rate': 4.284210526315789e-05, 'epoch': 0.72}
{'loss': 0.2884, 'learning_rate': 4.2631578947368425e-05, 'epoch': 0.74}
{'loss': 0.3664, 'learning_rate': 4.242105263157895e-05, 'epoch': 0.76}
{'loss': 0.4545, 'learning_rate': 4.221052631578948e-05, 'epoch': 0.78}
{'loss': 0.3262, 'learning_rate': 4.2e-05, 'epoch': 0.8}
{'loss': 0.1416, 'learning_rate': 4.178947368421053e-05, 'epoch': 0.82}
{'loss': 0.2248, 'learning_rate': 4.157894736842106e-05, 'epoch': 0.84}
{'loss': 0.2317, 'learning_rate': 4.136842105263158e-05, 'epoch': 0.86}
{'loss': 0.1359, 'learning_rate': 4.1157894736842104e-05, 'epoch': 0.88}
{'loss': 0.6361, 'learning_rate': 4.094736842105264e-05, 'epoch': 0.91}
{'loss': 0.3, 'learning_rate': 4.0736842105263164e-05, 'epoch': 0.93}
{'loss': 0.2158, 'learning_rate': 4.0526315789473684e-05, 'epoch': 0.95}
{'loss': 0.1941, 'learning_rate': 4.031578947368421e-05, 'epoch': 0.97}
{'loss': 0.5508, 'learning_rate': 4.010526315789474e-05, 'epoch': 0.99}
{'loss': 0.0805, 'learning_rate': 3.989473684210526e-05, 'epoch': 1.01}
{'loss': 0.113, 'learning_rate': 3.968421052631579e-05, 'epoch': 1.03}
{'loss': 0.1414, 'learning_rate': 3.9473684210526316e-05, 'epoch': 1.05}
{'loss': 0.1783, 'learning_rate': 3.926315789473684e-05, 'epoch': 1.07}
{'loss': 0.1831, 'learning_rate': 3.905263157894737e-05, 'epoch': 1.09}
{'loss': 0.2117, 'learning_rate': 3.8842105263157896e-05, 'epoch': 1.12}
{'loss': 0.1435, 'learning_rate': 3.863157894736842e-05, 'epoch': 1.14}
{'loss': 0.2489, 'learning_rate': 3.842105263157895e-05, 'epoch': 1.16}
{'loss': 0.1966, 'learning_rate': 3.8210526315789476e-05, 'epoch': 1.18}
{'loss': 0.1474, 'learning_rate': 3.8e-05, 'epoch': 1.2}
{'loss': 0.2811, 'learning_rate': 3.778947368421053e-05, 'epoch': 1.22}
{'loss': 0.0909, 'learning_rate': 3.7578947368421055e-05, 'epoch': 1.24}
{'loss': 0.0453, 'learning_rate': 3.736842105263158e-05, 'epoch': 1.26}
{'loss': 0.1221, 'learning_rate': 3.715789473684211e-05, 'epoch': 1.28}
{'loss': 0.1283, 'learning_rate': 3.6947368421052635e-05, 'epoch': 1.31}
{'loss': 0.4655, 'learning_rate': 3.673684210526316e-05, 'epoch': 1.33}
{'loss': 0.1559, 'learning_rate': 3.652631578947369e-05, 'epoch': 1.35}
{'loss': 0.2957, 'learning_rate': 3.6315789473684214e-05, 'epoch': 1.37}
{'loss': 0.1644, 'learning_rate': 3.6105263157894734e-05, 'epoch': 1.39}
{'loss': 0.1413, 'learning_rate': 3.589473684210527e-05, 'epoch': 1.41}
{'loss': 0.2148, 'learning_rate': 3.5684210526315794e-05, 'epoch': 1.43}
{'loss': 0.1362, 'learning_rate': 3.547368421052632e-05, 'epoch': 1.45}
{'loss': 0.1341, 'learning_rate': 3.526315789473684e-05, 'epoch': 1.47}
{'loss': 0.122, 'learning_rate': 3.505263157894737e-05, 'epoch': 1.49}
{'loss': 0.124, 'learning_rate': 3.48421052631579e-05, 'epoch': 1.52}
{'loss': 0.2162, 'learning_rate': 3.463157894736842e-05, 'epoch': 1.54}
{'loss': 0.0571, 'learning_rate': 3.442105263157895e-05, 'epoch': 1.56}
{'loss': 0.1608, 'learning_rate': 3.421052631578947e-05, 'epoch': 1.58}
{'loss': 0.1555, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.6}
{'loss': 0.1719, 'learning_rate': 3.3789473684210526e-05, 'epoch': 1.62}
{'loss': 0.091, 'learning_rate': 3.357894736842105e-05, 'epoch': 1.64}
{'loss': 0.0679, 'learning_rate': 3.336842105263158e-05, 'epoch': 1.66}
{'loss': 0.2109, 'learning_rate': 3.3157894736842106e-05, 'epoch': 1.68}
{'loss': 0.1306, 'learning_rate': 3.294736842105263e-05, 'epoch': 1.71}
{'loss': 0.074, 'learning_rate': 3.273684210526316e-05, 'epoch': 1.73}
{'loss': 0.1585, 'learning_rate': 3.2526315789473686e-05, 'epoch': 1.75}
{'loss': 0.1049, 'learning_rate': 3.231578947368421e-05, 'epoch': 1.77}
{'loss': 0.1365, 'learning_rate': 3.210526315789474e-05, 'epoch': 1.79}
{'loss': 0.1742, 'learning_rate': 3.1894736842105265e-05, 'epoch': 1.81}
{'loss': 0.1623, 'learning_rate': 3.168421052631579e-05, 'epoch': 1.83}
{'loss': 0.1556, 'learning_rate': 3.147368421052632e-05, 'epoch': 1.85}
{'loss': 0.0927, 'learning_rate': 3.1263157894736845e-05, 'epoch': 1.87}
{'loss': 0.162, 'learning_rate': 3.105263157894737e-05, 'epoch': 1.89}
{'loss': 0.0837, 'learning_rate': 3.084210526315789e-05, 'epoch': 1.92}
{'loss': 0.2268, 'learning_rate': 3.0631578947368425e-05, 'epoch': 1.94}
{'loss': 0.1661, 'learning_rate': 3.042105263157895e-05, 'epoch': 1.96}
{'loss': 0.1853, 'learning_rate': 3.021052631578947e-05, 'epoch': 1.98}
{'loss': 0.069, 'learning_rate': 3e-05, 'epoch': 2.0}
{'loss': 0.1229, 'learning_rate': 2.9789473684210527e-05, 'epoch': 2.02}
{'loss': 0.0554, 'learning_rate': 2.9578947368421057e-05, 'epoch': 2.04}
{'loss': 0.135, 'learning_rate': 2.9368421052631577e-05, 'epoch': 2.06}
{'loss': 0.1557, 'learning_rate': 2.9157894736842107e-05, 'epoch': 2.08}
{'loss': 0.0527, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.11}
{'loss': 0.0717, 'learning_rate': 2.8736842105263163e-05, 'epoch': 2.13}
{'loss': 0.1279, 'learning_rate': 2.8526315789473683e-05, 'epoch': 2.15}
{'loss': 0.122, 'learning_rate': 2.8315789473684213e-05, 'epoch': 2.17}
{'loss': 0.0942, 'learning_rate': 2.810526315789474e-05, 'epoch': 2.19}
{'loss': 0.0317, 'learning_rate': 2.7894736842105263e-05, 'epoch': 2.21}
{'loss': 0.1227, 'learning_rate': 2.768421052631579e-05, 'epoch': 2.23}
{'loss': 0.0968, 'learning_rate': 2.747368421052632e-05, 'epoch': 2.25}
{'loss': 0.0846, 'learning_rate': 2.7263157894736846e-05, 'epoch': 2.27}
{'loss': 0.0729, 'learning_rate': 2.705263157894737e-05, 'epoch': 2.29}
{'loss': 0.0258, 'learning_rate': 2.6842105263157896e-05, 'epoch': 2.32}
{'loss': 0.0316, 'learning_rate': 2.6631578947368426e-05, 'epoch': 2.34}
{'loss': 0.0993, 'learning_rate': 2.6421052631578945e-05, 'epoch': 2.36}
{'loss': 0.1411, 'learning_rate': 2.6210526315789475e-05, 'epoch': 2.38}
{'loss': 0.0864, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.4}
{'loss': 0.0525, 'learning_rate': 2.578947368421053e-05, 'epoch': 2.42}
{'loss': 0.0471, 'learning_rate': 2.557894736842105e-05, 'epoch': 2.44}
{'loss': 0.0607, 'learning_rate': 2.536842105263158e-05, 'epoch': 2.46}
{'loss': 0.0654, 'learning_rate': 2.5157894736842108e-05, 'epoch': 2.48}
{'loss': 0.0763, 'learning_rate': 2.4947368421052635e-05, 'epoch': 2.51}
{'loss': 0.0998, 'learning_rate': 2.4736842105263158e-05, 'epoch': 2.53}
{'loss': 0.1237, 'learning_rate': 2.4526315789473688e-05, 'epoch': 2.55}
{'loss': 0.0467, 'learning_rate': 2.431578947368421e-05, 'epoch': 2.57}
{'loss': 0.0442, 'learning_rate': 2.410526315789474e-05, 'epoch': 2.59}
{'loss': 0.0493, 'learning_rate': 2.3894736842105264e-05, 'epoch': 2.61}
{'loss': 0.0497, 'learning_rate': 2.368421052631579e-05, 'epoch': 2.63}
{'loss': 0.0674, 'learning_rate': 2.3473684210526317e-05, 'epoch': 2.65}
{'loss': 0.0893, 'learning_rate': 2.3263157894736844e-05, 'epoch': 2.67}
{'loss': 0.1028, 'learning_rate': 2.305263157894737e-05, 'epoch': 2.69}
{'loss': 0.0303, 'learning_rate': 2.2842105263157897e-05, 'epoch': 2.72}
{'loss': 0.0927, 'learning_rate': 2.2631578947368423e-05, 'epoch': 2.74}
{'loss': 0.0559, 'learning_rate': 2.242105263157895e-05, 'epoch': 2.76}
{'loss': 0.0588, 'learning_rate': 2.2210526315789476e-05, 'epoch': 2.78}
{'loss': 0.0632, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.8}
{'loss': 0.1439, 'learning_rate': 2.1789473684210526e-05, 'epoch': 2.82}
{'loss': 0.0994, 'learning_rate': 2.1578947368421053e-05, 'epoch': 2.84}
{'loss': 0.0874, 'learning_rate': 2.136842105263158e-05, 'epoch': 2.86}
{'loss': 0.1008, 'learning_rate': 2.1157894736842106e-05, 'epoch': 2.88}
{'loss': 0.1252, 'learning_rate': 2.0947368421052632e-05, 'epoch': 2.91}
{'loss': 0.1331, 'learning_rate': 2.073684210526316e-05, 'epoch': 2.93}
{'loss': 0.1128, 'learning_rate': 2.0526315789473685e-05, 'epoch': 2.95}
{'loss': 0.1422, 'learning_rate': 2.0315789473684212e-05, 'epoch': 2.97}
{'loss': 0.056, 'learning_rate': 2.010526315789474e-05, 'epoch': 2.99}
{'loss': 0.0595, 'learning_rate': 1.9894736842105265e-05, 'epoch': 3.01}
{'loss': 0.009, 'learning_rate': 1.968421052631579e-05, 'epoch': 3.03}
{'loss': 0.055, 'learning_rate': 1.9473684210526315e-05, 'epoch': 3.05}
{'loss': 0.0269, 'learning_rate': 1.9263157894736845e-05, 'epoch': 3.07}
{'loss': 0.0312, 'learning_rate': 1.9052631578947368e-05, 'epoch': 3.09}
{'loss': 0.0452, 'learning_rate': 1.8842105263157894e-05, 'epoch': 3.12}
{'loss': 0.0484, 'learning_rate': 1.863157894736842e-05, 'epoch': 3.14}
{'loss': 0.0507, 'learning_rate': 1.8421052631578947e-05, 'epoch': 3.16}
{'loss': 0.0353, 'learning_rate': 1.8210526315789474e-05, 'epoch': 3.18}
{'loss': 0.088, 'learning_rate': 1.8e-05, 'epoch': 3.2}
{'loss': 0.0219, 'learning_rate': 1.7789473684210527e-05, 'epoch': 3.22}
{'loss': 0.0074, 'learning_rate': 1.7578947368421054e-05, 'epoch': 3.24}
{'loss': 0.0473, 'learning_rate': 1.736842105263158e-05, 'epoch': 3.26}
{'loss': 0.0227, 'learning_rate': 1.7157894736842107e-05, 'epoch': 3.28}
{'loss': 0.0201, 'learning_rate': 1.694736842105263e-05, 'epoch': 3.31}
{'loss': 0.0224, 'learning_rate': 1.673684210526316e-05, 'epoch': 3.33}
{'loss': 0.0386, 'learning_rate': 1.6526315789473683e-05, 'epoch': 3.35}
{'loss': 0.009, 'learning_rate': 1.6315789473684213e-05, 'epoch': 3.37}
{'loss': 0.0146, 'learning_rate': 1.6105263157894736e-05, 'epoch': 3.39}
{'loss': 0.0053, 'learning_rate': 1.5894736842105266e-05, 'epoch': 3.41}
{'loss': 0.0359, 'learning_rate': 1.568421052631579e-05, 'epoch': 3.43}
{'loss': 0.0138, 'learning_rate': 1.5473684210526316e-05, 'epoch': 3.45}
{'loss': 0.041, 'learning_rate': 1.5263157894736842e-05, 'epoch': 3.47}
{'loss': 0.025, 'learning_rate': 1.5052631578947369e-05, 'epoch': 3.49}
{'loss': 0.0199, 'learning_rate': 1.4842105263157895e-05, 'epoch': 3.52}
{'loss': 0.0459, 'learning_rate': 1.4631578947368422e-05, 'epoch': 3.54}
{'loss': 0.0379, 'learning_rate': 1.4421052631578948e-05, 'epoch': 3.56}
{'loss': 0.0045, 'learning_rate': 1.4210526315789475e-05, 'epoch': 3.58}
{'loss': 0.0241, 'learning_rate': 1.4000000000000001e-05, 'epoch': 3.6}
{'loss': 0.0285, 'learning_rate': 1.3789473684210526e-05, 'epoch': 3.62}
{'loss': 0.0085, 'learning_rate': 1.3578947368421053e-05, 'epoch': 3.64}
{'loss': 0.019, 'learning_rate': 1.336842105263158e-05, 'epoch': 3.66}
{'loss': 0.0212, 'learning_rate': 1.3157894736842106e-05, 'epoch': 3.68}
{'loss': 0.0183, 'learning_rate': 1.2947368421052633e-05, 'epoch': 3.71}
{'loss': 0.051, 'learning_rate': 1.2736842105263157e-05, 'epoch': 3.73}
{'loss': 0.0288, 'learning_rate': 1.2526315789473686e-05, 'epoch': 3.75}
{'loss': 0.0485, 'learning_rate': 1.231578947368421e-05, 'epoch': 3.77}
{'loss': 0.01, 'learning_rate': 1.2105263157894737e-05, 'epoch': 3.79}
{'loss': 0.0173, 'learning_rate': 1.1894736842105264e-05, 'epoch': 3.81}
{'loss': 0.025, 'learning_rate': 1.168421052631579e-05, 'epoch': 3.83}
{'loss': 0.0163, 'learning_rate': 1.1473684210526315e-05, 'epoch': 3.85}
{'loss': 0.0145, 'learning_rate': 1.1263157894736842e-05, 'epoch': 3.87}
{'loss': 0.0257, 'learning_rate': 1.1052631578947368e-05, 'epoch': 3.89}
{'loss': 0.018, 'learning_rate': 1.0842105263157895e-05, 'epoch': 3.92}
{'loss': 0.0388, 'learning_rate': 1.0631578947368421e-05, 'epoch': 3.94}
{'loss': 0.0089, 'learning_rate': 1.0421052631578948e-05, 'epoch': 3.96}
{'loss': 0.0095, 'learning_rate': 1.0210526315789474e-05, 'epoch': 3.98}
{'loss': 0.0309, 'learning_rate': 1e-05, 'epoch': 4.0}
{'loss': 0.0062, 'learning_rate': 9.789473684210526e-06, 'epoch': 4.02}
{'loss': 0.0062, 'learning_rate': 9.578947368421052e-06, 'epoch': 4.04}
{'loss': 0.0105, 'learning_rate': 9.368421052631579e-06, 'epoch': 4.06}
{'loss': 0.0013, 'learning_rate': 9.157894736842105e-06, 'epoch': 4.08}
{'loss': 0.0133, 'learning_rate': 8.947368421052632e-06, 'epoch': 4.11}
{'loss': 0.0041, 'learning_rate': 8.736842105263158e-06, 'epoch': 4.13}
{'loss': 0.0028, 'learning_rate': 8.526315789473685e-06, 'epoch': 4.15}
{'loss': 0.0057, 'learning_rate': 8.315789473684212e-06, 'epoch': 4.17}
{'loss': 0.0133, 'learning_rate': 8.105263157894736e-06, 'epoch': 4.19}
{'loss': 0.0022, 'learning_rate': 7.894736842105263e-06, 'epoch': 4.21}
{'loss': 0.0162, 'learning_rate': 7.68421052631579e-06, 'epoch': 4.23}
{'loss': 0.0081, 'learning_rate': 7.473684210526316e-06, 'epoch': 4.25}
{'loss': 0.0027, 'learning_rate': 7.2631578947368426e-06, 'epoch': 4.27}
{'loss': 0.0106, 'learning_rate': 7.052631578947369e-06, 'epoch': 4.29}
{'loss': 0.0305, 'learning_rate': 6.842105263157896e-06, 'epoch': 4.32}
{'loss': 0.0026, 'learning_rate': 6.631578947368422e-06, 'epoch': 4.34}
{'loss': 0.0084, 'learning_rate': 6.421052631578947e-06, 'epoch': 4.36}
{'loss': 0.0116, 'learning_rate': 6.2105263157894745e-06, 'epoch': 4.38}
{'loss': 0.0028, 'learning_rate': 6e-06, 'epoch': 4.4}
{'loss': 0.0139, 'learning_rate': 5.789473684210527e-06, 'epoch': 4.42}
{'loss': 0.0032, 'learning_rate': 5.578947368421053e-06, 'epoch': 4.44}
{'loss': 0.0034, 'learning_rate': 5.36842105263158e-06, 'epoch': 4.46}
{'loss': 0.0056, 'learning_rate': 5.1578947368421055e-06, 'epoch': 4.48}
{'loss': 0.0016, 'learning_rate': 4.947368421052632e-06, 'epoch': 4.51}
{'loss': 0.0039, 'learning_rate': 4.736842105263159e-06, 'epoch': 4.53}
{'loss': 0.0108, 'learning_rate': 4.526315789473685e-06, 'epoch': 4.55}
{'loss': 0.0029, 'learning_rate': 4.315789473684211e-06, 'epoch': 4.57}
{'loss': 0.0023, 'learning_rate': 4.105263157894737e-06, 'epoch': 4.59}
{'loss': 0.0196, 'learning_rate': 3.894736842105264e-06, 'epoch': 4.61}
{'loss': 0.0068, 'learning_rate': 3.6842105263157892e-06, 'epoch': 4.63}
{'loss': 0.0017, 'learning_rate': 3.4736842105263158e-06, 'epoch': 4.65}
{'loss': 0.0074, 'learning_rate': 3.2631578947368423e-06, 'epoch': 4.67}
{'loss': 0.0131, 'learning_rate': 3.0526315789473684e-06, 'epoch': 4.69}
{'loss': 0.0031, 'learning_rate': 2.842105263157895e-06, 'epoch': 4.72}
{'loss': 0.0037, 'learning_rate': 2.631578947368421e-06, 'epoch': 4.74}
{'loss': 0.0033, 'learning_rate': 2.4210526315789477e-06, 'epoch': 4.76}
{'loss': 0.0092, 'learning_rate': 2.2105263157894738e-06, 'epoch': 4.78}
{'loss': 0.002, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.8}
{'loss': 0.0045, 'learning_rate': 1.7894736842105262e-06, 'epoch': 4.82}
{'loss': 0.0005, 'learning_rate': 1.5789473684210528e-06, 'epoch': 4.84}
{'loss': 0.005, 'learning_rate': 1.3684210526315791e-06, 'epoch': 4.86}
{'loss': 0.0279, 'learning_rate': 1.1578947368421055e-06, 'epoch': 4.88}
{'loss': 0.0109, 'learning_rate': 9.473684210526317e-07, 'epoch': 4.91}
{'loss': 0.0138, 'learning_rate': 7.368421052631579e-07, 'epoch': 4.93}
{'loss': 0.0027, 'learning_rate': 5.263157894736843e-07, 'epoch': 4.95}
{'loss': 0.0042, 'learning_rate': 3.1578947368421055e-07, 'epoch': 4.97}
{'loss': 0.0142, 'learning_rate': 1.0526315789473685e-07, 'epoch': 4.99}
{'train_runtime': 1443.5115, 'train_samples_per_second': 5.251, 'train_steps_per_second': 0.658, 'train_loss': 0.13045151573630345, 'epoch': 5.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-760/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-190/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-950/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-380/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-570/optimizer.pt
validate!
last validate 0.
INFO 11-24 06:35:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-190', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-190', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:36:16 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_40_1.0_0.3_115_5 epoch 1

------------------------------------------------

0.75

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_40_1.0_0.3_115_5/generated_contents/1
INFO 11-24 06:36:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-380', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-380', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:36:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_40_1.0_0.3_115_5 epoch 2

------------------------------------------------

0.816

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_40_1.0_0.3_115_5/generated_contents/2
INFO 11-24 06:37:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-570', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-570', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:37:37 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_40_1.0_0.3_115_5 epoch 3

------------------------------------------------

0.825

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_40_1.0_0.3_115_5/generated_contents/3
INFO 11-24 06:38:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-760', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-760', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:38:17 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_40_1.0_0.3_115_5 epoch 4

------------------------------------------------

0.848

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_40_1.0_0.3_115_5/generated_contents/4
INFO 11-24 06:38:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-950', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-950', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:38:57 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_40_1.0_0.3_115_5 epoch 5

------------------------------------------------

0.849

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_40_1.0_0.3_115_5/generated_contents/5
rm -rf /data2/cyzhao/best_ckpt/NI_task738_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-950 /data2/cyzhao/best_ckpt/NI_task738_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-190
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-380
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-570
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_40_1.0_0.3_115_5/checkpoint-760
searching parameters: NI_task738_50_10_45_0.9_0.35_125_3
/home/cyzhao/NI_task738_exp_1/NI_task738_50_10_45_0.9_0.35_125_3
/home/cyzhao/NI_task738_exp_1/NI_task738_50_10_45_0.9_0.35_125_3/config.json
generate_and_write_inputs!
INFO 11-24 06:39:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:39:46 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 06:51:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 06:51:43 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 401
expected_example_num: 500
selection_ratio: 0.802
finetune_vicuna!
{'loss': 0.5386, 'learning_rate': 4.869281045751634e-05, 'epoch': 0.08}
{'loss': 0.4984, 'learning_rate': 4.738562091503268e-05, 'epoch': 0.16}
{'loss': 1.2922, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 0.2907, 'learning_rate': 4.477124183006536e-05, 'epoch': 0.31}
{'loss': 0.2233, 'learning_rate': 4.3464052287581704e-05, 'epoch': 0.39}
{'loss': 0.3645, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.274, 'learning_rate': 4.084967320261438e-05, 'epoch': 0.55}
{'loss': 0.1695, 'learning_rate': 3.954248366013072e-05, 'epoch': 0.63}
{'loss': 0.351, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.1694, 'learning_rate': 3.6928104575163405e-05, 'epoch': 0.78}
{'loss': 0.2735, 'learning_rate': 3.562091503267974e-05, 'epoch': 0.86}
{'loss': 0.1722, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.2206, 'learning_rate': 3.300653594771242e-05, 'epoch': 1.02}
{'loss': 0.2535, 'learning_rate': 3.169934640522876e-05, 'epoch': 1.1}
{'loss': 0.0846, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.072, 'learning_rate': 2.9084967320261443e-05, 'epoch': 1.25}
{'loss': 0.1935, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1454, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.0823, 'learning_rate': 2.516339869281046e-05, 'epoch': 1.49}
{'loss': 0.0794, 'learning_rate': 2.38562091503268e-05, 'epoch': 1.57}
{'loss': 0.0544, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.0553, 'learning_rate': 2.1241830065359477e-05, 'epoch': 1.73}
{'loss': 0.1231, 'learning_rate': 1.993464052287582e-05, 'epoch': 1.8}
{'loss': 0.0916, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.1313, 'learning_rate': 1.7320261437908496e-05, 'epoch': 1.96}
{'loss': 0.087, 'learning_rate': 1.6013071895424836e-05, 'epoch': 2.04}
{'loss': 0.0689, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.0261, 'learning_rate': 1.3398692810457516e-05, 'epoch': 2.2}
{'loss': 0.0375, 'learning_rate': 1.2091503267973856e-05, 'epoch': 2.27}
{'loss': 0.0307, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.1703, 'learning_rate': 9.477124183006535e-06, 'epoch': 2.43}
{'loss': 0.0159, 'learning_rate': 8.169934640522877e-06, 'epoch': 2.51}
{'loss': 0.0475, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.0216, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0769, 'learning_rate': 4.2483660130718954e-06, 'epoch': 2.75}
{'loss': 0.0609, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'loss': 0.0397, 'learning_rate': 1.6339869281045753e-06, 'epoch': 2.9}
{'loss': 0.0425, 'learning_rate': 3.2679738562091505e-07, 'epoch': 2.98}
{'train_runtime': 373.048, 'train_samples_per_second': 3.225, 'train_steps_per_second': 0.41, 'train_loss': 0.18129192173383593, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-153/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-102/optimizer.pt
validate!
last validate 0.
INFO 11-24 07:00:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:00:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_10_45_0.9_0.35_125_3 epoch 1

------------------------------------------------

0.452

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_10_45_0.9_0.35_125_3/generated_contents/1
INFO 11-24 07:01:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-102', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-102', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:01:30 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_10_45_0.9_0.35_125_3 epoch 2

------------------------------------------------

0.698

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_10_45_0.9_0.35_125_3/generated_contents/2
INFO 11-24 07:01:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-153', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-153', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:02:13 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_10_45_0.9_0.35_125_3 epoch 3

------------------------------------------------

0.704

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_10_45_0.9_0.35_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-51
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-102
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_10_45_0.9_0.35_125_3/checkpoint-153
searching parameters: NI_task738_50_40_50_0.9_0.3_130_3
/home/cyzhao/NI_task738_exp_1/NI_task738_50_40_50_0.9_0.3_130_3
/home/cyzhao/NI_task738_exp_1/NI_task738_50_40_50_0.9_0.3_130_3/config.json
generate_and_write_inputs!
INFO 11-24 07:02:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:02:59 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 07:19:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:20:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 1572
expected_example_num: 2000
selection_ratio: 0.786
finetune_vicuna!
{'loss': 1.7446, 'learning_rate': 4.966159052453469e-05, 'epoch': 0.02}
{'loss': 1.0878, 'learning_rate': 4.932318104906938e-05, 'epoch': 0.04}
{'loss': 0.5451, 'learning_rate': 4.8984771573604064e-05, 'epoch': 0.06}
{'loss': 0.4427, 'learning_rate': 4.864636209813875e-05, 'epoch': 0.08}
{'loss': 0.4126, 'learning_rate': 4.8307952622673436e-05, 'epoch': 0.1}
{'loss': 0.3029, 'learning_rate': 4.7969543147208126e-05, 'epoch': 0.12}
{'loss': 0.6865, 'learning_rate': 4.763113367174281e-05, 'epoch': 0.14}
{'loss': 0.2607, 'learning_rate': 4.72927241962775e-05, 'epoch': 0.16}
{'loss': 0.1499, 'learning_rate': 4.695431472081219e-05, 'epoch': 0.18}
{'loss': 0.4451, 'learning_rate': 4.661590524534687e-05, 'epoch': 0.2}
{'loss': 0.247, 'learning_rate': 4.627749576988156e-05, 'epoch': 0.22}
{'loss': 0.4848, 'learning_rate': 4.593908629441624e-05, 'epoch': 0.24}
{'loss': 0.4348, 'learning_rate': 4.560067681895093e-05, 'epoch': 0.26}
{'loss': 0.3807, 'learning_rate': 4.526226734348562e-05, 'epoch': 0.28}
{'loss': 0.1371, 'learning_rate': 4.492385786802031e-05, 'epoch': 0.3}
{'loss': 0.3405, 'learning_rate': 4.458544839255499e-05, 'epoch': 0.32}
{'loss': 0.3467, 'learning_rate': 4.4247038917089676e-05, 'epoch': 0.35}
{'loss': 0.234, 'learning_rate': 4.3908629441624365e-05, 'epoch': 0.37}
{'loss': 0.1719, 'learning_rate': 4.3570219966159055e-05, 'epoch': 0.39}
{'loss': 0.1905, 'learning_rate': 4.3231810490693744e-05, 'epoch': 0.41}
{'loss': 0.1434, 'learning_rate': 4.289340101522843e-05, 'epoch': 0.43}
{'loss': 0.548, 'learning_rate': 4.2554991539763116e-05, 'epoch': 0.45}
{'loss': 0.3605, 'learning_rate': 4.2216582064297806e-05, 'epoch': 0.47}
{'loss': 0.1995, 'learning_rate': 4.187817258883249e-05, 'epoch': 0.49}
{'loss': 0.3579, 'learning_rate': 4.153976311336718e-05, 'epoch': 0.51}
{'loss': 0.2085, 'learning_rate': 4.120135363790186e-05, 'epoch': 0.53}
{'loss': 0.1867, 'learning_rate': 4.086294416243655e-05, 'epoch': 0.55}
{'loss': 0.1924, 'learning_rate': 4.052453468697124e-05, 'epoch': 0.57}
{'loss': 0.2806, 'learning_rate': 4.018612521150593e-05, 'epoch': 0.59}
{'loss': 0.2261, 'learning_rate': 3.9847715736040605e-05, 'epoch': 0.61}
{'loss': 0.3985, 'learning_rate': 3.9509306260575295e-05, 'epoch': 0.63}
{'loss': 0.3384, 'learning_rate': 3.9170896785109984e-05, 'epoch': 0.65}
{'loss': 0.3059, 'learning_rate': 3.8832487309644673e-05, 'epoch': 0.67}
{'loss': 0.2254, 'learning_rate': 3.8494077834179356e-05, 'epoch': 0.69}
{'loss': 0.1027, 'learning_rate': 3.8155668358714046e-05, 'epoch': 0.71}
{'loss': 0.39, 'learning_rate': 3.7817258883248735e-05, 'epoch': 0.73}
{'loss': 0.1389, 'learning_rate': 3.747884940778342e-05, 'epoch': 0.75}
{'loss': 0.2548, 'learning_rate': 3.714043993231811e-05, 'epoch': 0.77}
{'loss': 0.1792, 'learning_rate': 3.680203045685279e-05, 'epoch': 0.79}
{'loss': 0.2805, 'learning_rate': 3.646362098138748e-05, 'epoch': 0.81}
{'loss': 0.1444, 'learning_rate': 3.612521150592217e-05, 'epoch': 0.83}
{'loss': 0.1305, 'learning_rate': 3.578680203045686e-05, 'epoch': 0.85}
{'loss': 0.2848, 'learning_rate': 3.544839255499154e-05, 'epoch': 0.87}
{'loss': 0.1703, 'learning_rate': 3.5109983079526224e-05, 'epoch': 0.89}
{'loss': 0.2338, 'learning_rate': 3.477157360406091e-05, 'epoch': 0.91}
{'loss': 0.1812, 'learning_rate': 3.44331641285956e-05, 'epoch': 0.93}
{'loss': 0.15, 'learning_rate': 3.409475465313029e-05, 'epoch': 0.95}
{'loss': 0.4898, 'learning_rate': 3.3756345177664975e-05, 'epoch': 0.97}
{'loss': 0.3291, 'learning_rate': 3.3417935702199664e-05, 'epoch': 0.99}
{'loss': 0.1456, 'learning_rate': 3.307952622673435e-05, 'epoch': 1.02}
{'loss': 0.1409, 'learning_rate': 3.2741116751269036e-05, 'epoch': 1.04}
{'loss': 0.175, 'learning_rate': 3.2402707275803726e-05, 'epoch': 1.06}
{'loss': 0.1087, 'learning_rate': 3.206429780033841e-05, 'epoch': 1.08}
{'loss': 0.0686, 'learning_rate': 3.17258883248731e-05, 'epoch': 1.1}
{'loss': 0.0744, 'learning_rate': 3.138747884940779e-05, 'epoch': 1.12}
{'loss': 0.1206, 'learning_rate': 3.104906937394248e-05, 'epoch': 1.14}
{'loss': 0.1291, 'learning_rate': 3.071065989847716e-05, 'epoch': 1.16}
{'loss': 0.1867, 'learning_rate': 3.0372250423011846e-05, 'epoch': 1.18}
{'loss': 0.1373, 'learning_rate': 3.0033840947546532e-05, 'epoch': 1.2}
{'loss': 0.105, 'learning_rate': 2.969543147208122e-05, 'epoch': 1.22}
{'loss': 0.0949, 'learning_rate': 2.9357021996615907e-05, 'epoch': 1.24}
{'loss': 0.1901, 'learning_rate': 2.9018612521150597e-05, 'epoch': 1.26}
{'loss': 0.1105, 'learning_rate': 2.8680203045685283e-05, 'epoch': 1.28}
{'loss': 0.2271, 'learning_rate': 2.8341793570219966e-05, 'epoch': 1.3}
{'loss': 0.0868, 'learning_rate': 2.800338409475465e-05, 'epoch': 1.32}
{'loss': 0.1047, 'learning_rate': 2.766497461928934e-05, 'epoch': 1.34}
{'loss': 0.1324, 'learning_rate': 2.7326565143824027e-05, 'epoch': 1.36}
{'loss': 0.0593, 'learning_rate': 2.6988155668358717e-05, 'epoch': 1.38}
{'loss': 0.1368, 'learning_rate': 2.6649746192893406e-05, 'epoch': 1.4}
{'loss': 0.1397, 'learning_rate': 2.6311336717428085e-05, 'epoch': 1.42}
{'loss': 0.1092, 'learning_rate': 2.5972927241962775e-05, 'epoch': 1.44}
{'loss': 0.1308, 'learning_rate': 2.563451776649746e-05, 'epoch': 1.46}
{'loss': 0.2259, 'learning_rate': 2.529610829103215e-05, 'epoch': 1.48}
{'loss': 0.167, 'learning_rate': 2.4957698815566837e-05, 'epoch': 1.5}
{'loss': 0.1996, 'learning_rate': 2.4619289340101523e-05, 'epoch': 1.52}
{'loss': 0.1608, 'learning_rate': 2.4280879864636212e-05, 'epoch': 1.54}
{'loss': 0.1, 'learning_rate': 2.3942470389170898e-05, 'epoch': 1.56}
{'loss': 0.0951, 'learning_rate': 2.3604060913705588e-05, 'epoch': 1.58}
{'loss': 0.0802, 'learning_rate': 2.326565143824027e-05, 'epoch': 1.6}
{'loss': 0.1552, 'learning_rate': 2.292724196277496e-05, 'epoch': 1.62}
{'loss': 0.1312, 'learning_rate': 2.2588832487309646e-05, 'epoch': 1.64}
{'loss': 0.0724, 'learning_rate': 2.2250423011844332e-05, 'epoch': 1.66}
{'loss': 0.0438, 'learning_rate': 2.1912013536379018e-05, 'epoch': 1.69}
{'loss': 0.0201, 'learning_rate': 2.1573604060913707e-05, 'epoch': 1.71}
{'loss': 0.204, 'learning_rate': 2.1235194585448394e-05, 'epoch': 1.73}
{'loss': 0.093, 'learning_rate': 2.089678510998308e-05, 'epoch': 1.75}
{'loss': 0.0716, 'learning_rate': 2.055837563451777e-05, 'epoch': 1.77}
{'loss': 0.0646, 'learning_rate': 2.0219966159052452e-05, 'epoch': 1.79}
{'loss': 0.1614, 'learning_rate': 1.988155668358714e-05, 'epoch': 1.81}
{'loss': 0.0249, 'learning_rate': 1.9543147208121827e-05, 'epoch': 1.83}
{'loss': 0.0806, 'learning_rate': 1.9204737732656517e-05, 'epoch': 1.85}
{'loss': 0.086, 'learning_rate': 1.8866328257191203e-05, 'epoch': 1.87}
{'loss': 0.0714, 'learning_rate': 1.852791878172589e-05, 'epoch': 1.89}
{'loss': 0.0981, 'learning_rate': 1.818950930626058e-05, 'epoch': 1.91}
{'loss': 0.1295, 'learning_rate': 1.785109983079526e-05, 'epoch': 1.93}
{'loss': 0.1614, 'learning_rate': 1.751269035532995e-05, 'epoch': 1.95}
{'loss': 0.1016, 'learning_rate': 1.7174280879864637e-05, 'epoch': 1.97}
{'loss': 0.0865, 'learning_rate': 1.6835871404399323e-05, 'epoch': 1.99}
{'loss': 0.1095, 'learning_rate': 1.649746192893401e-05, 'epoch': 2.01}
{'loss': 0.0492, 'learning_rate': 1.6159052453468698e-05, 'epoch': 2.03}
{'loss': 0.0475, 'learning_rate': 1.5820642978003384e-05, 'epoch': 2.05}
{'loss': 0.0613, 'learning_rate': 1.548223350253807e-05, 'epoch': 2.07}
{'loss': 0.0664, 'learning_rate': 1.5143824027072758e-05, 'epoch': 2.09}
{'loss': 0.0875, 'learning_rate': 1.4805414551607446e-05, 'epoch': 2.11}
{'loss': 0.0246, 'learning_rate': 1.4467005076142132e-05, 'epoch': 2.13}
{'loss': 0.0996, 'learning_rate': 1.412859560067682e-05, 'epoch': 2.15}
{'loss': 0.0482, 'learning_rate': 1.3790186125211508e-05, 'epoch': 2.17}
{'loss': 0.0481, 'learning_rate': 1.3451776649746192e-05, 'epoch': 2.19}
{'loss': 0.1182, 'learning_rate': 1.311336717428088e-05, 'epoch': 2.21}
{'loss': 0.0353, 'learning_rate': 1.2774957698815568e-05, 'epoch': 2.23}
{'loss': 0.0497, 'learning_rate': 1.2436548223350254e-05, 'epoch': 2.25}
{'loss': 0.0142, 'learning_rate': 1.2098138747884941e-05, 'epoch': 2.27}
{'loss': 0.0527, 'learning_rate': 1.1759729272419629e-05, 'epoch': 2.29}
{'loss': 0.0103, 'learning_rate': 1.1421319796954315e-05, 'epoch': 2.31}
{'loss': 0.1441, 'learning_rate': 1.1082910321489003e-05, 'epoch': 2.34}
{'loss': 0.0467, 'learning_rate': 1.0744500846023689e-05, 'epoch': 2.36}
{'loss': 0.064, 'learning_rate': 1.0406091370558377e-05, 'epoch': 2.38}
{'loss': 0.0665, 'learning_rate': 1.0067681895093063e-05, 'epoch': 2.4}
{'loss': 0.0837, 'learning_rate': 9.729272419627749e-06, 'epoch': 2.42}
{'loss': 0.0493, 'learning_rate': 9.390862944162437e-06, 'epoch': 2.44}
{'loss': 0.0401, 'learning_rate': 9.052453468697125e-06, 'epoch': 2.46}
{'loss': 0.0294, 'learning_rate': 8.714043993231812e-06, 'epoch': 2.48}
{'loss': 0.0114, 'learning_rate': 8.375634517766498e-06, 'epoch': 2.5}
{'loss': 0.0862, 'learning_rate': 8.037225042301184e-06, 'epoch': 2.52}
{'loss': 0.0038, 'learning_rate': 7.698815566835872e-06, 'epoch': 2.54}
{'loss': 0.0467, 'learning_rate': 7.360406091370558e-06, 'epoch': 2.56}
{'loss': 0.0206, 'learning_rate': 7.021996615905245e-06, 'epoch': 2.58}
{'loss': 0.0302, 'learning_rate': 6.683587140439933e-06, 'epoch': 2.6}
{'loss': 0.0431, 'learning_rate': 6.345177664974619e-06, 'epoch': 2.62}
{'loss': 0.0389, 'learning_rate': 6.006768189509306e-06, 'epoch': 2.64}
{'loss': 0.012, 'learning_rate': 5.668358714043994e-06, 'epoch': 2.66}
{'loss': 0.0474, 'learning_rate': 5.329949238578681e-06, 'epoch': 2.68}
{'loss': 0.0184, 'learning_rate': 4.991539763113368e-06, 'epoch': 2.7}
{'loss': 0.0439, 'learning_rate': 4.6531302876480546e-06, 'epoch': 2.72}
{'loss': 0.0392, 'learning_rate': 4.3147208121827415e-06, 'epoch': 2.74}
{'loss': 0.0772, 'learning_rate': 3.976311336717428e-06, 'epoch': 2.76}
{'loss': 0.0408, 'learning_rate': 3.6379018612521153e-06, 'epoch': 2.78}
{'loss': 0.0576, 'learning_rate': 3.2994923857868023e-06, 'epoch': 2.8}
{'loss': 0.0246, 'learning_rate': 2.961082910321489e-06, 'epoch': 2.82}
{'loss': 0.0632, 'learning_rate': 2.622673434856176e-06, 'epoch': 2.84}
{'loss': 0.0509, 'learning_rate': 2.284263959390863e-06, 'epoch': 2.86}
{'loss': 0.0689, 'learning_rate': 1.94585448392555e-06, 'epoch': 2.88}
{'loss': 0.0794, 'learning_rate': 1.607445008460237e-06, 'epoch': 2.9}
{'loss': 0.0448, 'learning_rate': 1.2690355329949238e-06, 'epoch': 2.92}
{'loss': 0.0643, 'learning_rate': 9.306260575296109e-07, 'epoch': 2.94}
{'loss': 0.0355, 'learning_rate': 5.922165820642979e-07, 'epoch': 2.96}
{'loss': 0.0315, 'learning_rate': 2.5380710659898475e-07, 'epoch': 2.98}
{'train_runtime': 763.8472, 'train_samples_per_second': 6.174, 'train_steps_per_second': 0.774, 'train_loss': 0.1679306387374975, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-591/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-197/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-394/optimizer.pt
validate!
last validate 0.
INFO 11-24 07:38:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-197', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-197', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:38:58 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_50_0.9_0.3_130_3 epoch 1

------------------------------------------------

0.694

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_50_0.9_0.3_130_3/generated_contents/1
INFO 11-24 07:39:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-394', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-394', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:39:40 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_50_0.9_0.3_130_3 epoch 2

------------------------------------------------

0.708

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_50_0.9_0.3_130_3/generated_contents/2
INFO 11-24 07:40:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-591', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-591', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:40:22 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_40_50_0.9_0.3_130_3 epoch 3

------------------------------------------------

0.796

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_40_50_0.9_0.3_130_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-197
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-394
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_40_50_0.9_0.3_130_3/checkpoint-591
searching parameters: NI_task738_30_30_50_0.8_0.3_130_3
/home/cyzhao/NI_task738_exp_1/NI_task738_30_30_50_0.8_0.3_130_3
/home/cyzhao/NI_task738_exp_1/NI_task738_30_30_50_0.8_0.3_130_3/config.json
generate_and_write_inputs!
INFO 11-24 07:40:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:41:11 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 07:52:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 07:52:40 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 715
expected_example_num: 900
selection_ratio: 0.7944444444444444
finetune_vicuna!
{'loss': 1.1965, 'learning_rate': 4.925925925925926e-05, 'epoch': 0.04}
{'loss': 0.9233, 'learning_rate': 4.851851851851852e-05, 'epoch': 0.09}
{'loss': 0.9596, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.13}
{'loss': 0.2396, 'learning_rate': 4.703703703703704e-05, 'epoch': 0.18}
{'loss': 0.4635, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.22}
{'loss': 0.1722, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.1492, 'learning_rate': 4.481481481481482e-05, 'epoch': 0.31}
{'loss': 0.1957, 'learning_rate': 4.4074074074074076e-05, 'epoch': 0.36}
{'loss': 0.3119, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.1362, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.1819, 'learning_rate': 4.185185185185185e-05, 'epoch': 0.49}
{'loss': 0.2096, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.1573, 'learning_rate': 4.0370370370370374e-05, 'epoch': 0.58}
{'loss': 0.2469, 'learning_rate': 3.962962962962963e-05, 'epoch': 0.62}
{'loss': 0.2647, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.0801, 'learning_rate': 3.814814814814815e-05, 'epoch': 0.71}
{'loss': 0.3643, 'learning_rate': 3.740740740740741e-05, 'epoch': 0.76}
{'loss': 0.0887, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.0647, 'learning_rate': 3.592592592592593e-05, 'epoch': 0.84}
{'loss': 0.1222, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.1746, 'learning_rate': 3.444444444444445e-05, 'epoch': 0.93}
{'loss': 0.1211, 'learning_rate': 3.3703703703703706e-05, 'epoch': 0.98}
{'loss': 0.0831, 'learning_rate': 3.2962962962962964e-05, 'epoch': 1.02}
{'loss': 0.2163, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.0472, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}
{'loss': 0.0487, 'learning_rate': 3.074074074074074e-05, 'epoch': 1.16}
{'loss': 0.1454, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.0355, 'learning_rate': 2.925925925925926e-05, 'epoch': 1.24}
{'loss': 0.2791, 'learning_rate': 2.851851851851852e-05, 'epoch': 1.29}
{'loss': 0.0578, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2587, 'learning_rate': 2.7037037037037037e-05, 'epoch': 1.38}
{'loss': 0.0726, 'learning_rate': 2.6296296296296296e-05, 'epoch': 1.42}
{'loss': 0.0925, 'learning_rate': 2.5555555555555554e-05, 'epoch': 1.47}
{'loss': 0.031, 'learning_rate': 2.4814814814814816e-05, 'epoch': 1.51}
{'loss': 0.1035, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.56}
{'loss': 0.033, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1228, 'learning_rate': 2.2592592592592594e-05, 'epoch': 1.64}
{'loss': 0.0604, 'learning_rate': 2.1851851851851852e-05, 'epoch': 1.69}
{'loss': 0.1268, 'learning_rate': 2.111111111111111e-05, 'epoch': 1.73}
{'loss': 0.0577, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1098, 'learning_rate': 1.962962962962963e-05, 'epoch': 1.82}
{'loss': 0.0827, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.0592, 'learning_rate': 1.814814814814815e-05, 'epoch': 1.91}
{'loss': 0.0901, 'learning_rate': 1.740740740740741e-05, 'epoch': 1.96}
{'loss': 0.0287, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0284, 'learning_rate': 1.5925925925925926e-05, 'epoch': 2.04}
{'loss': 0.0253, 'learning_rate': 1.5185185185185186e-05, 'epoch': 2.09}
{'loss': 0.0373, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0424, 'learning_rate': 1.3703703703703704e-05, 'epoch': 2.18}
{'loss': 0.0489, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0937, 'learning_rate': 1.2222222222222222e-05, 'epoch': 2.27}
{'loss': 0.0603, 'learning_rate': 1.1481481481481482e-05, 'epoch': 2.31}
{'loss': 0.009, 'learning_rate': 1.074074074074074e-05, 'epoch': 2.36}
{'loss': 0.0553, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0253, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.44}
{'loss': 0.0141, 'learning_rate': 8.518518518518519e-06, 'epoch': 2.49}
{'loss': 0.0032, 'learning_rate': 7.777777777777777e-06, 'epoch': 2.53}
{'loss': 0.0852, 'learning_rate': 7.0370370370370375e-06, 'epoch': 2.58}
{'loss': 0.0135, 'learning_rate': 6.296296296296296e-06, 'epoch': 2.62}
{'loss': 0.0864, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0866, 'learning_rate': 4.814814814814815e-06, 'epoch': 2.71}
{'loss': 0.0045, 'learning_rate': 4.074074074074075e-06, 'epoch': 2.76}
{'loss': 0.0145, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0526, 'learning_rate': 2.5925925925925925e-06, 'epoch': 2.84}
{'loss': 0.0314, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.89}
{'loss': 0.0438, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'loss': 0.0321, 'learning_rate': 3.703703703703704e-07, 'epoch': 2.98}
{'train_runtime': 492.9337, 'train_samples_per_second': 4.351, 'train_steps_per_second': 0.548, 'train_loss': 0.14799507759787417, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-270/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-180/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-90/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:04:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-90', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:04:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_30_30_50_0.8_0.3_130_3 epoch 1

------------------------------------------------

0.783

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_30_30_50_0.8_0.3_130_3/generated_contents/1
INFO 11-24 08:05:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-180', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-180', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:05:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_30_30_50_0.8_0.3_130_3 epoch 2

------------------------------------------------

0.866

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_30_30_50_0.8_0.3_130_3/generated_contents/2
INFO 11-24 08:05:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-270', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-270', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:06:08 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_30_30_50_0.8_0.3_130_3 epoch 3

------------------------------------------------

0.827

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_30_30_50_0.8_0.3_130_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task738_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-180 /data2/cyzhao/best_ckpt/NI_task738_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-90
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_50_0.8_0.3_130_3/checkpoint-270
searching parameters: NI_task738_10_40_50_1.0_0.35_115_3
/home/cyzhao/NI_task738_exp_1/NI_task738_10_40_50_1.0_0.35_115_3
/home/cyzhao/NI_task738_exp_1/NI_task738_10_40_50_1.0_0.35_115_3/config.json
generate_and_write_inputs!
INFO 11-24 08:06:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:06:54 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:10:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:10:21 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 311
expected_example_num: 400
selection_ratio: 0.7775
finetune_vicuna!
{'loss': 1.5721, 'learning_rate': 4.829059829059829e-05, 'epoch': 0.1}
{'loss': 0.6862, 'learning_rate': 4.6581196581196586e-05, 'epoch': 0.21}
{'loss': 0.4585, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.2501, 'learning_rate': 4.316239316239317e-05, 'epoch': 0.41}
{'loss': 0.3953, 'learning_rate': 4.145299145299146e-05, 'epoch': 0.51}
{'loss': 0.7396, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.4567, 'learning_rate': 3.8034188034188035e-05, 'epoch': 0.72}
{'loss': 0.3103, 'learning_rate': 3.6324786324786323e-05, 'epoch': 0.82}
{'loss': 0.3507, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.1885, 'learning_rate': 3.290598290598291e-05, 'epoch': 1.03}
{'loss': 0.0989, 'learning_rate': 3.1196581196581195e-05, 'epoch': 1.13}
{'loss': 0.0954, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.1742, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1399, 'learning_rate': 2.606837606837607e-05, 'epoch': 1.44}
{'loss': 0.0809, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.1773, 'learning_rate': 2.264957264957265e-05, 'epoch': 1.64}
{'loss': 0.0375, 'learning_rate': 2.0940170940170943e-05, 'epoch': 1.74}
{'loss': 0.1958, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1354, 'learning_rate': 1.752136752136752e-05, 'epoch': 1.95}
{'loss': 0.0862, 'learning_rate': 1.581196581196581e-05, 'epoch': 2.05}
{'loss': 0.0331, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.0647, 'learning_rate': 1.2393162393162394e-05, 'epoch': 2.26}
{'loss': 0.0744, 'learning_rate': 1.0683760683760684e-05, 'epoch': 2.36}
{'loss': 0.1562, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0769, 'learning_rate': 7.264957264957266e-06, 'epoch': 2.56}
{'loss': 0.0092, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.068, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'loss': 0.0216, 'learning_rate': 2.136752136752137e-06, 'epoch': 2.87}
{'loss': 0.0984, 'learning_rate': 4.273504273504274e-07, 'epoch': 2.97}
{'train_runtime': 272.2942, 'train_samples_per_second': 3.426, 'train_steps_per_second': 0.43, 'train_loss': 0.24728557468256634, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-117/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-78/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:16:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:16:50 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_40_50_1.0_0.35_115_3 epoch 1

------------------------------------------------

0.717

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_40_50_1.0_0.35_115_3/generated_contents/1
INFO 11-24 08:17:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-78', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:17:29 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_40_50_1.0_0.35_115_3 epoch 2

------------------------------------------------

0.852

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_40_50_1.0_0.35_115_3/generated_contents/2
INFO 11-24 08:17:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-117', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-117', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:18:09 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_40_50_1.0_0.35_115_3 epoch 3

------------------------------------------------

0.811

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_40_50_1.0_0.35_115_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-39
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-78
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_40_50_1.0_0.35_115_3/checkpoint-117
searching parameters: NI_task738_50_15_40_0.8_0.3_125_4
/home/cyzhao/NI_task738_exp_1/NI_task738_50_15_40_0.8_0.3_125_4
/home/cyzhao/NI_task738_exp_1/NI_task738_50_15_40_0.8_0.3_125_4/config.json
generate_and_write_inputs!
INFO 11-24 08:18:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:18:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:26:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:27:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 652
expected_example_num: 750
selection_ratio: 0.8693333333333333
finetune_vicuna!
{'loss': 0.8849, 'learning_rate': 4.9390243902439024e-05, 'epoch': 0.05}
{'loss': 0.4561, 'learning_rate': 4.878048780487805e-05, 'epoch': 0.1}
{'loss': 0.7856, 'learning_rate': 4.817073170731707e-05, 'epoch': 0.15}
{'loss': 1.3784, 'learning_rate': 4.75609756097561e-05, 'epoch': 0.2}
{'loss': 0.6794, 'learning_rate': 4.695121951219512e-05, 'epoch': 0.24}
{'loss': 0.5968, 'learning_rate': 4.634146341463415e-05, 'epoch': 0.29}
{'loss': 0.3, 'learning_rate': 4.573170731707318e-05, 'epoch': 0.34}
{'loss': 0.2207, 'learning_rate': 4.51219512195122e-05, 'epoch': 0.39}
{'loss': 0.8574, 'learning_rate': 4.451219512195122e-05, 'epoch': 0.44}
{'loss': 3.5815, 'learning_rate': 4.390243902439025e-05, 'epoch': 0.49}
{'loss': 0.2558, 'learning_rate': 4.329268292682927e-05, 'epoch': 0.54}
{'loss': 0.1187, 'learning_rate': 4.26829268292683e-05, 'epoch': 0.59}
{'loss': 0.4695, 'learning_rate': 4.207317073170732e-05, 'epoch': 0.63}
{'loss': 0.3895, 'learning_rate': 4.146341463414634e-05, 'epoch': 0.68}
{'loss': 0.1376, 'learning_rate': 4.085365853658537e-05, 'epoch': 0.73}
{'loss': 0.2598, 'learning_rate': 4.0243902439024395e-05, 'epoch': 0.78}
{'loss': 0.1621, 'learning_rate': 3.9634146341463416e-05, 'epoch': 0.83}
{'loss': 0.1489, 'learning_rate': 3.9024390243902444e-05, 'epoch': 0.88}
{'loss': 0.3435, 'learning_rate': 3.8414634146341465e-05, 'epoch': 0.93}
{'loss': 0.3958, 'learning_rate': 3.780487804878049e-05, 'epoch': 0.98}
{'loss': 0.1226, 'learning_rate': 3.7195121951219514e-05, 'epoch': 1.02}
{'loss': 0.1432, 'learning_rate': 3.6585365853658535e-05, 'epoch': 1.07}
{'loss': 0.1914, 'learning_rate': 3.597560975609756e-05, 'epoch': 1.12}
{'loss': 0.1238, 'learning_rate': 3.5365853658536584e-05, 'epoch': 1.17}
{'loss': 0.1585, 'learning_rate': 3.475609756097561e-05, 'epoch': 1.22}
{'loss': 0.0747, 'learning_rate': 3.414634146341464e-05, 'epoch': 1.27}
{'loss': 0.0698, 'learning_rate': 3.353658536585366e-05, 'epoch': 1.32}
{'loss': 0.0902, 'learning_rate': 3.292682926829269e-05, 'epoch': 1.37}
{'loss': 0.2452, 'learning_rate': 3.231707317073171e-05, 'epoch': 1.41}
{'loss': 0.1621, 'learning_rate': 3.170731707317073e-05, 'epoch': 1.46}
{'loss': 0.0208, 'learning_rate': 3.109756097560976e-05, 'epoch': 1.51}
{'loss': 0.1272, 'learning_rate': 3.048780487804878e-05, 'epoch': 1.56}
{'loss': 0.0348, 'learning_rate': 2.9878048780487805e-05, 'epoch': 1.61}
{'loss': 0.1273, 'learning_rate': 2.926829268292683e-05, 'epoch': 1.66}
{'loss': 0.1931, 'learning_rate': 2.8658536585365854e-05, 'epoch': 1.71}
{'loss': 0.0994, 'learning_rate': 2.8048780487804882e-05, 'epoch': 1.76}
{'loss': 0.1472, 'learning_rate': 2.7439024390243906e-05, 'epoch': 1.8}
{'loss': 0.1078, 'learning_rate': 2.682926829268293e-05, 'epoch': 1.85}
{'loss': 0.0497, 'learning_rate': 2.6219512195121952e-05, 'epoch': 1.9}
{'loss': 0.2019, 'learning_rate': 2.5609756097560977e-05, 'epoch': 1.95}
{'loss': 0.0713, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.1183, 'learning_rate': 2.4390243902439026e-05, 'epoch': 2.05}
{'loss': 0.0892, 'learning_rate': 2.378048780487805e-05, 'epoch': 2.1}
{'loss': 0.0541, 'learning_rate': 2.3170731707317075e-05, 'epoch': 2.15}
{'loss': 0.0488, 'learning_rate': 2.25609756097561e-05, 'epoch': 2.2}
{'loss': 0.0202, 'learning_rate': 2.1951219512195124e-05, 'epoch': 2.24}
{'loss': 0.0089, 'learning_rate': 2.134146341463415e-05, 'epoch': 2.29}
{'loss': 0.0558, 'learning_rate': 2.073170731707317e-05, 'epoch': 2.34}
{'loss': 0.0345, 'learning_rate': 2.0121951219512197e-05, 'epoch': 2.39}
{'loss': 0.0125, 'learning_rate': 1.9512195121951222e-05, 'epoch': 2.44}
{'loss': 0.0547, 'learning_rate': 1.8902439024390246e-05, 'epoch': 2.49}
{'loss': 0.0447, 'learning_rate': 1.8292682926829268e-05, 'epoch': 2.54}
{'loss': 0.0731, 'learning_rate': 1.7682926829268292e-05, 'epoch': 2.59}
{'loss': 0.0451, 'learning_rate': 1.707317073170732e-05, 'epoch': 2.63}
{'loss': 0.0538, 'learning_rate': 1.6463414634146345e-05, 'epoch': 2.68}
{'loss': 0.0036, 'learning_rate': 1.5853658536585366e-05, 'epoch': 2.73}
{'loss': 0.0278, 'learning_rate': 1.524390243902439e-05, 'epoch': 2.78}
{'loss': 0.0845, 'learning_rate': 1.4634146341463415e-05, 'epoch': 2.83}
{'loss': 0.0068, 'learning_rate': 1.4024390243902441e-05, 'epoch': 2.88}
{'loss': 0.0406, 'learning_rate': 1.3414634146341466e-05, 'epoch': 2.93}
{'loss': 0.0559, 'learning_rate': 1.2804878048780488e-05, 'epoch': 2.98}
{'loss': 0.0774, 'learning_rate': 1.2195121951219513e-05, 'epoch': 3.02}
{'loss': 0.0269, 'learning_rate': 1.1585365853658537e-05, 'epoch': 3.07}
{'loss': 0.002, 'learning_rate': 1.0975609756097562e-05, 'epoch': 3.12}
{'loss': 0.0164, 'learning_rate': 1.0365853658536585e-05, 'epoch': 3.17}
{'loss': 0.0095, 'learning_rate': 9.756097560975611e-06, 'epoch': 3.22}
{'loss': 0.0024, 'learning_rate': 9.146341463414634e-06, 'epoch': 3.27}
{'loss': 0.0165, 'learning_rate': 8.53658536585366e-06, 'epoch': 3.32}
{'loss': 0.016, 'learning_rate': 7.926829268292683e-06, 'epoch': 3.37}
{'loss': 0.0629, 'learning_rate': 7.317073170731707e-06, 'epoch': 3.41}
{'loss': 0.001, 'learning_rate': 6.707317073170733e-06, 'epoch': 3.46}
{'loss': 0.0015, 'learning_rate': 6.0975609756097564e-06, 'epoch': 3.51}
{'loss': 0.0097, 'learning_rate': 5.487804878048781e-06, 'epoch': 3.56}
{'loss': 0.0061, 'learning_rate': 4.8780487804878055e-06, 'epoch': 3.61}
{'loss': 0.0025, 'learning_rate': 4.26829268292683e-06, 'epoch': 3.66}
{'loss': 0.0302, 'learning_rate': 3.6585365853658537e-06, 'epoch': 3.71}
{'loss': 0.0153, 'learning_rate': 3.0487804878048782e-06, 'epoch': 3.76}
{'loss': 0.0101, 'learning_rate': 2.4390243902439027e-06, 'epoch': 3.8}
{'loss': 0.0046, 'learning_rate': 1.8292682926829268e-06, 'epoch': 3.85}
{'loss': 0.0121, 'learning_rate': 1.2195121951219514e-06, 'epoch': 3.9}
{'loss': 0.0436, 'learning_rate': 6.097560975609757e-07, 'epoch': 3.95}
{'loss': 0.0396, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 502.7802, 'train_samples_per_second': 5.187, 'train_steps_per_second': 0.652, 'train_loss': 0.19905819201404684, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-328/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-82/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-246/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-164/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:37:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-82', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-82', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:38:07 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_15_40_0.8_0.3_125_4 epoch 1

------------------------------------------------

0.815

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_15_40_0.8_0.3_125_4/generated_contents/1
INFO 11-24 08:38:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-164', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-164', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:38:48 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_15_40_0.8_0.3_125_4 epoch 2

------------------------------------------------

0.738

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_15_40_0.8_0.3_125_4/generated_contents/2
INFO 11-24 08:39:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-246', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-246', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:39:28 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_15_40_0.8_0.3_125_4 epoch 3

------------------------------------------------

0.73

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_15_40_0.8_0.3_125_4/generated_contents/3
INFO 11-24 08:39:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-328', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-328', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:40:09 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_50_15_40_0.8_0.3_125_4 epoch 4

------------------------------------------------

0.739

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_50_15_40_0.8_0.3_125_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-82
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-164
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-246
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_50_15_40_0.8_0.3_125_4/checkpoint-328
searching parameters: NI_task738_10_30_45_0.8_0.35_125_4
/home/cyzhao/NI_task738_exp_1/NI_task738_10_30_45_0.8_0.35_125_4
/home/cyzhao/NI_task738_exp_1/NI_task738_10_30_45_0.8_0.35_125_4/config.json
generate_and_write_inputs!
INFO 11-24 08:40:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:40:56 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:44:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:44:53 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 235
expected_example_num: 300
selection_ratio: 0.7833333333333333
finetune_vicuna!
{'loss': 1.2691, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.13}
{'loss': 1.2187, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.27}
{'loss': 1.176, 'learning_rate': 4.5e-05, 'epoch': 0.4}
{'loss': 0.2342, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.53}
{'loss': 0.3161, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.67}
{'loss': 0.5106, 'learning_rate': 4e-05, 'epoch': 0.8}
{'loss': 0.1198, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.93}
{'loss': 0.3602, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.07}
{'loss': 0.149, 'learning_rate': 3.5e-05, 'epoch': 1.2}
{'loss': 0.11, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.33}
{'loss': 0.0495, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.47}
{'loss': 0.1249, 'learning_rate': 3e-05, 'epoch': 1.6}
{'loss': 0.1645, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.73}
{'loss': 0.2181, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.87}
{'loss': 0.0981, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.0499, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.13}
{'loss': 0.0392, 'learning_rate': 2.1666666666666667e-05, 'epoch': 2.27}
{'loss': 0.0845, 'learning_rate': 2e-05, 'epoch': 2.4}
{'loss': 0.0351, 'learning_rate': 1.8333333333333333e-05, 'epoch': 2.53}
{'loss': 0.0437, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.67}
{'loss': 0.0665, 'learning_rate': 1.5e-05, 'epoch': 2.8}
{'loss': 0.0555, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.93}
{'loss': 0.0109, 'learning_rate': 1.1666666666666668e-05, 'epoch': 3.07}
{'loss': 0.0248, 'learning_rate': 1e-05, 'epoch': 3.2}
{'loss': 0.0579, 'learning_rate': 8.333333333333334e-06, 'epoch': 3.33}
{'loss': 0.0386, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.47}
{'loss': 0.0248, 'learning_rate': 5e-06, 'epoch': 3.6}
{'loss': 0.0204, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.73}
{'loss': 0.0119, 'learning_rate': 1.6666666666666667e-06, 'epoch': 3.87}
{'loss': 0.0056, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 364.1227, 'train_samples_per_second': 2.582, 'train_steps_per_second': 0.33, 'train_loss': 0.2229449264705181, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-120/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-60/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-90/optimizer.pt
validate!
last validate 0.
INFO 11-24 08:53:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:53:17 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_30_45_0.8_0.35_125_4 epoch 1

------------------------------------------------

0.548

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_30_45_0.8_0.35_125_4/generated_contents/1
INFO 11-24 08:53:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-60', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-60', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:53:57 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_30_45_0.8_0.35_125_4 epoch 2

------------------------------------------------

0.815

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_30_45_0.8_0.35_125_4/generated_contents/2
INFO 11-24 08:54:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-90', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:54:38 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_30_45_0.8_0.35_125_4 epoch 3

------------------------------------------------

0.809

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_30_45_0.8_0.35_125_4/generated_contents/3
INFO 11-24 08:55:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-120', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-120', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:55:20 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_30_45_0.8_0.35_125_4 epoch 4

------------------------------------------------

0.81

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_30_45_0.8_0.35_125_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-60
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-90
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_30_45_0.8_0.35_125_4/checkpoint-120
searching parameters: NI_task738_10_15_50_1.0_0.5_130_4
/home/cyzhao/NI_task738_exp_1/NI_task738_10_15_50_1.0_0.5_130_4
/home/cyzhao/NI_task738_exp_1/NI_task738_10_15_50_1.0_0.5_130_4/config.json
generate_and_write_inputs!
INFO 11-24 08:55:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:56:05 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 08:58:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 08:58:39 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 111
expected_example_num: 150
selection_ratio: 0.74
finetune_vicuna!
{'loss': 1.9983, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.29}
{'loss': 0.9397, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.57}
{'loss': 0.853, 'learning_rate': 3.928571428571429e-05, 'epoch': 0.86}
{'loss': 0.3845, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.14}
{'loss': 0.2129, 'learning_rate': 3.2142857142857144e-05, 'epoch': 1.43}
{'loss': 0.2045, 'learning_rate': 2.857142857142857e-05, 'epoch': 1.71}
{'loss': 0.3185, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.1522, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.29}
{'loss': 0.1442, 'learning_rate': 1.785714285714286e-05, 'epoch': 2.57}
{'loss': 0.2085, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.86}
{'loss': 0.0503, 'learning_rate': 1.0714285714285714e-05, 'epoch': 3.14}
{'loss': 0.0596, 'learning_rate': 7.142857142857143e-06, 'epoch': 3.43}
{'loss': 0.0779, 'learning_rate': 3.5714285714285714e-06, 'epoch': 3.71}
{'loss': 0.1348, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 274.2526, 'train_samples_per_second': 1.619, 'train_steps_per_second': 0.204, 'train_loss': 0.40992580114730764, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-56/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-28/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-14/optimizer.pt
validate!
last validate 0.
INFO 11-24 09:04:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:04:45 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_15_50_1.0_0.5_130_4 epoch 1

------------------------------------------------

0.548

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_15_50_1.0_0.5_130_4/generated_contents/1
INFO 11-24 09:05:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-28', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-28', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:05:26 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_15_50_1.0_0.5_130_4 epoch 2

------------------------------------------------

0.824

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_15_50_1.0_0.5_130_4/generated_contents/2
INFO 11-24 09:05:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:06:06 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_15_50_1.0_0.5_130_4 epoch 3

------------------------------------------------

0.782

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_15_50_1.0_0.5_130_4/generated_contents/3
INFO 11-24 09:06:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-56', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-56', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:06:47 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_10_15_50_1.0_0.5_130_4 epoch 4

------------------------------------------------

0.816

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_10_15_50_1.0_0.5_130_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-28
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_10_15_50_1.0_0.5_130_4/checkpoint-56
searching parameters: NI_task738_30_30_40_0.8_0.35_125_4
/home/cyzhao/NI_task738_exp_1/NI_task738_30_30_40_0.8_0.35_125_4
/home/cyzhao/NI_task738_exp_1/NI_task738_30_30_40_0.8_0.35_125_4/config.json
generate_and_write_inputs!
INFO 11-24 09:07:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:07:33 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 09:16:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:16:55 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024
generated_example_num: 751
expected_example_num: 900
selection_ratio: 0.8344444444444444
finetune_vicuna!
{'loss': 1.8692, 'learning_rate': 4.946808510638298e-05, 'epoch': 0.04}
{'loss': 0.5196, 'learning_rate': 4.893617021276596e-05, 'epoch': 0.09}
{'loss': 0.5739, 'learning_rate': 4.840425531914894e-05, 'epoch': 0.13}
{'loss': 0.6735, 'learning_rate': 4.787234042553192e-05, 'epoch': 0.17}
{'loss': 0.3546, 'learning_rate': 4.734042553191489e-05, 'epoch': 0.21}
{'loss': 0.206, 'learning_rate': 4.680851063829788e-05, 'epoch': 0.26}
{'loss': 0.4836, 'learning_rate': 4.627659574468085e-05, 'epoch': 0.3}
{'loss': 0.2776, 'learning_rate': 4.574468085106383e-05, 'epoch': 0.34}
{'loss': 0.2059, 'learning_rate': 4.5212765957446815e-05, 'epoch': 0.38}
{'loss': 0.1098, 'learning_rate': 4.468085106382979e-05, 'epoch': 0.43}
{'loss': 0.329, 'learning_rate': 4.414893617021277e-05, 'epoch': 0.47}
{'loss': 0.6063, 'learning_rate': 4.3617021276595746e-05, 'epoch': 0.51}
{'loss': 0.2339, 'learning_rate': 4.3085106382978725e-05, 'epoch': 0.55}
{'loss': 0.1719, 'learning_rate': 4.2553191489361704e-05, 'epoch': 0.6}
{'loss': 0.2849, 'learning_rate': 4.2021276595744684e-05, 'epoch': 0.64}
{'loss': 0.1663, 'learning_rate': 4.148936170212766e-05, 'epoch': 0.68}
{'loss': 0.0657, 'learning_rate': 4.095744680851064e-05, 'epoch': 0.72}
{'loss': 0.2456, 'learning_rate': 4.0425531914893614e-05, 'epoch': 0.77}
{'loss': 0.5672, 'learning_rate': 3.9893617021276594e-05, 'epoch': 0.81}
{'loss': 0.2615, 'learning_rate': 3.936170212765958e-05, 'epoch': 0.85}
{'loss': 0.1949, 'learning_rate': 3.882978723404255e-05, 'epoch': 0.89}
{'loss': 0.1752, 'learning_rate': 3.829787234042553e-05, 'epoch': 0.94}
{'loss': 0.3608, 'learning_rate': 3.776595744680852e-05, 'epoch': 0.98}
{'loss': 0.2412, 'learning_rate': 3.723404255319149e-05, 'epoch': 1.02}
{'loss': 0.0481, 'learning_rate': 3.670212765957447e-05, 'epoch': 1.06}
{'loss': 0.178, 'learning_rate': 3.617021276595745e-05, 'epoch': 1.11}
{'loss': 0.0769, 'learning_rate': 3.563829787234043e-05, 'epoch': 1.15}
{'loss': 0.3005, 'learning_rate': 3.5106382978723407e-05, 'epoch': 1.19}
{'loss': 0.2045, 'learning_rate': 3.4574468085106386e-05, 'epoch': 1.23}
{'loss': 0.1931, 'learning_rate': 3.4042553191489365e-05, 'epoch': 1.28}
{'loss': 0.1744, 'learning_rate': 3.3510638297872344e-05, 'epoch': 1.32}
{'loss': 0.1359, 'learning_rate': 3.2978723404255317e-05, 'epoch': 1.36}
{'loss': 0.1853, 'learning_rate': 3.2446808510638296e-05, 'epoch': 1.4}
{'loss': 0.3426, 'learning_rate': 3.191489361702128e-05, 'epoch': 1.45}
{'loss': 0.1437, 'learning_rate': 3.1382978723404254e-05, 'epoch': 1.49}
{'loss': 0.1536, 'learning_rate': 3.085106382978723e-05, 'epoch': 1.53}
{'loss': 0.2527, 'learning_rate': 3.0319148936170216e-05, 'epoch': 1.57}
{'loss': 0.1687, 'learning_rate': 2.9787234042553192e-05, 'epoch': 1.62}
{'loss': 0.0385, 'learning_rate': 2.925531914893617e-05, 'epoch': 1.66}
{'loss': 0.0565, 'learning_rate': 2.8723404255319154e-05, 'epoch': 1.7}
{'loss': 0.1672, 'learning_rate': 2.819148936170213e-05, 'epoch': 1.74}
{'loss': 0.1218, 'learning_rate': 2.765957446808511e-05, 'epoch': 1.79}
{'loss': 0.1528, 'learning_rate': 2.7127659574468084e-05, 'epoch': 1.83}
{'loss': 0.0763, 'learning_rate': 2.6595744680851064e-05, 'epoch': 1.87}
{'loss': 0.3457, 'learning_rate': 2.6063829787234046e-05, 'epoch': 1.91}
{'loss': 0.1175, 'learning_rate': 2.5531914893617022e-05, 'epoch': 1.96}
{'loss': 0.2901, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'loss': 0.0364, 'learning_rate': 2.446808510638298e-05, 'epoch': 2.04}
{'loss': 0.0556, 'learning_rate': 2.393617021276596e-05, 'epoch': 2.09}
{'loss': 0.0541, 'learning_rate': 2.340425531914894e-05, 'epoch': 2.13}
{'loss': 0.1282, 'learning_rate': 2.2872340425531915e-05, 'epoch': 2.17}
{'loss': 0.0613, 'learning_rate': 2.2340425531914894e-05, 'epoch': 2.21}
{'loss': 0.1966, 'learning_rate': 2.1808510638297873e-05, 'epoch': 2.26}
{'loss': 0.1216, 'learning_rate': 2.1276595744680852e-05, 'epoch': 2.3}
{'loss': 0.1022, 'learning_rate': 2.074468085106383e-05, 'epoch': 2.34}
{'loss': 0.1093, 'learning_rate': 2.0212765957446807e-05, 'epoch': 2.38}
{'loss': 0.0327, 'learning_rate': 1.968085106382979e-05, 'epoch': 2.43}
{'loss': 0.049, 'learning_rate': 1.9148936170212766e-05, 'epoch': 2.47}
{'loss': 0.025, 'learning_rate': 1.8617021276595745e-05, 'epoch': 2.51}
{'loss': 0.098, 'learning_rate': 1.8085106382978724e-05, 'epoch': 2.55}
{'loss': 0.093, 'learning_rate': 1.7553191489361703e-05, 'epoch': 2.6}
{'loss': 0.0899, 'learning_rate': 1.7021276595744682e-05, 'epoch': 2.64}
{'loss': 0.0218, 'learning_rate': 1.6489361702127658e-05, 'epoch': 2.68}
{'loss': 0.1243, 'learning_rate': 1.595744680851064e-05, 'epoch': 2.72}
{'loss': 0.0368, 'learning_rate': 1.5425531914893617e-05, 'epoch': 2.77}
{'loss': 0.0402, 'learning_rate': 1.4893617021276596e-05, 'epoch': 2.81}
{'loss': 0.0537, 'learning_rate': 1.4361702127659577e-05, 'epoch': 2.85}
{'loss': 0.0679, 'learning_rate': 1.3829787234042554e-05, 'epoch': 2.89}
{'loss': 0.0577, 'learning_rate': 1.3297872340425532e-05, 'epoch': 2.94}
{'loss': 0.059, 'learning_rate': 1.2765957446808511e-05, 'epoch': 2.98}
{'loss': 0.0716, 'learning_rate': 1.223404255319149e-05, 'epoch': 3.02}
{'loss': 0.0429, 'learning_rate': 1.170212765957447e-05, 'epoch': 3.06}
{'loss': 0.0547, 'learning_rate': 1.1170212765957447e-05, 'epoch': 3.11}
{'loss': 0.0295, 'learning_rate': 1.0638297872340426e-05, 'epoch': 3.15}
{'loss': 0.0432, 'learning_rate': 1.0106382978723404e-05, 'epoch': 3.19}
{'loss': 0.037, 'learning_rate': 9.574468085106383e-06, 'epoch': 3.23}
{'loss': 0.0074, 'learning_rate': 9.042553191489362e-06, 'epoch': 3.28}
{'loss': 0.0438, 'learning_rate': 8.510638297872341e-06, 'epoch': 3.32}
{'loss': 0.0395, 'learning_rate': 7.97872340425532e-06, 'epoch': 3.36}
{'loss': 0.0693, 'learning_rate': 7.446808510638298e-06, 'epoch': 3.4}
{'loss': 0.0494, 'learning_rate': 6.914893617021277e-06, 'epoch': 3.45}
{'loss': 0.0228, 'learning_rate': 6.3829787234042555e-06, 'epoch': 3.49}
{'loss': 0.0103, 'learning_rate': 5.851063829787235e-06, 'epoch': 3.53}
{'loss': 0.0233, 'learning_rate': 5.319148936170213e-06, 'epoch': 3.57}
{'loss': 0.0206, 'learning_rate': 4.787234042553191e-06, 'epoch': 3.62}
{'loss': 0.0197, 'learning_rate': 4.255319148936171e-06, 'epoch': 3.66}
{'loss': 0.0262, 'learning_rate': 3.723404255319149e-06, 'epoch': 3.7}
{'loss': 0.0269, 'learning_rate': 3.1914893617021277e-06, 'epoch': 3.74}
{'loss': 0.0123, 'learning_rate': 2.6595744680851065e-06, 'epoch': 3.79}
{'loss': 0.04, 'learning_rate': 2.1276595744680853e-06, 'epoch': 3.83}
{'loss': 0.0072, 'learning_rate': 1.5957446808510639e-06, 'epoch': 3.87}
{'loss': 0.0476, 'learning_rate': 1.0638297872340427e-06, 'epoch': 3.91}
{'loss': 0.0441, 'learning_rate': 5.319148936170213e-07, 'epoch': 3.96}
{'loss': 0.0541, 'learning_rate': 0.0, 'epoch': 4.0}
{'train_runtime': 606.2398, 'train_samples_per_second': 4.955, 'train_steps_per_second': 0.62, 'train_loss': 0.16659520751241833, 'epoch': 4.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-94/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-188/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-282/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-376/optimizer.pt
validate!
last validate 0.
INFO 11-24 09:30:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-94', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-94', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:30:30 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_30_30_40_0.8_0.35_125_4 epoch 1

------------------------------------------------

0.534

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_30_30_40_0.8_0.35_125_4/generated_contents/1
INFO 11-24 09:30:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-188', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-188', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:31:12 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_30_30_40_0.8_0.35_125_4 epoch 2

------------------------------------------------

0.848

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_30_30_40_0.8_0.35_125_4/generated_contents/2
INFO 11-24 09:31:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-282', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-282', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:31:52 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_30_30_40_0.8_0.35_125_4 epoch 3

------------------------------------------------

0.793

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_30_30_40_0.8_0.35_125_4/generated_contents/3
INFO 11-24 09:32:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-376', tokenizer='/data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-376', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:32:31 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of NI_task738_30_30_40_0.8_0.35_125_4 epoch 4

------------------------------------------------

0.809

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/NI_task738_30_30_40_0.8_0.35_125_4/generated_contents/4
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-94
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-188
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-282
rm -rf /data2/cyzhao/ckpt_data_p2ms/NI_task738_30_30_40_0.8_0.35_125_4/checkpoint-376
{'generation_epochs': 30, 'generation_batch_size': 30, 'generation_top_k': 50, 'generation_temperature': 0.8, 'min_frequency': 0.3, 'min_input_length': 130, 'training_epochs': 3}
test best ckpt.
INFO 11-24 09:33:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task738_exp_1', tokenizer='/data2/cyzhao/best_ckpt/NI_task738_exp_1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 09:33:16 llm_engine.py:207] # GPU blocks: 14474, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task738_exp_1

------------------------------------------------

0.819

------------------------------------------------


The best ckpt on test set gain 0.819
Genrated contents are stored in /home/cyzhao/NI_task738_exp_1/best_ckpt_generated_content
