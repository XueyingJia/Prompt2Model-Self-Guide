[2023-11-29 05:39:58,069] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1557
searching parameters: task1557_20_20_40_0.4_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_20_40_0.4_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_20_40_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 05:40:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:40:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 05:41:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:41:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 33
expected_example_num: 400
selection_ratio: 0.0825
finetune_deepseek!
{'loss': 0.637, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.0561, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1189, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0091, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 170.9919, 'train_samples_per_second': 0.579, 'train_steps_per_second': 0.105, 'train_loss': 0.19772244431078434, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 05:45:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:45:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_40_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.858652731962182

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_40_0.4_0.3_3_1/generated_contents/1
INFO 11-29 05:45:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:46:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_40_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.8689075351250235

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_40_0.4_0.3_3_1/generated_contents/2
INFO 11-29 05:46:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:46:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_40_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.8673965828104685

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_40_0.4_0.3_3_1/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task1557_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_40_0.4_0.3_3_1/checkpoint-18
searching parameters: task1557_20_15_50_0.5_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.5_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.5_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 05:47:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:47:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 05:48:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:48:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 26
expected_example_num: 300
selection_ratio: 0.08666666666666667
finetune_deepseek!
{'loss': 0.7723, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3138, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.0707, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 169.5861, 'train_samples_per_second': 0.46, 'train_steps_per_second': 0.088, 'train_loss': 0.3262942691644033, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-29 05:52:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:52:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_50_0.5_0.3_3_1 epoch 1

------------------------------------------------

0.2452338836321525

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.5_0.3_3_1/generated_contents/1
INFO 11-29 05:53:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:53:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_50_0.5_0.3_3_1 epoch 2

------------------------------------------------

0.8835197259115942

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.5_0.3_3_1/generated_contents/2
INFO 11-29 05:54:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:54:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_50_0.5_0.3_3_1 epoch 3

------------------------------------------------

0.9001910366981235

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.5_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1557_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-15 /data2/cyzhao/best_ckpt/NI_task1557_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.5_0.3_3_1/checkpoint-10
searching parameters: task1557_20_20_50_0.4_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.4_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 05:54:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:55:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 05:56:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 05:56:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 33
expected_example_num: 400
selection_ratio: 0.0825
finetune_deepseek!
{'loss': 0.637, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.0561, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1189, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0091, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 169.6487, 'train_samples_per_second': 0.584, 'train_steps_per_second': 0.106, 'train_loss': 0.19772244431078434, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:00:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:00:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_50_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.858652731962182

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.4_0.3_3_1/generated_contents/1
INFO 11-29 06:01:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:01:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_50_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.8689075351250235

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.4_0.3_3_1/generated_contents/2
INFO 11-29 06:01:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:01:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_50_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.8673965828104685

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.4_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.4_0.3_3_1/checkpoint-18
searching parameters: task1557_30_20_40_0.4_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_20_40_0.4_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_20_40_0.4_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:02:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:02:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:04:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:04:27 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 45
expected_example_num: 600
selection_ratio: 0.075
finetune_deepseek!
{'loss': 0.3665, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.1937, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0427, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.094, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0151, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.0133, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 171.2904, 'train_samples_per_second': 0.788, 'train_steps_per_second': 0.14, 'train_loss': 0.12087391223758459, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-16/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:08:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:08:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_40_0.4_0.35_3_1 epoch 1

------------------------------------------------

0.6354397511517713

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_40_0.4_0.35_3_1/generated_contents/1
INFO 11-29 06:08:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:08:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_40_0.4_0.35_3_1 epoch 2

------------------------------------------------

0.7047475991419171

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_40_0.4_0.35_3_1/generated_contents/2
INFO 11-29 06:09:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:09:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_40_0.4_0.35_3_1 epoch 3

------------------------------------------------

0.720724496524434

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_40_0.4_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_40_0.4_0.35_3_1/checkpoint-24
searching parameters: task1557_20_10_40_0.5_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_40_0.5_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_40_0.5_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:09:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:09:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:10:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:11:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 22
expected_example_num: 200
selection_ratio: 0.11
finetune_deepseek!
{'loss': 0.1718, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.7742, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0998, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 182.0543, 'train_samples_per_second': 0.363, 'train_steps_per_second': 0.066, 'train_loss': 0.348575954635938, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:14:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:15:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_40_0.5_0.3_3_1 epoch 1

------------------------------------------------

0.544967710861157

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_40_0.5_0.3_3_1/generated_contents/1
INFO 11-29 06:15:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:15:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_40_0.5_0.3_3_1 epoch 2

------------------------------------------------

0.6035997259095717

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_40_0.5_0.3_3_1/generated_contents/2
INFO 11-29 06:16:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:16:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_40_0.5_0.3_3_1 epoch 3

------------------------------------------------

0.6416801173838312

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_40_0.5_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_40_0.5_0.3_3_1/checkpoint-12
searching parameters: task1557_20_15_45_0.7_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.7_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:17:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:17:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:19:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:19:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 41
expected_example_num: 300
selection_ratio: 0.13666666666666666
finetune_deepseek!
{'loss': 0.3261, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.2314, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0514, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.037, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0317, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 186.3488, 'train_samples_per_second': 0.66, 'train_steps_per_second': 0.113, 'train_loss': 0.13144152079309737, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:23:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:23:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_45_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.9013415963945716

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.7_0.3_3_1/generated_contents/1
INFO 11-29 06:23:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:23:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_45_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.901217125842678

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.7_0.3_3_1/generated_contents/2
INFO 11-29 06:24:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:24:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_45_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.9010382843524081

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1557_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-7 /data2/cyzhao/best_ckpt/NI_task1557_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.7_0.3_3_1/checkpoint-21
searching parameters: task1557_30_20_50_0.6_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_20_50_0.6_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_20_50_0.6_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:24:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:25:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:27:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:27:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 54
expected_example_num: 600
selection_ratio: 0.09
finetune_deepseek!
{'loss': 0.4146, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 1.0984, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.0942, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0631, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.0281, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0243, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 199.5243, 'train_samples_per_second': 0.812, 'train_steps_per_second': 0.135, 'train_loss': 0.25717505688468617, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:31:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:31:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_50_0.6_0.3_3_1 epoch 1

------------------------------------------------

0.897130749285651

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_50_0.6_0.3_3_1/generated_contents/1
INFO 11-29 06:32:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:32:30 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_50_0.6_0.3_3_1 epoch 2

------------------------------------------------

0.5204205471430441

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_50_0.6_0.3_3_1/generated_contents/2
INFO 11-29 06:32:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:33:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_50_0.6_0.3_3_1 epoch 3

------------------------------------------------

0.7475759460297458

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_50_0.6_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_50_0.6_0.3_3_1/checkpoint-27
searching parameters: task1557_20_20_50_0.5_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.5_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:33:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:33:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:35:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:35:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 34
expected_example_num: 400
selection_ratio: 0.085
finetune_deepseek!
{'loss': 0.3784, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1263, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0356, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0146, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 194.8056, 'train_samples_per_second': 0.524, 'train_steps_per_second': 0.092, 'train_loss': 0.1277682539075613, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:39:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:40:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_50_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.5976539185126951

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.5_0.35_3_1/generated_contents/1
INFO 11-29 06:40:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:40:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_50_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.6518573414226815

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.5_0.35_3_1/generated_contents/2
INFO 11-29 06:40:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:41:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_20_50_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.5396930271034933

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_20_50_0.5_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_20_50_0.5_0.35_3_1/checkpoint-18
searching parameters: task1557_30_15_50_0.5_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_15_50_0.5_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_15_50_0.5_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:41:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:41:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:43:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:44:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 33
expected_example_num: 450
selection_ratio: 0.07333333333333333
finetune_deepseek!
{'loss': 0.3611, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.1137, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0902, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0219, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 173.9912, 'train_samples_per_second': 0.569, 'train_steps_per_second': 0.103, 'train_loss': 0.13075050939288405, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:47:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:47:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_15_50_0.5_0.35_3_1 epoch 1

------------------------------------------------

0.8205255781121084

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_15_50_0.5_0.35_3_1/generated_contents/1
INFO 11-29 06:48:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:48:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_15_50_0.5_0.35_3_1 epoch 2

------------------------------------------------

0.8677117539212551

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_15_50_0.5_0.35_3_1/generated_contents/2
INFO 11-29 06:49:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:49:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_15_50_0.5_0.35_3_1 epoch 3

------------------------------------------------

0.8758439596746528

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_15_50_0.5_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_15_50_0.5_0.35_3_1/checkpoint-18
searching parameters: task1557_30_20_45_0.6_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_20_45_0.6_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_30_20_45_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:49:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:49:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:52:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:52:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 54
expected_example_num: 600
selection_ratio: 0.09
finetune_deepseek!
{'loss': 0.4146, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 1.0984, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.0942, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0631, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.0281, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0243, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 190.557, 'train_samples_per_second': 0.85, 'train_steps_per_second': 0.142, 'train_loss': 0.25717505688468617, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 06:56:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:56:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_45_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.897130749285651

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_45_0.6_0.4_3_1/generated_contents/1
INFO 11-29 06:57:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:57:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_45_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.5204205471430441

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_45_0.6_0.4_3_1/generated_contents/2
INFO 11-29 06:57:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:57:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_30_20_45_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.7475759460297458

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_30_20_45_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_30_20_45_0.6_0.4_3_1/checkpoint-27
searching parameters: task1557_10_15_45_0.7_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 06:58:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:58:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 06:59:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 06:59:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 21
expected_example_num: 150
selection_ratio: 0.14
finetune_deepseek!
{'loss': 0.276, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0962, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0571, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 178.4529, 'train_samples_per_second': 0.353, 'train_steps_per_second': 0.067, 'train_loss': 0.14313594748576483, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:03:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:03:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_15_45_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.845177037564058

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.4_3_1/generated_contents/1
INFO 11-29 07:03:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:04:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_15_45_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.8499120876317837

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.4_3_1/generated_contents/2
INFO 11-29 07:04:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:04:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_15_45_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.8621017963365347

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.4_3_1/checkpoint-12
searching parameters: task1557_20_15_45_0.8_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.8_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:05:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:05:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:06:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:07:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 62
expected_example_num: 300
selection_ratio: 0.20666666666666667
finetune_deepseek!
{'loss': 0.4942, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.1624, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.1162, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.0648, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.0819, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.0422, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.007, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0104, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 201.3728, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.164, 'train_loss': 0.11868671237342907, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:11:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:11:32 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_45_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.8584573208817579

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.8_0.3_3_1/generated_contents/1
INFO 11-29 07:11:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:12:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_45_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.7069321315279885

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.8_0.3_3_1/generated_contents/2
INFO 11-29 07:12:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:13:02 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_45_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.5914213261317308

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_45_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_45_0.8_0.3_3_1/checkpoint-33
searching parameters: task1557_10_15_45_0.7_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:13:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:13:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:14:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:14:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 21
expected_example_num: 150
selection_ratio: 0.14
finetune_deepseek!
{'loss': 0.276, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0962, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0571, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 176.7773, 'train_samples_per_second': 0.356, 'train_steps_per_second': 0.068, 'train_loss': 0.14313594748576483, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:18:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:18:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_15_45_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.845177037564058

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.3_3_1/generated_contents/1
INFO 11-29 07:19:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:19:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_15_45_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.8499120876317837

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.3_3_1/generated_contents/2
INFO 11-29 07:19:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:19:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_15_45_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.8621017963365347

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_15_45_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_15_45_0.7_0.3_3_1/checkpoint-12
searching parameters: task1557_20_15_50_0.7_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.7_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:20:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:20:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:22:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:22:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 41
expected_example_num: 300
selection_ratio: 0.13666666666666666
finetune_deepseek!
{'loss': 0.3261, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.2314, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.0514, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.037, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0317, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 200.1348, 'train_samples_per_second': 0.615, 'train_steps_per_second': 0.105, 'train_loss': 0.13144152079309737, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:26:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:26:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_50_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.9013415963945716

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.7_0.3_3_1/generated_contents/1
INFO 11-29 07:27:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:27:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_50_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.901217125842678

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.7_0.3_3_1/generated_contents/2
INFO 11-29 07:27:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:27:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_15_50_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.9010382843524081

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_15_50_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_15_50_0.7_0.3_3_1/checkpoint-21
searching parameters: task1557_20_10_45_0.7_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.7_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:28:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:28:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:29:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:29:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 26
expected_example_num: 200
selection_ratio: 0.13
finetune_deepseek!
{'loss': 0.3064, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.1128, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.0109, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 181.0717, 'train_samples_per_second': 0.431, 'train_steps_per_second': 0.083, 'train_loss': 0.11572176925837993, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:33:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:33:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_45_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.37546394746502293

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.7_0.4_3_1/generated_contents/1
INFO 11-29 07:34:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:34:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_45_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.6474275942085617

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.7_0.4_3_1/generated_contents/2
INFO 11-29 07:34:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:35:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_45_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.5897166546939852

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.7_0.4_3_1/checkpoint-15
searching parameters: task1557_20_15_50_0.7_0.3_3_1
searching parameters: task1557_10_15_45_0.7_0.3_3_1
searching parameters: task1557_20_10_50_0.8_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_50_0.8_0.3_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_50_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:35:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:35:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:37:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:37:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 26
expected_example_num: 200
selection_ratio: 0.13
finetune_deepseek!
{'loss': 1.2096, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3246, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.068, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 177.4573, 'train_samples_per_second': 0.44, 'train_steps_per_second': 0.085, 'train_loss': 0.43651882708072665, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:40:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:41:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_50_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.17733300841626282

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_50_0.8_0.3_3_1/generated_contents/1
INFO 11-29 07:41:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:41:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_50_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.9071000487793215

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_50_0.8_0.3_3_1/generated_contents/2
INFO 11-29 07:42:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:42:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_50_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.8045242638226001

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_50_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1557_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-10 /data2/cyzhao/best_ckpt/NI_task1557_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_50_0.8_0.3_3_1/checkpoint-15
searching parameters: task1557_20_10_45_0.8_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.8_0.35_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.8_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:43:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:43:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:44:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:45:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 24
expected_example_num: 200
selection_ratio: 0.12
finetune_deepseek!
{'loss': 0.2625, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.0427, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0076, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 173.1693, 'train_samples_per_second': 0.416, 'train_steps_per_second': 0.069, 'train_loss': 0.10423702327534556, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-8/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-4/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:48:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:49:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_45_0.8_0.35_3_1 epoch 1

------------------------------------------------

0.8887671535501197

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.8_0.35_3_1/generated_contents/1
INFO 11-29 07:49:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-8', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:49:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_45_0.8_0.35_3_1 epoch 2

------------------------------------------------

0.5441721258717114

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.8_0.35_3_1/generated_contents/2
INFO 11-29 07:50:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:50:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_20_10_45_0.8_0.35_3_1 epoch 3

------------------------------------------------

0.7173952000566576

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_20_10_45_0.8_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-8
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_20_10_45_0.8_0.35_3_1/checkpoint-12
searching parameters: task1557_10_10_40_0.8_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_10_10_40_0.8_0.4_3_1
/home/cyzhao/NI_task1557_exp_1/task1557_10_10_40_0.8_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 07:51:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:51:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 07:52:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer='/data/ckpts/huggingface/models/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:52:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 17
expected_example_num: 100
selection_ratio: 0.17
finetune_deepseek!
{'loss': 0.2159, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 1.3433, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 161.4049, 'train_samples_per_second': 0.316, 'train_steps_per_second': 0.056, 'train_loss': 0.7004998624324799, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-29 07:56:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:56:21 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_10_40_0.8_0.4_3_1 epoch 1

------------------------------------------------

0.3399515560154095

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_10_40_0.8_0.4_3_1/generated_contents/1
INFO 11-29 07:57:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:57:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_10_40_0.8_0.4_3_1 epoch 2

------------------------------------------------

0.41567165433685727

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_10_40_0.8_0.4_3_1/generated_contents/2
INFO 11-29 07:58:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 07:58:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1557_10_10_40_0.8_0.4_3_1 epoch 3

------------------------------------------------

0.556857347742932

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/task1557_10_10_40_0.8_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1557_10_10_40_0.8_0.4_3_1/checkpoint-9
{'generation_epochs': 20, 'generation_batch_size': 10, 'generation_top_k': 50, 'generation_temperature': 0.8, 'min_frequency': 0.3, 'training_epochs': 3}
test best ckpt.
INFO 11-29 07:59:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task1557_exp_1', tokenizer='/data2/cyzhao/best_ckpt/NI_task1557_exp_1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 08:00:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task1557_exp_1

------------------------------------------------

0.9107963378747399

------------------------------------------------


The best ckpt on test set gain 0.9107963378747399
Genrated contents are stored in /home/cyzhao/NI_task1557_exp_1/best_ckpt_generated_content
