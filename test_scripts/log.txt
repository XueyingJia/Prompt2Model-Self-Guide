[2023-11-21 08:24:59,055] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/SQuAD_experiments_4/SQuAD_20_20_30_1.0_0.3_120_3
/home/cyzhao/SQuAD_experiments_4/SQuAD_20_20_30_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-21 08:25:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 08:25:19 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-21 08:31:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 08:32:09 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
finetune_vicuna!
{'train_runtime': 436.8622, 'train_samples_per_second': 2.452, 'train_steps_per_second': 0.309, 'train_loss': 0.19067435087981047, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-135/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-90/optimizer.pt
validate!
last validate 0.
INFO 11-21 08:41:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 08:43:20 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.666

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_4/SQuAD_20_20_30_1.0_0.3_120_3/generated_contents/1
INFO 11-21 08:45:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-90', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 08:46:12 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.657

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_4/SQuAD_20_20_30_1.0_0.3_120_3/generated_contents/2
INFO 11-21 08:47:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-135', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-135', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 08:48:18 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.666

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_4/SQuAD_20_20_30_1.0_0.3_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-45
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-90
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-135
/home/cyzhao/SQuAD_experiments_4/SQuAD_10_20_30_1.0_0.3_120_3
/home/cyzhao/SQuAD_experiments_4/SQuAD_10_20_30_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-21 08:50:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 08:50:22 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-21 08:54:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 08:54:43 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
finetune_vicuna!
{'train_runtime': 291.3318, 'train_samples_per_second': 1.73, 'train_steps_per_second': 0.216, 'train_loss': 0.17272083343021452, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-21 09:01:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 09:03:01 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_30_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.477

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_4/SQuAD_10_20_30_1.0_0.3_120_3/generated_contents/1
INFO 11-21 09:04:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 09:05:37 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_30_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.612

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_4/SQuAD_10_20_30_1.0_0.3_120_3/generated_contents/2
INFO 11-21 09:07:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-21 09:07:30 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_30_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.641

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_4/SQuAD_10_20_30_1.0_0.3_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-21
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-63
