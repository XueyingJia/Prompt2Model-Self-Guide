[2023-11-22 07:25:06,429] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
searching parameters: SQuAD_20_10_45_1.0_0.35_110_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_20_10_45_1.0_0.35_110_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_20_10_45_1.0_0.35_110_3/config.json
generate_and_write_inputs!
INFO 11-22 07:25:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:25:27 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 07:30:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:30:19 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 178
expected_example_num: 200
selection_ratio: 0.89
finetune_vicuna!
{'train_runtime': 289.7902, 'train_samples_per_second': 1.843, 'train_steps_per_second': 0.238, 'train_loss': 0.14797064186870187, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-69/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-23/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-46/optimizer.pt
validate!
last validate 0.
INFO 11-22 07:37:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-23', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-23', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:37:21 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_10_45_1.0_0.35_110_3 epoch 1

------------------------------------------------

0.586

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_20_10_45_1.0_0.35_110_3/generated_contents/1
INFO 11-22 07:39:28 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-46', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-46', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:39:43 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_10_45_1.0_0.35_110_3 epoch 2

------------------------------------------------

0.592

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_20_10_45_1.0_0.35_110_3/generated_contents/2
INFO 11-22 07:41:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-69', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-69', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:41:56 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_10_45_1.0_0.35_110_3 epoch 3

------------------------------------------------

0.604

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_20_10_45_1.0_0.35_110_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/SQuAD
mv /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-69 /data2/cyzhao/best_ckpt/SQuAD
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-23
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_10_45_1.0_0.35_110_3/checkpoint-46
searching parameters: SQuAD_10_20_50_0.9_0.3_115_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.9_0.3_115_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.9_0.3_115_3/config.json
generate_and_write_inputs!
INFO 11-22 07:44:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:44:17 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 07:50:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:50:49 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 190
expected_example_num: 200
selection_ratio: 0.95
finetune_vicuna!
{'train_runtime': 329.685, 'train_samples_per_second': 1.729, 'train_steps_per_second': 0.218, 'train_loss': 0.22056584888034397, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-72/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-22 07:58:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 07:58:36 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_50_0.9_0.3_115_3 epoch 1

------------------------------------------------

0.468

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.9_0.3_115_3/generated_contents/1
INFO 11-22 08:00:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:00:39 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_50_0.9_0.3_115_3 epoch 2

------------------------------------------------

0.635

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.9_0.3_115_3/generated_contents/2
INFO 11-22 08:02:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-72', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-72', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:02:50 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_50_0.9_0.3_115_3 epoch 3

------------------------------------------------

0.62

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.9_0.3_115_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/SQuAD
mv /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-48 /data2/cyzhao/best_ckpt/SQuAD
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.9_0.3_115_3/checkpoint-72
searching parameters: SQuAD_10_10_50_0.8_0.35_110_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_50_0.8_0.35_110_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_50_0.8_0.35_110_3/config.json
generate_and_write_inputs!
INFO 11-22 08:04:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:05:07 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 08:07:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:07:41 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 94
expected_example_num: 100
selection_ratio: 0.94
finetune_vicuna!
{'train_runtime': 221.7515, 'train_samples_per_second': 1.272, 'train_steps_per_second': 0.162, 'train_loss': 0.23337674140930176, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-22 08:12:31 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:12:45 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_10_50_0.8_0.35_110_3 epoch 1

------------------------------------------------

0.537

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_50_0.8_0.35_110_3/generated_contents/1
INFO 11-22 08:14:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:14:59 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_10_50_0.8_0.35_110_3 epoch 2

------------------------------------------------

0.668

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_50_0.8_0.35_110_3/generated_contents/2
INFO 11-22 08:17:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:17:54 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_10_50_0.8_0.35_110_3 epoch 3

------------------------------------------------

0.643

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_50_0.8_0.35_110_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/SQuAD
mv /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-24 /data2/cyzhao/best_ckpt/SQuAD
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_50_0.8_0.35_110_3/checkpoint-36
searching parameters: SQuAD_20_20_40_0.8_0.3_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_20_20_40_0.8_0.3_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_20_20_40_0.8_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-22 08:20:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:20:31 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 08:28:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:28:58 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 359
expected_example_num: 400
selection_ratio: 0.8975
finetune_vicuna!
{'train_runtime': 430.184, 'train_samples_per_second': 2.504, 'train_steps_per_second': 0.314, 'train_loss': 0.1996178662335431, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-135/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-90/optimizer.pt
validate!
last validate 0.
INFO 11-22 08:38:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:39:01 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_20_40_0.8_0.3_120_3 epoch 1

------------------------------------------------

0.679

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_20_20_40_0.8_0.3_120_3/generated_contents/1
INFO 11-22 08:41:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-90', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:41:17 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_20_40_0.8_0.3_120_3 epoch 2

------------------------------------------------

0.663

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_20_20_40_0.8_0.3_120_3/generated_contents/2
INFO 11-22 08:43:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-135', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-135', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:43:51 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_20_20_40_0.8_0.3_120_3 epoch 3

------------------------------------------------

0.687

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_20_20_40_0.8_0.3_120_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/SQuAD
mv /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-135 /data2/cyzhao/best_ckpt/SQuAD
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-45
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_0.8_0.3_120_3/checkpoint-90
searching parameters: SQuAD_30_20_50_1.0_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_50_1.0_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_50_1.0_0.4_120_3/config.json
generate_and_write_inputs!
INFO 11-22 08:46:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:46:30 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 08:59:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 08:59:18 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 535
expected_example_num: 600
selection_ratio: 0.8916666666666667
finetune_vicuna!
{'train_runtime': 563.7307, 'train_samples_per_second': 2.847, 'train_steps_per_second': 0.357, 'train_loss': 0.13150939182262517, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-134/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-67/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-201/optimizer.pt
validate!
last validate 0.
INFO 11-22 09:12:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-67', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-67', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:13:03 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_20_50_1.0_0.4_120_3 epoch 1

------------------------------------------------

0.699

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_50_1.0_0.4_120_3/generated_contents/1
INFO 11-22 09:15:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-134', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-134', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:15:25 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_20_50_1.0_0.4_120_3 epoch 2

------------------------------------------------

0.696

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_50_1.0_0.4_120_3/generated_contents/2
INFO 11-22 09:17:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-201', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-201', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:18:00 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_20_50_1.0_0.4_120_3 epoch 3

------------------------------------------------

0.699

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_50_1.0_0.4_120_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/SQuAD
mv /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-67 /data2/cyzhao/best_ckpt/SQuAD
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-134
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_50_1.0_0.4_120_3/checkpoint-201
searching parameters: SQuAD_10_10_45_0.7_0.4_115_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_45_0.7_0.4_115_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_45_0.7_0.4_115_3/config.json
generate_and_write_inputs!
INFO 11-22 09:20:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:20:33 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 09:23:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:23:18 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 84
expected_example_num: 100
selection_ratio: 0.84
finetune_vicuna!
{'train_runtime': 195.7394, 'train_samples_per_second': 1.287, 'train_steps_per_second': 0.169, 'train_loss': 0.38391029473507043, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-22 09:27:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:27:50 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_10_45_0.7_0.4_115_3 epoch 1

------------------------------------------------

0.238

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_45_0.7_0.4_115_3/generated_contents/1
INFO 11-22 09:29:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:29:56 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_10_45_0.7_0.4_115_3 epoch 2

------------------------------------------------

0.408

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_45_0.7_0.4_115_3/generated_contents/2
INFO 11-22 09:31:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:32:03 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_10_45_0.7_0.4_115_3 epoch 3

------------------------------------------------

0.409

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_10_45_0.7_0.4_115_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_10_45_0.7_0.4_115_3/checkpoint-33
searching parameters: SQuAD_10_15_40_0.8_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_120_3/config.json
generate_and_write_inputs!
INFO 11-22 09:33:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:34:11 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 09:37:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:37:32 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 125
expected_example_num: 150
selection_ratio: 0.8333333333333334
finetune_vicuna!
{'train_runtime': 252.1174, 'train_samples_per_second': 1.487, 'train_steps_per_second': 0.19, 'train_loss': 0.16476627190907797, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-22 09:43:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:43:29 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_15_40_0.8_0.4_120_3 epoch 1

------------------------------------------------

0.557

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_120_3/generated_contents/1
INFO 11-22 09:45:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:46:13 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_15_40_0.8_0.4_120_3 epoch 2

------------------------------------------------

0.55

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_120_3/generated_contents/2
INFO 11-22 09:48:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:48:52 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_15_40_0.8_0.4_120_3 epoch 3

------------------------------------------------

0.766

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_120_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/SQuAD
mv /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-48 /data2/cyzhao/best_ckpt/SQuAD
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_120_3/checkpoint-32
searching parameters: SQuAD_30_15_50_0.9_0.3_125_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_125_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_125_3/config.json
generate_and_write_inputs!
INFO 11-22 09:52:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 09:52:15 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
WARNING 11-22 09:56:26 scheduler.py:146] Input prompt (4105 tokens) is too long and exceeds limit of 4096
WARNING 11-22 09:57:41 scheduler.py:146] Input prompt (4354 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:02:08 scheduler.py:146] Input prompt (4336 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:03:17 scheduler.py:146] Input prompt (4135 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:03:18 scheduler.py:146] Input prompt (4405 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:03:53 scheduler.py:146] Input prompt (4170 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:03:53 scheduler.py:146] Input prompt (4130 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:03:54 scheduler.py:146] Input prompt (4373 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:03:54 scheduler.py:146] Input prompt (4163 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:05:05 scheduler.py:146] Input prompt (4290 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:06:17 scheduler.py:146] Input prompt (4111 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:06:46 scheduler.py:146] Input prompt (4298 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:06:46 scheduler.py:146] Input prompt (4384 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:08:15 scheduler.py:146] Input prompt (4103 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-22 10:09:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 10:09:51 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 386
expected_example_num: 450
selection_ratio: 0.8577777777777778
finetune_vicuna!
{'train_runtime': 525.3933, 'train_samples_per_second': 2.204, 'train_steps_per_second': 0.28, 'train_loss': 0.15631320200809817, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-147/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-98/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-49/optimizer.pt
validate!
last validate 0.
INFO 11-22 10:22:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-49', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-49', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 10:23:03 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_50_0.9_0.3_125_3 epoch 1

------------------------------------------------

0.556

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_125_3/generated_contents/1
INFO 11-22 10:25:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-98', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-98', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 10:25:29 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_50_0.9_0.3_125_3 epoch 2

------------------------------------------------

0.521

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_125_3/generated_contents/2
INFO 11-22 10:27:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-147', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-147', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 10:27:34 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_50_0.9_0.3_125_3 epoch 3

------------------------------------------------

0.554

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-49
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-98
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_125_3/checkpoint-147
searching parameters: SQuAD_30_15_50_0.9_0.3_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-22 10:29:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 10:29:53 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
WARNING 11-22 10:33:43 scheduler.py:146] Input prompt (4105 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:35:06 scheduler.py:146] Input prompt (4354 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:39:51 scheduler.py:146] Input prompt (4336 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:41:11 scheduler.py:146] Input prompt (4135 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:41:12 scheduler.py:146] Input prompt (4405 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:41:48 scheduler.py:146] Input prompt (4170 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:41:48 scheduler.py:146] Input prompt (4130 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:41:49 scheduler.py:146] Input prompt (4373 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:41:49 scheduler.py:146] Input prompt (4163 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:43:00 scheduler.py:146] Input prompt (4290 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:44:16 scheduler.py:146] Input prompt (4111 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:44:53 scheduler.py:146] Input prompt (4298 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:44:53 scheduler.py:146] Input prompt (4384 tokens) is too long and exceeds limit of 4096
WARNING 11-22 10:46:53 scheduler.py:146] Input prompt (4103 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-22 10:48:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 10:48:33 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 387
expected_example_num: 450
selection_ratio: 0.86
finetune_vicuna!
{'train_runtime': 532.0632, 'train_samples_per_second': 2.182, 'train_steps_per_second': 0.276, 'train_loss': 0.15287189094387754, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-147/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-98/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-49/optimizer.pt
validate!
last validate 0.
INFO 11-22 11:01:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-49', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-49', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:01:51 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_50_0.9_0.3_120_3 epoch 1

------------------------------------------------

0.645

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_120_3/generated_contents/1
INFO 11-22 11:03:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-98', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-98', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:03:59 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_50_0.9_0.3_120_3 epoch 2

------------------------------------------------

0.643

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_120_3/generated_contents/2
INFO 11-22 11:05:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-147', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-147', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:06:05 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_50_0.9_0.3_120_3 epoch 3

------------------------------------------------

0.678

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_50_0.9_0.3_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-49
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-98
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_50_0.9_0.3_120_3/checkpoint-147
searching parameters: SQuAD_10_20_50_0.7_0.3_115_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.7_0.3_115_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.7_0.3_115_3/config.json
generate_and_write_inputs!
INFO 11-22 11:08:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:08:19 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 11:11:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:11:59 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 182
expected_example_num: 200
selection_ratio: 0.91
finetune_vicuna!
{'train_runtime': 291.9918, 'train_samples_per_second': 1.87, 'train_steps_per_second': 0.236, 'train_loss': 0.24890587295311084, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-69/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-23/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-46/optimizer.pt
validate!
last validate 0.
INFO 11-22 11:18:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-23', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-23', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:18:34 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_50_0.7_0.3_115_3 epoch 1

------------------------------------------------

0.676

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.7_0.3_115_3/generated_contents/1
INFO 11-22 11:20:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-46', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-46', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:21:13 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_50_0.7_0.3_115_3 epoch 2

------------------------------------------------

0.559

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.7_0.3_115_3/generated_contents/2
INFO 11-22 11:23:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-69', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-69', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:23:32 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_20_50_0.7_0.3_115_3 epoch 3

------------------------------------------------

0.556

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_20_50_0.7_0.3_115_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-23
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-46
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_20_50_0.7_0.3_115_3/checkpoint-69
searching parameters: SQuAD_10_15_40_0.8_0.4_125_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_125_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_125_3/config.json
generate_and_write_inputs!
INFO 11-22 11:25:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:25:57 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 11:28:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:28:40 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 125
expected_example_num: 150
selection_ratio: 0.8333333333333334
finetune_vicuna!
{'train_runtime': 253.0568, 'train_samples_per_second': 1.482, 'train_steps_per_second': 0.19, 'train_loss': 0.17629383007685342, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-32/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-16/optimizer.pt
validate!
last validate 0.
INFO 11-22 11:34:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-16', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-16', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:34:38 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_15_40_0.8_0.4_125_3 epoch 1

------------------------------------------------

0.49

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_125_3/generated_contents/1
INFO 11-22 11:37:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-32', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:37:28 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_15_40_0.8_0.4_125_3 epoch 2

------------------------------------------------

0.553

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_125_3/generated_contents/2
INFO 11-22 11:40:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:40:22 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_10_15_40_0.8_0.4_125_3 epoch 3

------------------------------------------------

0.537

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_10_15_40_0.8_0.4_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-16
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-32
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_10_15_40_0.8_0.4_125_3/checkpoint-48
searching parameters: SQuAD_30_15_40_1.0_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_40_1.0_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_40_1.0_0.4_120_3/config.json
generate_and_write_inputs!
INFO 11-22 11:42:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:43:10 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 11:53:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 11:53:31 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 402
expected_example_num: 450
selection_ratio: 0.8933333333333333
finetune_vicuna!
{'train_runtime': 481.9577, 'train_samples_per_second': 2.502, 'train_steps_per_second': 0.317, 'train_loss': 0.11241637335883246, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-153/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-102/optimizer.pt
validate!
last validate 0.
INFO 11-22 12:04:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:05:13 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_40_1.0_0.4_120_3 epoch 1

------------------------------------------------

0.62

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_40_1.0_0.4_120_3/generated_contents/1
INFO 11-22 12:07:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-102', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-102', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:07:35 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_40_1.0_0.4_120_3 epoch 2

------------------------------------------------

0.544

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_40_1.0_0.4_120_3/generated_contents/2
INFO 11-22 12:09:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-153', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-153', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:09:40 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_15_40_1.0_0.4_120_3 epoch 3

------------------------------------------------

0.577

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_15_40_1.0_0.4_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-51
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-102
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_15_40_1.0_0.4_120_3/checkpoint-153
searching parameters: SQuAD_30_20_40_1.0_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_40_1.0_0.4_120_3
/home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_40_1.0_0.4_120_3/config.json
generate_and_write_inputs!
INFO 11-22 12:11:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:11:49 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
annotate_and_write_outputs!
INFO 11-22 12:23:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:23:23 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048
generated_example_num: 535
expected_example_num: 600
selection_ratio: 0.8916666666666667
finetune_vicuna!
{'train_runtime': 588.7688, 'train_samples_per_second': 2.726, 'train_steps_per_second': 0.341, 'train_loss': 0.15728084127701336, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-134/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-67/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-201/optimizer.pt
validate!
last validate 0.
INFO 11-22 12:37:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-67', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-67', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:37:31 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_20_40_1.0_0.4_120_3 epoch 1

------------------------------------------------

0.693

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_40_1.0_0.4_120_3/generated_contents/1
INFO 11-22 12:40:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-134', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-134', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:40:18 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_20_40_1.0_0.4_120_3 epoch 2

------------------------------------------------

0.717

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_40_1.0_0.4_120_3/generated_contents/2
INFO 11-22 12:42:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-201', tokenizer='/data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-201', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-22 12:42:41 llm_engine.py:207] # GPU blocks: 34604, # CPU blocks: 2048


result of SQuAD_30_20_40_1.0_0.4_120_3 epoch 3

------------------------------------------------

0.711

------------------------------------------------


Genrated contents are stored in /home/cyzhao/SQuAD_experiments_7/SQuAD_30_20_40_1.0_0.4_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-67
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-134
rm -rf /data2/cyzhao/ckpt_data_p2ms/SQuAD_30_20_40_1.0_0.4_120_3/checkpoint-201
searching parameters: SQuAD_30_15_40_1.0_0.4_120_3
