[2023-11-14 03:38:30,442] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/config.json
Checkpoint 'checkpoint-43' is complete.
Checkpoint 'checkpoint-86' is complete.
Checkpoint 'checkpoint-129' is complete.
evaluate!
last evaluate 0.
INFO 11-14 03:38:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-43', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-43', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 03:38:49 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_40_10_50_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.591

------------------------------------------------


INFO 11-14 03:40:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-86', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-86', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 03:40:54 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_40_10_50_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.557

------------------------------------------------


INFO 11-14 03:42:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-129', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-129', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 03:42:49 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_40_10_50_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.624

------------------------------------------------


[2023-11-14 03:44:30,237] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/config.json
finetune_vicuna!
{'train_runtime': 399.4876, 'train_samples_per_second': 2.606, 'train_steps_per_second': 0.33, 'train_loss': 0.12809903693921637, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-44/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-88/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-132/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-14 03:51:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-44', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 03:51:52 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_50_0.7_0.3_120_3 epoch 1

------------------------------------------------

0.51

------------------------------------------------


INFO 11-14 03:53:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-88', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-88', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 03:53:35 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_50_0.7_0.3_120_3 epoch 2

------------------------------------------------

0.572

------------------------------------------------


INFO 11-14 03:55:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-132', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-132', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 03:55:19 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_50_0.7_0.3_120_3 epoch 3

------------------------------------------------

0.612

------------------------------------------------


[2023-11-14 03:56:44,913] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/config.json
finetune_vicuna!
{'train_runtime': 361.9012, 'train_samples_per_second': 2.578, 'train_steps_per_second': 0.323, 'train_loss': 0.15713116246410924, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-39/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-78/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-117/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-14 04:03:15 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-39', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:03:29 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_50_0.5_0.3_120_3 epoch 1

------------------------------------------------

0.353

------------------------------------------------


INFO 11-14 04:04:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-78', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:05:08 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_50_0.5_0.3_120_3 epoch 2

------------------------------------------------

0.362

------------------------------------------------


INFO 11-14 04:06:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-117', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-117', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:07:06 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_50_0.5_0.3_120_3 epoch 3

------------------------------------------------

0.492

------------------------------------------------


[2023-11-14 04:09:08,121] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/config.json
finetune_vicuna!
{'train_runtime': 327.2448, 'train_samples_per_second': 2.191, 'train_steps_per_second': 0.275, 'train_loss': 0.22070558336046006, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-30/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-60/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-90/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-14 04:15:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-30', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:15:17 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_40_10_50_0.3_0.3_120_3 epoch 1

------------------------------------------------

0.471

------------------------------------------------


INFO 11-14 04:19:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-60', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-60', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:19:41 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_40_10_50_0.3_0.3_120_3 epoch 2

------------------------------------------------

0.232

------------------------------------------------


INFO 11-14 04:21:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-90', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:21:48 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_40_10_50_0.3_0.3_120_3 epoch 3

------------------------------------------------

0.308

------------------------------------------------


[2023-11-14 04:23:58,710] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/config.json
finetune_vicuna!
{'train_runtime': 401.8043, 'train_samples_per_second': 2.651, 'train_steps_per_second': 0.336, 'train_loss': 0.18291592068142362, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-45/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-90/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-135/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-14 04:31:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-45', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:31:21 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_40_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.43

------------------------------------------------


INFO 11-14 04:33:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-90', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:33:16 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_40_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.626

------------------------------------------------


INFO 11-14 04:35:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-135', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-135', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:35:20 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_40_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.607

------------------------------------------------


[2023-11-14 04:37:05,038] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/config.json
finetune_vicuna!
{'train_runtime': 403.7031, 'train_samples_per_second': 2.497, 'train_steps_per_second': 0.312, 'train_loss': 0.4392752268957713, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-42/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-84/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-126/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-14 04:44:16 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-42', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:44:30 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.633

------------------------------------------------


INFO 11-14 04:46:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-84', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-84', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:46:33 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.624

------------------------------------------------


INFO 11-14 04:48:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-126', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-126', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:48:37 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.668

------------------------------------------------


[2023-11-14 04:50:31,335] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/config.json
finetune_vicuna!
{'train_runtime': 268.579, 'train_samples_per_second': 1.944, 'train_steps_per_second': 0.246, 'train_loss': 0.26891835530598956, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-22/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-44/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-66/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-14 04:55:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-22', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:55:41 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048


result of SQuAD_10_20_30_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.402

------------------------------------------------


INFO 11-14 04:57:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-44', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, seed=0)
INFO 11-14 04:57:56 llm_engine.py:207] # GPU blocks: 18382, # CPU blocks: 2048
