[2023-11-15 02:51:12,575] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 02:51:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 02:51:32 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 03:00:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:00:21 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 437.1562, 'train_samples_per_second': 2.464, 'train_steps_per_second': 0.309, 'train_loss': 0.10358219853153935, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-45/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-90/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-135/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 03:10:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-45', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:10:48 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_40_10_50_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.69

------------------------------------------------


INFO 11-15 03:12:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-90', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-90', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:12:46 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_40_10_50_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.667

------------------------------------------------


INFO 11-15 03:14:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-135', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/checkpoint-135', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:14:45 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_40_10_50_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.673

------------------------------------------------


[2023-11-15 03:16:37,046] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 03:16:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:16:56 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 03:22:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:23:04 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 408.6195, 'train_samples_per_second': 2.54, 'train_steps_per_second': 0.323, 'train_loss': 0.12889772472959576, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-44/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-88/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-132/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 03:32:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-44', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:32:15 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_50_0.7_0.3_120_3 epoch 1

------------------------------------------------

0.674

------------------------------------------------


INFO 11-15 03:34:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-88', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-88', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:35:05 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_50_0.7_0.3_120_3 epoch 2

------------------------------------------------

0.615

------------------------------------------------


INFO 11-15 03:37:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-132', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/checkpoint-132', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:37:16 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_50_0.7_0.3_120_3 epoch 3

------------------------------------------------

0.593

------------------------------------------------


[2023-11-15 03:39:16,081] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 03:39:22 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:39:36 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 03:44:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:45:10 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 375.5601, 'train_samples_per_second': 2.476, 'train_steps_per_second': 0.312, 'train_loss': 0.09729572035308577, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-39/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-78/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-117/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 03:53:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-39', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:53:22 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_50_0.5_0.3_120_3 epoch 1

------------------------------------------------

0.219

------------------------------------------------


INFO 11-15 03:55:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-78', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:55:14 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_50_0.5_0.3_120_3 epoch 2

------------------------------------------------

0.274

------------------------------------------------


INFO 11-15 03:56:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-117', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/checkpoint-117', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:57:09 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_50_0.5_0.3_120_3 epoch 3

------------------------------------------------

0.31

------------------------------------------------


[2023-11-15 03:58:57,772] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 03:59:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 03:59:17 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 04:05:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:05:44 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 363.6877, 'train_samples_per_second': 2.103, 'train_steps_per_second': 0.264, 'train_loss': 0.32669474681218463, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-32/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-64/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-96/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 04:13:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-32', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-32', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:13:33 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_40_10_50_0.3_0.3_120_3 epoch 1

------------------------------------------------

0.118

------------------------------------------------


INFO 11-15 04:15:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-64', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-64', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:15:16 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_40_10_50_0.3_0.3_120_3 epoch 2

------------------------------------------------

0.115

------------------------------------------------


INFO 11-15 04:16:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-96', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/checkpoint-96', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:17:04 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_40_10_50_0.3_0.3_120_3 epoch 3

------------------------------------------------

0.137

------------------------------------------------


[2023-11-15 04:18:41,252] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 04:18:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:19:01 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 04:26:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:26:31 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 424.5187, 'train_samples_per_second': 2.48, 'train_steps_per_second': 0.311, 'train_loss': 0.15130446173928, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-44/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-88/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-132/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 04:36:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-44', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:36:13 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_40_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.69

------------------------------------------------


INFO 11-15 04:37:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-88', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-88', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:38:12 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_40_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.676

------------------------------------------------


INFO 11-15 04:39:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-132', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/checkpoint-132', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:40:09 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_40_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.674

------------------------------------------------


[2023-11-15 04:41:56,673] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 04:42:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:42:16 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 04:48:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:48:13 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 416.7368, 'train_samples_per_second': 2.498, 'train_steps_per_second': 0.317, 'train_loss': 0.21056636174519858, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-44/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-88/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-132/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 04:58:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-44', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 04:58:34 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.563

------------------------------------------------


INFO 11-15 05:00:13 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-88', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-88', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:00:25 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.672

------------------------------------------------


INFO 11-15 05:02:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-132', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/checkpoint-132', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:02:12 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_20_20_30_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.695

------------------------------------------------


[2023-11-15 05:03:50,717] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 05:03:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:04:09 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 05:07:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:07:24 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 274.6602, 'train_samples_per_second': 1.933, 'train_steps_per_second': 0.251, 'train_loss': 0.12049039204915364, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-23/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-46/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-69/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 05:13:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-23', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-23', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:13:39 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_10_20_30_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.637

------------------------------------------------


INFO 11-15 05:15:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-46', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-46', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:15:29 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_10_20_30_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.7

------------------------------------------------


INFO 11-15 05:17:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-69', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/checkpoint-69', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:17:24 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_10_20_30_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.704

------------------------------------------------


[2023-11-15 05:19:14,698] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 05:19:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:19:34 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-15 05:28:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:28:51 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
{'train_runtime': 571.9533, 'train_samples_per_second': 2.832, 'train_steps_per_second': 0.357, 'train_loss': 0.13671611337100759, 'epoch': 3.0}
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-68/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-136/optimizer.pt
Deleting /data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-204/optimizer.pt
evaluate!
last evaluate 0.
INFO 11-15 05:41:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-68', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-68', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:42:03 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_30_20_30_1.0_0.3_120_3 epoch 1

------------------------------------------------

0.423

------------------------------------------------


INFO 11-15 05:43:35 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-136', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-136', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:43:48 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_30_20_30_1.0_0.3_120_3 epoch 2

------------------------------------------------

0.641

------------------------------------------------


INFO 11-15 05:45:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-204', tokenizer='/data1/cyzhao/ckpt_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/checkpoint-204', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:45:38 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024


result of SQuAD_30_20_30_1.0_0.3_120_3 epoch 3

------------------------------------------------

0.686

------------------------------------------------


[2023-11-15 05:47:18,943] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_40_20_30_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 05:47:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 05:47:38 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
WARNING 11-15 05:50:31 scheduler.py:146] Input prompt (4267 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:50:59 scheduler.py:146] Input prompt (4190 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:51:29 scheduler.py:146] Input prompt (4233 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:51:32 scheduler.py:146] Input prompt (4215 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:52:02 scheduler.py:146] Input prompt (4346 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:52:32 scheduler.py:146] Input prompt (4175 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:53:29 scheduler.py:146] Input prompt (4322 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:54:04 scheduler.py:146] Input prompt (4205 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:54:31 scheduler.py:146] Input prompt (4326 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:55:22 scheduler.py:146] Input prompt (4145 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:55:23 scheduler.py:146] Input prompt (4212 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:55:50 scheduler.py:146] Input prompt (4101 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:55:50 scheduler.py:146] Input prompt (4194 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:55:51 scheduler.py:146] Input prompt (4469 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:56:17 scheduler.py:146] Input prompt (4264 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:56:17 scheduler.py:146] Input prompt (4328 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:56:18 scheduler.py:146] Input prompt (4472 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:58:33 scheduler.py:146] Input prompt (4245 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:59:05 scheduler.py:146] Input prompt (4147 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:59:07 scheduler.py:146] Input prompt (4220 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:59:38 scheduler.py:146] Input prompt (4347 tokens) is too long and exceeds limit of 4096
WARNING 11-15 05:59:39 scheduler.py:146] Input prompt (4177 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:00:12 scheduler.py:146] Input prompt (4199 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:01:26 scheduler.py:146] Input prompt (4100 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:01:26 scheduler.py:146] Input prompt (4359 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:01:57 scheduler.py:146] Input prompt (4330 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:02:00 scheduler.py:146] Input prompt (4448 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:02:00 scheduler.py:146] Input prompt (4376 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:03:06 scheduler.py:146] Input prompt (4291 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:03:09 scheduler.py:146] Input prompt (4451 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:03:42 scheduler.py:146] Input prompt (4162 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:04:19 scheduler.py:146] Input prompt (4370 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:04:21 scheduler.py:146] Input prompt (4586 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:04:54 scheduler.py:146] Input prompt (4137 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:04:54 scheduler.py:146] Input prompt (4154 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:04:55 scheduler.py:146] Input prompt (4494 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:06:43 scheduler.py:146] Input prompt (4379 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:06:45 scheduler.py:146] Input prompt (4628 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:07:14 scheduler.py:146] Input prompt (4230 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:07:14 scheduler.py:146] Input prompt (4228 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-15 06:07:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 06:08:02 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
[2023-11-15 06:28:45,842] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/cyzhao/log_and_data_p2ms/SQuAD_50_20_30_1.0_0.3_120_3/config.json
generate_and_write_inputs!
INFO 11-15 06:28:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 06:29:08 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
WARNING 11-15 06:46:25 scheduler.py:146] Input prompt (4208 tokens) is too long and exceeds limit of 4096
WARNING 11-15 06:47:16 scheduler.py:146] Input prompt (4113 tokens) is too long and exceeds limit of 4096
annotate_and_write_outputs!
INFO 11-15 06:49:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-15 06:50:08 llm_engine.py:207] # GPU blocks: 17516, # CPU blocks: 1024
finetune_vicuna!
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_40_10_50_1.0_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_50_0.7_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_50_0.5_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_40_10_50_0.3_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_40_1.0_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_20_20_30_1.0_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_10_20_30_1.0_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_30_20_30_1.0_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_40_20_30_1.0_0.3_120_3/config.json
CUDA_VISIBLE_DEVICES=0,1 python3 main.py --config=/home/cyzhao/log_and_data_p2ms/SQuAD_50_20_30_1.0_0.3_120_3/config.json
