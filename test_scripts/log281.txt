[2023-11-24 22:29:40,139] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task281
searching parameters: task281_15_20_45_0.9_0.4_120_3
/home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.9_0.4_120_3
/home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.9_0.4_120_3/config.json
generate_and_write_inputs!
INFO 11-24 22:29:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:30:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 22:33:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:33:56 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 184
expected_example_num: 300
selection_ratio: 0.6133333333333333
finetune_vicuna!
{'loss': 0.3872, 'learning_rate': 4.710144927536232e-05, 'epoch': 0.17}
{'loss': 0.1928, 'learning_rate': 4.4202898550724645e-05, 'epoch': 0.35}
{'loss': 0.2239, 'learning_rate': 4.130434782608696e-05, 'epoch': 0.52}
{'loss': 0.1592, 'learning_rate': 3.8405797101449274e-05, 'epoch': 0.7}
{'loss': 0.1224, 'learning_rate': 3.5507246376811596e-05, 'epoch': 0.87}
{'loss': 0.1234, 'learning_rate': 3.260869565217392e-05, 'epoch': 1.04}
{'loss': 0.0529, 'learning_rate': 2.971014492753623e-05, 'epoch': 1.22}
{'loss': 0.0766, 'learning_rate': 2.6811594202898553e-05, 'epoch': 1.39}
{'loss': 0.0481, 'learning_rate': 2.391304347826087e-05, 'epoch': 1.57}
{'loss': 0.081, 'learning_rate': 2.101449275362319e-05, 'epoch': 1.74}
{'loss': 0.0459, 'learning_rate': 1.8115942028985507e-05, 'epoch': 1.91}
{'loss': 0.0429, 'learning_rate': 1.5217391304347828e-05, 'epoch': 2.09}
{'loss': 0.0205, 'learning_rate': 1.2318840579710146e-05, 'epoch': 2.26}
{'loss': 0.0262, 'learning_rate': 9.420289855072464e-06, 'epoch': 2.43}
{'loss': 0.0293, 'learning_rate': 6.521739130434783e-06, 'epoch': 2.61}
{'loss': 0.0175, 'learning_rate': 3.6231884057971017e-06, 'epoch': 2.78}
{'loss': 0.0276, 'learning_rate': 7.246376811594203e-07, 'epoch': 2.96}
{'train_runtime': 322.4447, 'train_samples_per_second': 1.712, 'train_steps_per_second': 0.214, 'train_loss': 0.0973676136708346, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-69/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-23/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-46/optimizer.pt
validate!
last validate 0.
INFO 11-24 22:42:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-23', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-23', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:42:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_20_45_0.9_0.4_120_3 epoch 1

------------------------------------------------

0.3036413908944396

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.9_0.4_120_3/generated_contents/1
INFO 11-24 22:44:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-46', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-46', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:44:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_20_45_0.9_0.4_120_3 epoch 2

------------------------------------------------

0.30866196780460287

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.9_0.4_120_3/generated_contents/2
INFO 11-24 22:45:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-69', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-69', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:45:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_20_45_0.9_0.4_120_3 epoch 3

------------------------------------------------

0.31541027667003757

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.9_0.4_120_3/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-69 /data2/cyzhao/best_ckpt/NI_task281_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-23
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.9_0.4_120_3/checkpoint-46
searching parameters: task281_10_10_40_0.4_0.35_115_3
/home/cyzhao/NI_task281_exp_2/task281_10_10_40_0.4_0.35_115_3
/home/cyzhao/NI_task281_exp_2/task281_10_10_40_0.4_0.35_115_3/config.json
generate_and_write_inputs!
INFO 11-24 22:46:50 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:47:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 22:47:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:47:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 1
expected_example_num: 100
selection_ratio: 0.01
finetune_vicuna!
{'train_runtime': 150.9434, 'train_samples_per_second': 0.02, 'train_steps_per_second': 0.02, 'train_loss': 0.11876546343167622, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-24 22:50:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:51:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_10_10_40_0.4_0.35_115_3 epoch 1

------------------------------------------------

0.2885380416734601

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_10_10_40_0.4_0.35_115_3/generated_contents/1
INFO 11-24 22:52:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:52:26 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_10_10_40_0.4_0.35_115_3 epoch 2

------------------------------------------------

0.28274359482388606

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_10_10_40_0.4_0.35_115_3/generated_contents/2
INFO 11-24 22:53:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:53:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_10_10_40_0.4_0.35_115_3 epoch 3

------------------------------------------------

0.2936279144532677

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_10_10_40_0.4_0.35_115_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_10_10_40_0.4_0.35_115_3/checkpoint-3
searching parameters: task281_20_20_40_0.4_0.35_125_3
/home/cyzhao/NI_task281_exp_2/task281_20_20_40_0.4_0.35_125_3
/home/cyzhao/NI_task281_exp_2/task281_20_20_40_0.4_0.35_125_3/config.json
generate_and_write_inputs!
INFO 11-24 22:54:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:54:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 22:58:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 22:58:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 71
expected_example_num: 400
selection_ratio: 0.1775
finetune_vicuna!
{'loss': 0.588, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.1786, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.1267, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0716, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.0566, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0245, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 184.255, 'train_samples_per_second': 1.156, 'train_steps_per_second': 0.147, 'train_loss': 0.15893273993774695, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-24 23:03:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:03:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_20_40_0.4_0.35_125_3 epoch 1

------------------------------------------------

0.3458554976855245

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_20_40_0.4_0.35_125_3/generated_contents/1
INFO 11-24 23:04:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:04:38 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_20_40_0.4_0.35_125_3 epoch 2

------------------------------------------------

0.33116557357481485

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_20_40_0.4_0.35_125_3/generated_contents/2
INFO 11-24 23:05:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:05:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_20_40_0.4_0.35_125_3 epoch 3

------------------------------------------------

0.32725385551426656

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_20_40_0.4_0.35_125_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task281_exp_2
mv /data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-9 /data2/cyzhao/best_ckpt/NI_task281_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_20_20_40_0.4_0.35_125_3/checkpoint-27
searching parameters: task281_15_20_45_0.4_0.3_125_3
/home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.4_0.3_125_3
/home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.4_0.3_125_3/config.json
generate_and_write_inputs!
INFO 11-24 23:07:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:07:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 23:10:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:10:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 56
expected_example_num: 300
selection_ratio: 0.18666666666666668
finetune_vicuna!
{'loss': 0.4028, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.1892, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.1077, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.0724, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.0192, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 176.8098, 'train_samples_per_second': 0.95, 'train_steps_per_second': 0.119, 'train_loss': 0.15201569916236968, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-24 23:14:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:14:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_20_45_0.4_0.3_125_3 epoch 1

------------------------------------------------

0.32742705386298854

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.4_0.3_125_3/generated_contents/1
INFO 11-24 23:15:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:15:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_20_45_0.4_0.3_125_3 epoch 2

------------------------------------------------

0.3113823068348899

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.4_0.3_125_3/generated_contents/2
INFO 11-24 23:17:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:17:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_20_45_0.4_0.3_125_3 epoch 3

------------------------------------------------

0.31660139197145115

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_20_45_0.4_0.3_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_20_45_0.4_0.3_125_3/checkpoint-21
searching parameters: task281_10_20_50_0.4_0.5_125_3
/home/cyzhao/NI_task281_exp_2/task281_10_20_50_0.4_0.5_125_3
/home/cyzhao/NI_task281_exp_2/task281_10_20_50_0.4_0.5_125_3/config.json
generate_and_write_inputs!
INFO 11-24 23:18:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:18:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 23:20:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:20:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 36
expected_example_num: 200
selection_ratio: 0.18
finetune_vicuna!
{'loss': 0.3664, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.1735, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.0492, 'learning_rate': 1e-05, 'epoch': 2.4}
{'train_runtime': 166.5283, 'train_samples_per_second': 0.649, 'train_steps_per_second': 0.09, 'train_loss': 0.16372284044822058, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-5/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-24 23:24:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-5', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:24:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_10_20_50_0.4_0.5_125_3 epoch 1

------------------------------------------------

0.29401584494535044

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_10_20_50_0.4_0.5_125_3/generated_contents/1
INFO 11-24 23:26:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:26:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_10_20_50_0.4_0.5_125_3 epoch 2

------------------------------------------------

0.3152949510389109

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_10_20_50_0.4_0.5_125_3/generated_contents/2
INFO 11-24 23:27:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:27:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_10_20_50_0.4_0.5_125_3 epoch 3

------------------------------------------------

0.3123303816065538

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_10_20_50_0.4_0.5_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-5
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_10_20_50_0.4_0.5_125_3/checkpoint-15
searching parameters: task281_20_10_50_0.9_0.35_115_3
/home/cyzhao/NI_task281_exp_2/task281_20_10_50_0.9_0.35_115_3
/home/cyzhao/NI_task281_exp_2/task281_20_10_50_0.9_0.35_115_3/config.json
generate_and_write_inputs!
INFO 11-24 23:28:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:29:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 23:31:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:31:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 95
expected_example_num: 200
selection_ratio: 0.475
finetune_vicuna!
{'loss': 0.4671, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.2045, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.223, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1141, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.0632, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.0668, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.0218, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.033, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0266, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 208.9403, 'train_samples_per_second': 1.364, 'train_steps_per_second': 0.172, 'train_loss': 0.13557065092027187, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-24 23:36:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:37:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_10_50_0.9_0.35_115_3 epoch 1

------------------------------------------------

0.3814230956674305

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_10_50_0.9_0.35_115_3/generated_contents/1
INFO 11-24 23:38:14 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:38:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_10_50_0.9_0.35_115_3 epoch 2

------------------------------------------------

0.3568641043886194

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_10_50_0.9_0.35_115_3/generated_contents/2
INFO 11-24 23:39:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:39:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_10_50_0.9_0.35_115_3 epoch 3

------------------------------------------------

0.35345906048634024

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_10_50_0.9_0.35_115_3/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task281_exp_2
mv /data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-12 /data2/cyzhao/best_ckpt/NI_task281_exp_2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_20_10_50_0.9_0.35_115_3/checkpoint-36
searching parameters: task281_15_15_50_0.4_0.35_115_3
/home/cyzhao/NI_task281_exp_2/task281_15_15_50_0.4_0.35_115_3
/home/cyzhao/NI_task281_exp_2/task281_15_15_50_0.4_0.35_115_3/config.json
generate_and_write_inputs!
INFO 11-24 23:41:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:41:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 23:42:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:43:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 13
expected_example_num: 225
selection_ratio: 0.057777777777777775
finetune_vicuna!
{'loss': 0.4233, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'train_runtime': 156.7377, 'train_samples_per_second': 0.249, 'train_steps_per_second': 0.038, 'train_loss': 0.3095804899930954, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-4/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-6/optimizer.pt
validate!
last validate 0.
INFO 11-24 23:46:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:46:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_15_50_0.4_0.35_115_3 epoch 1

------------------------------------------------

0.3280648228944058

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_15_50_0.4_0.35_115_3/generated_contents/1
INFO 11-24 23:47:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-4', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-4', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:47:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_15_50_0.4_0.35_115_3 epoch 2

------------------------------------------------

0.27565044458116605

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_15_50_0.4_0.35_115_3/generated_contents/2
INFO 11-24 23:48:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:49:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_15_50_0.4_0.35_115_3 epoch 3

------------------------------------------------

0.37069936823086197

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_15_50_0.4_0.35_115_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-4
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_15_50_0.4_0.35_115_3/checkpoint-6
searching parameters: task281_15_10_45_0.4_0.35_125_3
/home/cyzhao/NI_task281_exp_2/task281_15_10_45_0.4_0.35_125_3
/home/cyzhao/NI_task281_exp_2/task281_15_10_45_0.4_0.35_125_3/config.json
generate_and_write_inputs!
INFO 11-24 23:50:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:50:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-24 23:51:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:51:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 1
expected_example_num: 150
selection_ratio: 0.006666666666666667
finetune_vicuna!
{'train_runtime': 152.206, 'train_samples_per_second': 0.02, 'train_steps_per_second': 0.02, 'train_loss': 0.11731883883476257, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-3/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-2/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-1/optimizer.pt
validate!
last validate 0.
INFO 11-24 23:54:21 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-1', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:54:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_10_45_0.4_0.35_125_3 epoch 1

------------------------------------------------

0.2916777356023563

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_10_45_0.4_0.35_125_3/generated_contents/1
INFO 11-24 23:55:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-2', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:55:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_10_45_0.4_0.35_125_3 epoch 2

------------------------------------------------

0.2838026626110652

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_10_45_0.4_0.35_125_3/generated_contents/2
INFO 11-24 23:56:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-3', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:57:04 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_10_45_0.4_0.35_125_3 epoch 3

------------------------------------------------

0.29370792097956194

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_10_45_0.4_0.35_125_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-2
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_10_45_0.4_0.35_125_3/checkpoint-3
searching parameters: task281_20_10_45_0.8_0.3_130_3
/home/cyzhao/NI_task281_exp_2/task281_20_10_45_0.8_0.3_130_3
/home/cyzhao/NI_task281_exp_2/task281_20_10_45_0.8_0.3_130_3/config.json
generate_and_write_inputs!
INFO 11-24 23:58:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-24 23:58:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-25 00:00:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:00:39 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 83
expected_example_num: 200
selection_ratio: 0.415
finetune_vicuna!
{'loss': 0.2499, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.2041, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.0983, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.0451, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.0387, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.0358, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.0152, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0112, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 196.1503, 'train_samples_per_second': 1.269, 'train_steps_per_second': 0.168, 'train_loss': 0.08507334613100145, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-11/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-33/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-22/optimizer.pt
validate!
last validate 0.
INFO 11-25 00:05:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-11', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-11', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:05:31 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_10_45_0.8_0.3_130_3 epoch 1

------------------------------------------------

0.2994702585703962

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_10_45_0.8_0.3_130_3/generated_contents/1
INFO 11-25 00:06:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:07:10 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_10_45_0.8_0.3_130_3 epoch 2

------------------------------------------------

0.34904597281721544

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_10_45_0.8_0.3_130_3/generated_contents/2
INFO 11-25 00:08:19 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-33', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-33', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:08:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_20_10_45_0.8_0.3_130_3 epoch 3

------------------------------------------------

0.3445775338289116

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_20_10_45_0.8_0.3_130_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-11
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_20_10_45_0.8_0.3_130_3/checkpoint-33
searching parameters: task281_15_10_50_0.9_0.5_120_3
/home/cyzhao/NI_task281_exp_2/task281_15_10_50_0.9_0.5_120_3
/home/cyzhao/NI_task281_exp_2/task281_15_10_50_0.9_0.5_120_3/config.json
generate_and_write_inputs!
INFO 11-25 00:09:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:09:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-25 00:11:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:12:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 78
expected_example_num: 150
selection_ratio: 0.52
finetune_vicuna!
{'loss': 0.3074, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.1589, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.1209, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.017, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.0368, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.023, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0167, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 199.7546, 'train_samples_per_second': 1.171, 'train_steps_per_second': 0.15, 'train_loss': 0.09152960491677126, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-25 00:16:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:17:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_10_50_0.9_0.5_120_3 epoch 1

------------------------------------------------

0.2952417443606212

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_10_50_0.9_0.5_120_3/generated_contents/1
INFO 11-25 00:18:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:18:36 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_10_50_0.9_0.5_120_3 epoch 2

------------------------------------------------

0.35290033390800385

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_10_50_0.9_0.5_120_3/generated_contents/2
INFO 11-25 00:19:30 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:19:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task281_15_10_50_0.9_0.5_120_3 epoch 3

------------------------------------------------

0.3309858899531566

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/task281_15_10_50_0.9_0.5_120_3/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task281_15_10_50_0.9_0.5_120_3/checkpoint-30
{'generation_epochs': 20, 'generation_batch_size': 10, 'generation_top_k': 50, 'generation_temperature': 0.9, 'min_frequency': 0.35, 'min_input_length': 115, 'training_epochs': 3}
test best ckpt.
INFO 11-25 00:20:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task281_exp_2', tokenizer='/data2/cyzhao/best_ckpt/NI_task281_exp_2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-25 00:21:08 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task281_exp_2

------------------------------------------------

0.37185482839772693

------------------------------------------------


The best ckpt on test set gain 0.37185482839772693
Genrated contents are stored in /home/cyzhao/NI_task281_exp_2/best_ckpt_generated_content
