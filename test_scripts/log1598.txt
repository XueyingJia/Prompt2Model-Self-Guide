[2023-11-28 22:58:19,016] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
task1598
searching parameters: task1598_30_10_50_0.6_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_10_50_0.6_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_10_50_0.6_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 22:58:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 22:58:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 23:00:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:01:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 78
expected_example_num: 300
selection_ratio: 0.26
finetune_vicuna!
{'loss': 1.5473, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 1.9891, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.6098, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.4305, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.2497, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.2456, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.2104, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.1469, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.1301, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 220.2251, 'train_samples_per_second': 1.063, 'train_steps_per_second': 0.177, 'train_loss': 0.5826132603180714, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-28 23:06:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:06:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_10_50_0.6_0.3_3_1 epoch 1

------------------------------------------------

0.4702962957349594

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_10_50_0.6_0.3_3_1/generated_contents/1
INFO 11-28 23:07:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:07:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_10_50_0.6_0.3_3_1 epoch 2

------------------------------------------------

0.47702755198596963

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_10_50_0.6_0.3_3_1/generated_contents/2
INFO 11-28 23:08:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:08:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_10_50_0.6_0.3_3_1 epoch 3

------------------------------------------------

0.4813337099508737

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_10_50_0.6_0.3_3_1/generated_contents/3
mv /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-39 /data2/cyzhao/best_ckpt/NI_task1598_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_50_0.6_0.3_3_1/checkpoint-26
searching parameters: task1598_30_15_50_0.8_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.8_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 23:09:10 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:09:24 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 23:12:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:12:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 123
expected_example_num: 450
selection_ratio: 0.2733333333333333
finetune_vicuna!
{'loss': 1.7581, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.19}
{'loss': 1.1007, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.38}
{'loss': 0.4079, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.475, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.76}
{'loss': 0.4285, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}
{'loss': 0.2873, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.201, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2851, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.52}
{'loss': 0.1694, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.1514, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}
{'loss': 0.1421, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.1}
{'loss': 0.0951, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.1118, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.48}
{'loss': 0.0888, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0554, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 208.9786, 'train_samples_per_second': 1.766, 'train_steps_per_second': 0.301, 'train_loss': 0.369260582895506, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-63/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-42/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-21/optimizer.pt
validate!
last validate 0.
INFO 11-28 23:17:20 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:17:35 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_50_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.49600088302284273

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.8_0.3_3_1/generated_contents/1
INFO 11-28 23:18:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-42', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-42', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:18:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_50_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.4840601316433752

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.8_0.3_3_1/generated_contents/2
INFO 11-28 23:19:24 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-63', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-63', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:19:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_50_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.4897421466564785

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1598_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-21 /data2/cyzhao/best_ckpt/NI_task1598_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-42
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.8_0.3_3_1/checkpoint-63
searching parameters: task1598_30_10_40_0.6_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_10_40_0.6_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_10_40_0.6_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-28 23:20:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:20:47 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 23:22:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:23:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 74
expected_example_num: 300
selection_ratio: 0.24666666666666667
finetune_vicuna!
{'loss': 1.771, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.913, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.3853, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.3135, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.2599, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.2151, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.2329, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.1512, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.1322, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'train_runtime': 190.7914, 'train_samples_per_second': 1.164, 'train_steps_per_second': 0.204, 'train_loss': 0.4559441147706447, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-13/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-39/optimizer.pt
validate!
last validate 0.
INFO 11-28 23:27:18 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-13', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-13', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:27:34 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_10_40_0.6_0.3_3_1 epoch 1

------------------------------------------------

0.4693162425010533

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_10_40_0.6_0.3_3_1/generated_contents/1
INFO 11-28 23:28:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:28:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_10_40_0.6_0.3_3_1 epoch 2

------------------------------------------------

0.4814609015152315

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_10_40_0.6_0.3_3_1/generated_contents/2
INFO 11-28 23:29:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-39', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-39', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:29:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_10_40_0.6_0.3_3_1 epoch 3

------------------------------------------------

0.47380561049388026

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_10_40_0.6_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-13
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_10_40_0.6_0.3_3_1/checkpoint-39
searching parameters: task1598_10_10_50_0.6_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_10_50_0.6_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_10_50_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-28 23:30:29 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:30:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 23:31:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:31:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 34
expected_example_num: 100
selection_ratio: 0.34
finetune_vicuna!
{'loss': 1.904, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.7312, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.3302, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1375, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 177.3701, 'train_samples_per_second': 0.575, 'train_steps_per_second': 0.101, 'train_loss': 0.7104126744800143, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-6/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-28 23:35:39 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-6', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-6', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:35:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_10_50_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.4520058450244999

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_10_50_0.6_0.4_3_1/generated_contents/1
INFO 11-28 23:36:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:36:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_10_50_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.4574925320720016

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_10_50_0.6_0.4_3_1/generated_contents/2
INFO 11-28 23:37:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:37:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_10_50_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.4701968511028214

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_10_50_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-6
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_10_50_0.6_0.4_3_1/checkpoint-18
searching parameters: task1598_30_20_40_0.4_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.4_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.4_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 23:38:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:38:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 23:41:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:42:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 157
expected_example_num: 600
selection_ratio: 0.26166666666666666
finetune_vicuna!
{'loss': 1.2902, 'learning_rate': 4.7530864197530866e-05, 'epoch': 0.15}
{'loss': 0.7505, 'learning_rate': 4.506172839506173e-05, 'epoch': 0.3}
{'loss': 0.3627, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.441, 'learning_rate': 4.012345679012346e-05, 'epoch': 0.59}
{'loss': 0.3445, 'learning_rate': 3.7654320987654326e-05, 'epoch': 0.74}
{'loss': 0.3751, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.2649, 'learning_rate': 3.271604938271605e-05, 'epoch': 1.04}
{'loss': 0.173, 'learning_rate': 3.0246913580246916e-05, 'epoch': 1.19}
{'loss': 0.2399, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.1763, 'learning_rate': 2.5308641975308646e-05, 'epoch': 1.48}
{'loss': 0.1677, 'learning_rate': 2.2839506172839506e-05, 'epoch': 1.63}
{'loss': 0.1739, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1674, 'learning_rate': 1.7901234567901236e-05, 'epoch': 1.93}
{'loss': 0.1969, 'learning_rate': 1.54320987654321e-05, 'epoch': 2.07}
{'loss': 0.0806, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.0964, 'learning_rate': 1.0493827160493827e-05, 'epoch': 2.37}
{'loss': 0.092, 'learning_rate': 8.02469135802469e-06, 'epoch': 2.52}
{'loss': 0.0886, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.1045, 'learning_rate': 3.0864197530864196e-06, 'epoch': 2.81}
{'loss': 0.099, 'learning_rate': 6.17283950617284e-07, 'epoch': 2.96}
{'train_runtime': 217.6064, 'train_samples_per_second': 2.164, 'train_steps_per_second': 0.372, 'train_loss': 0.28092054146583434, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-81/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-54/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-27/optimizer.pt
validate!
last validate 0.
INFO 11-28 23:47:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:48:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.4_0.35_3_1 epoch 1

------------------------------------------------

0.43605448012515513

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.4_0.35_3_1/generated_contents/1
INFO 11-28 23:48:48 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-54', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-54', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:49:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.4_0.35_3_1 epoch 2

------------------------------------------------

0.4841566951412803

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.4_0.35_3_1/generated_contents/2
INFO 11-28 23:49:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-81', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-81', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:49:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.4_0.35_3_1 epoch 3

------------------------------------------------

0.4850957090121174

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.4_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-27
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-54
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.4_0.35_3_1/checkpoint-81
searching parameters: task1598_20_10_40_0.8_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_10_40_0.8_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_10_40_0.8_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-28 23:50:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:50:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-28 23:52:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:52:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 49
expected_example_num: 200
selection_ratio: 0.245
finetune_vicuna!
{'loss': 1.5491, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.6701, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.2213, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.3082, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1625, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.1057, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 183.9013, 'train_samples_per_second': 0.799, 'train_steps_per_second': 0.147, 'train_loss': 0.4635659456253052, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-28 23:56:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:56:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_10_40_0.8_0.35_3_1 epoch 1

------------------------------------------------

0.48493964276683654

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_10_40_0.8_0.35_3_1/generated_contents/1
INFO 11-28 23:57:41 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:57:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_10_40_0.8_0.35_3_1 epoch 2

------------------------------------------------

0.4741336702548401

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_10_40_0.8_0.35_3_1/generated_contents/2
INFO 11-28 23:58:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:58:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_10_40_0.8_0.35_3_1 epoch 3

------------------------------------------------

0.47562810022471624

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_10_40_0.8_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_10_40_0.8_0.35_3_1/checkpoint-27
searching parameters: task1598_30_20_40_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-28 23:59:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-28 23:59:58 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 00:03:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:03:37 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 157
expected_example_num: 600
selection_ratio: 0.26166666666666666
finetune_vicuna!
{'loss': 1.535, 'learning_rate': 4.7530864197530866e-05, 'epoch': 0.15}
{'loss': 0.9733, 'learning_rate': 4.506172839506173e-05, 'epoch': 0.3}
{'loss': 0.6872, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.6763, 'learning_rate': 4.012345679012346e-05, 'epoch': 0.59}
{'loss': 0.3687, 'learning_rate': 3.7654320987654326e-05, 'epoch': 0.74}
{'loss': 0.374, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.2793, 'learning_rate': 3.271604938271605e-05, 'epoch': 1.04}
{'loss': 0.1513, 'learning_rate': 3.0246913580246916e-05, 'epoch': 1.19}
{'loss': 0.1609, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.192, 'learning_rate': 2.5308641975308646e-05, 'epoch': 1.48}
{'loss': 0.1991, 'learning_rate': 2.2839506172839506e-05, 'epoch': 1.63}
{'loss': 0.2184, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.2066, 'learning_rate': 1.7901234567901236e-05, 'epoch': 1.93}
{'loss': 0.1116, 'learning_rate': 1.54320987654321e-05, 'epoch': 2.07}
{'loss': 0.0868, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.1472, 'learning_rate': 1.0493827160493827e-05, 'epoch': 2.37}
{'loss': 0.0897, 'learning_rate': 8.02469135802469e-06, 'epoch': 2.52}
{'loss': 0.0772, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0878, 'learning_rate': 3.0864197530864196e-06, 'epoch': 2.81}
{'loss': 0.0688, 'learning_rate': 6.17283950617284e-07, 'epoch': 2.96}
{'train_runtime': 221.989, 'train_samples_per_second': 2.122, 'train_steps_per_second': 0.365, 'train_loss': 0.33323807462497995, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-81/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-54/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-27/optimizer.pt
validate!
last validate 0.
INFO 11-29 00:09:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:09:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.48247617765480566

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.7_0.4_3_1/generated_contents/1
INFO 11-29 00:10:01 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-54', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-54', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:10:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.47154225494649094

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.7_0.4_3_1/generated_contents/2
INFO 11-29 00:11:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-81', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-81', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:11:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.49916763086862875

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1598_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-81 /data2/cyzhao/best_ckpt/NI_task1598_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-27
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.7_0.4_3_1/checkpoint-54
searching parameters: task1598_30_20_40_0.6_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.6_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.6_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 00:12:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:12:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 00:15:45 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:15:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 131
expected_example_num: 600
selection_ratio: 0.21833333333333332
finetune_vicuna!
{'loss': 2.1247, 'learning_rate': 4.696969696969697e-05, 'epoch': 0.18}
{'loss': 1.2014, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.4675, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.55}
{'loss': 0.5407, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.5147, 'learning_rate': 3.484848484848485e-05, 'epoch': 0.91}
{'loss': 0.2793, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2453, 'learning_rate': 2.878787878787879e-05, 'epoch': 1.27}
{'loss': 0.243, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.2555, 'learning_rate': 2.272727272727273e-05, 'epoch': 1.64}
{'loss': 0.2521, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.2717, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1073, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.1318, 'learning_rate': 1.0606060606060607e-05, 'epoch': 2.36}
{'loss': 0.128, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.1224, 'learning_rate': 4.5454545454545455e-06, 'epoch': 2.73}
{'loss': 0.1033, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 204.8128, 'train_samples_per_second': 1.919, 'train_steps_per_second': 0.322, 'train_loss': 0.4265921851902297, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-44/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-22/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-66/optimizer.pt
validate!
last validate 0.
INFO 11-29 00:20:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:21:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.6_0.4_3_1 epoch 1

------------------------------------------------

0.4886331767856867

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.6_0.4_3_1/generated_contents/1
INFO 11-29 00:21:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-44', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:22:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.6_0.4_3_1 epoch 2

------------------------------------------------

0.4747097700435856

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.6_0.4_3_1/generated_contents/2
INFO 11-29 00:22:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-66', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-66', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:23:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.6_0.4_3_1 epoch 3

------------------------------------------------

0.49080505698656507

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.6_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-44
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.6_0.4_3_1/checkpoint-66
searching parameters: task1598_30_15_40_0.8_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_40_0.8_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_40_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 00:23:58 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:24:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 00:27:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:27:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 113
expected_example_num: 450
selection_ratio: 0.2511111111111111
finetune_vicuna!
{'loss': 1.7926, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.7269, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.4361, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.3409, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.3836, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.2016, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.2016, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.1733, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.1732, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.1392, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0832, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.1007, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.0977, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.0803, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 200.5915, 'train_samples_per_second': 1.69, 'train_steps_per_second': 0.284, 'train_loss': 0.3468907784605235, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-29 00:31:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:32:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_40_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.46261061699941586

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_40_0.8_0.3_3_1/generated_contents/1
INFO 11-29 00:32:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:33:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_40_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.5029645181737747

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_40_0.8_0.3_3_1/generated_contents/2
INFO 11-29 00:33:56 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:34:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_40_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.5014993221480849

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_40_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/best_ckpt/NI_task1598_exp_1
mv /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-38 /data2/cyzhao/best_ckpt/NI_task1598_exp_1
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_40_0.8_0.3_3_1/checkpoint-57
searching parameters: task1598_30_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.7_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.7_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 00:34:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:35:16 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 00:38:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:38:17 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 119
expected_example_num: 450
selection_ratio: 0.2644444444444444
finetune_vicuna!
{'loss': 1.3724, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}
{'loss': 0.727, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.4552, 'learning_rate': 4e-05, 'epoch': 0.6}
{'loss': 0.5855, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3972, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1512, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.1999, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}
{'loss': 0.2437, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.2441, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.2628, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1268, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}
{'loss': 0.1107, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.1212, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}
{'loss': 0.1046, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'loss': 0.0708, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 201.1725, 'train_samples_per_second': 1.775, 'train_steps_per_second': 0.298, 'train_loss': 0.344878185292085, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-40/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-20/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-60/optimizer.pt
validate!
last validate 0.
INFO 11-29 00:43:11 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:43:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_50_0.7_0.35_3_1 epoch 1

------------------------------------------------

0.4617838757298241

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.7_0.35_3_1/generated_contents/1
INFO 11-29 00:44:12 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-40', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-40', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:44:28 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_50_0.7_0.35_3_1 epoch 2

------------------------------------------------

0.479125104379089

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.7_0.35_3_1/generated_contents/2
INFO 11-29 00:45:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-60', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-60', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:45:41 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_50_0.7_0.35_3_1 epoch 3

------------------------------------------------

0.4764994494867409

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_50_0.7_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-40
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_50_0.7_0.35_3_1/checkpoint-60
searching parameters: task1598_10_15_45_0.5_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_15_45_0.5_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_15_45_0.5_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 00:46:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:46:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 00:47:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:48:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 54
expected_example_num: 150
selection_ratio: 0.36
finetune_vicuna!
{'loss': 1.6093, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}
{'loss': 0.6698, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}
{'loss': 0.3781, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.243, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}
{'loss': 0.1834, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}
{'loss': 0.1111, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'train_runtime': 180.3166, 'train_samples_per_second': 0.898, 'train_steps_per_second': 0.15, 'train_loss': 0.4844560247880441, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-9/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-27/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-18/optimizer.pt
validate!
last validate 0.
INFO 11-29 00:52:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-9', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-9', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:52:14 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_15_45_0.5_0.3_3_1 epoch 1

------------------------------------------------

0.48330171855452286

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_15_45_0.5_0.3_3_1/generated_contents/1
INFO 11-29 00:53:04 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-18', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-18', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:53:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_15_45_0.5_0.3_3_1 epoch 2

------------------------------------------------

0.4806016748049856

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_15_45_0.5_0.3_3_1/generated_contents/2
INFO 11-29 00:53:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-27', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-27', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:54:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_15_45_0.5_0.3_3_1 epoch 3

------------------------------------------------

0.48243576337408894

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_15_45_0.5_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-9
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-18
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_45_0.5_0.3_3_1/checkpoint-27
searching parameters: task1598_20_20_40_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_20_40_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_20_40_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 00:55:00 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:55:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 00:57:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 00:57:46 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 110
expected_example_num: 400
selection_ratio: 0.275
finetune_vicuna!
{'loss': 1.5427, 'learning_rate': 4.649122807017544e-05, 'epoch': 0.21}
{'loss': 0.7624, 'learning_rate': 4.298245614035088e-05, 'epoch': 0.42}
{'loss': 0.5468, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.63}
{'loss': 0.4079, 'learning_rate': 3.5964912280701756e-05, 'epoch': 0.84}
{'loss': 0.4967, 'learning_rate': 3.24561403508772e-05, 'epoch': 1.05}
{'loss': 0.177, 'learning_rate': 2.8947368421052634e-05, 'epoch': 1.26}
{'loss': 0.2039, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.47}
{'loss': 0.231, 'learning_rate': 2.1929824561403507e-05, 'epoch': 1.68}
{'loss': 0.1635, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.89}
{'loss': 0.1451, 'learning_rate': 1.4912280701754386e-05, 'epoch': 2.11}
{'loss': 0.0837, 'learning_rate': 1.1403508771929824e-05, 'epoch': 2.32}
{'loss': 0.1122, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.53}
{'loss': 0.1175, 'learning_rate': 4.3859649122807014e-06, 'epoch': 2.74}
{'loss': 0.1152, 'learning_rate': 8.771929824561404e-07, 'epoch': 2.95}
{'train_runtime': 201.7612, 'train_samples_per_second': 1.636, 'train_steps_per_second': 0.283, 'train_loss': 0.3601183547523984, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-38/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-19/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-57/optimizer.pt
validate!
last validate 0.
INFO 11-29 01:02:38 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-19', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-19', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:02:52 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_20_40_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.4601109630453509

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_20_40_0.7_0.4_3_1/generated_contents/1
INFO 11-29 01:03:36 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-38', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-38', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:03:50 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_20_40_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.47887195364384955

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_20_40_0.7_0.4_3_1/generated_contents/2
INFO 11-29 01:04:33 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-57', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-57', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:04:49 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_20_40_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.47063460922659295

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_20_40_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-19
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-38
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_20_40_0.7_0.4_3_1/checkpoint-57
searching parameters: task1598_30_20_45_0.8_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.8_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.8_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 01:05:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:05:54 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 01:09:09 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:09:25 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 143
expected_example_num: 600
selection_ratio: 0.23833333333333334
finetune_vicuna!
{'loss': 1.9176, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.17}
{'loss': 1.1077, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.5723, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}
{'loss': 0.5906, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.4608, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}
{'loss': 0.4374, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.1961, 'learning_rate': 3.055555555555556e-05, 'epoch': 1.17}
{'loss': 0.2485, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2833, 'learning_rate': 2.5e-05, 'epoch': 1.5}
{'loss': 0.2249, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2054, 'learning_rate': 1.9444444444444445e-05, 'epoch': 1.83}
{'loss': 0.216, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1191, 'learning_rate': 1.388888888888889e-05, 'epoch': 2.17}
{'loss': 0.1048, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.1345, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}
{'loss': 0.1015, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0981, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.83}
{'loss': 0.0783, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 214.8012, 'train_samples_per_second': 1.997, 'train_steps_per_second': 0.335, 'train_loss': 0.3942675089670552, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-72/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-48/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-29 01:14:47 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:15:01 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_45_0.8_0.4_3_1 epoch 1

------------------------------------------------

0.46480158259173243

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.8_0.4_3_1/generated_contents/1
INFO 11-29 01:15:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-48', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-48', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:16:11 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_45_0.8_0.4_3_1 epoch 2

------------------------------------------------

0.48019779857882056

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.8_0.4_3_1/generated_contents/2
INFO 11-29 01:16:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-72', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-72', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:17:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_45_0.8_0.4_3_1 epoch 3

------------------------------------------------

0.4903374716283233

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.8_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-48
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.8_0.4_3_1/checkpoint-72
searching parameters: task1598_20_15_40_0.7_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_15_40_0.7_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_15_40_0.7_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 01:17:54 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:18:09 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 01:20:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:20:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 69
expected_example_num: 300
selection_ratio: 0.23
finetune_vicuna!
{'loss': 1.8121, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}
{'loss': 0.6579, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}
{'loss': 0.5423, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'loss': 0.2, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.2013, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'loss': 0.2242, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1083, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}
{'loss': 0.0733, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.1247, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 186.9234, 'train_samples_per_second': 1.107, 'train_steps_per_second': 0.193, 'train_loss': 0.43823328283098006, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-12/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-36/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-24/optimizer.pt
validate!
last validate 0.
INFO 11-29 01:24:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-12', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-12', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:24:59 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_15_40_0.7_0.3_3_1 epoch 1

------------------------------------------------

0.4828778810129229

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_15_40_0.7_0.3_3_1/generated_contents/1
INFO 11-29 01:25:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-24', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-24', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:26:12 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_15_40_0.7_0.3_3_1 epoch 2

------------------------------------------------

0.47519238972119676

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_15_40_0.7_0.3_3_1/generated_contents/2
INFO 11-29 01:26:55 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-36', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-36', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:27:13 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_15_40_0.7_0.3_3_1 epoch 3

------------------------------------------------

0.4882853974027257

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_15_40_0.7_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-12
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-24
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_40_0.7_0.3_3_1/checkpoint-36
searching parameters: task1598_30_20_40_0.5_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.5_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.5_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 01:28:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:28:18 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 01:31:23 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:31:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 99
expected_example_num: 600
selection_ratio: 0.165
finetune_vicuna!
{'loss': 2.17, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}
{'loss': 1.4847, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}
{'loss': 0.4421, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}
{'loss': 0.4815, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}
{'loss': 0.35, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}
{'loss': 0.3019, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}
{'loss': 0.2424, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}
{'loss': 0.2208, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}
{'loss': 0.2405, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}
{'loss': 0.1574, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}
{'loss': 0.076, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}
{'loss': 0.1567, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}
{'train_runtime': 196.4718, 'train_samples_per_second': 1.512, 'train_steps_per_second': 0.26, 'train_loss': 0.5040190447779263, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-51/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-17/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-34/optimizer.pt
validate!
last validate 0.
INFO 11-29 01:36:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-17', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-17', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:36:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.5_0.4_3_1 epoch 1

------------------------------------------------

0.48273257380650136

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.5_0.4_3_1/generated_contents/1
INFO 11-29 01:37:26 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-34', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-34', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:37:44 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.5_0.4_3_1 epoch 2

------------------------------------------------

0.4725103454186924

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.5_0.4_3_1/generated_contents/2
INFO 11-29 01:38:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-51', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-51', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:38:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_40_0.5_0.4_3_1 epoch 3

------------------------------------------------

0.4642386666501387

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_40_0.5_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-17
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-34
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_40_0.5_0.4_3_1/checkpoint-51
searching parameters: task1598_10_15_40_0.4_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_15_40_0.4_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_15_40_0.4_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 01:39:32 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:39:48 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 01:40:25 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:40:40 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 42
expected_example_num: 150
selection_ratio: 0.28
finetune_vicuna!
{'loss': 1.8255, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}
{'loss': 0.6686, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}
{'loss': 0.3268, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}
{'loss': 0.229, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}
{'loss': 0.1605, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}
{'train_runtime': 181.5717, 'train_samples_per_second': 0.694, 'train_steps_per_second': 0.116, 'train_loss': 0.6220248462188811, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-21/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-14/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-7/optimizer.pt
validate!
last validate 0.
INFO 11-29 01:44:37 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-7', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-7', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:44:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_15_40_0.4_0.3_3_1 epoch 1

------------------------------------------------

0.47414988171439615

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_15_40_0.4_0.3_3_1/generated_contents/1
INFO 11-29 01:45:40 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-14', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-14', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:45:55 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_15_40_0.4_0.3_3_1 epoch 2

------------------------------------------------

0.45314959443876585

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_15_40_0.4_0.3_3_1/generated_contents/2
INFO 11-29 01:46:51 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-21', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-21', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:47:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_15_40_0.4_0.3_3_1 epoch 3

------------------------------------------------

0.4647015400225163

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_15_40_0.4_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-7
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-14
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_15_40_0.4_0.3_3_1/checkpoint-21
searching parameters: task1598_30_20_45_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 01:48:02 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:48:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 01:51:49 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:52:05 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 156
expected_example_num: 600
selection_ratio: 0.26
finetune_vicuna!
{'loss': 1.7065, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.15}
{'loss': 0.5523, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.31}
{'loss': 0.5298, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.46}
{'loss': 0.5361, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.62}
{'loss': 0.424, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.77}
{'loss': 0.3473, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.92}
{'loss': 0.2922, 'learning_rate': 3.205128205128206e-05, 'epoch': 1.08}
{'loss': 0.2407, 'learning_rate': 2.948717948717949e-05, 'epoch': 1.23}
{'loss': 0.2124, 'learning_rate': 2.6923076923076923e-05, 'epoch': 1.38}
{'loss': 0.1863, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}
{'loss': 0.2249, 'learning_rate': 2.1794871794871795e-05, 'epoch': 1.69}
{'loss': 0.132, 'learning_rate': 1.923076923076923e-05, 'epoch': 1.85}
{'loss': 0.1788, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1216, 'learning_rate': 1.4102564102564104e-05, 'epoch': 2.15}
{'loss': 0.089, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.31}
{'loss': 0.1205, 'learning_rate': 8.974358974358976e-06, 'epoch': 2.46}
{'loss': 0.0788, 'learning_rate': 6.41025641025641e-06, 'epoch': 2.62}
{'loss': 0.0935, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.77}
{'loss': 0.0809, 'learning_rate': 1.282051282051282e-06, 'epoch': 2.92}
{'train_runtime': 202.6502, 'train_samples_per_second': 2.309, 'train_steps_per_second': 0.385, 'train_loss': 0.3173593229208237, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-78/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-26/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-52/optimizer.pt
validate!
last validate 0.
INFO 11-29 01:57:17 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-26', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-26', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:57:33 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_45_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.4639048463804603

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.7_0.4_3_1/generated_contents/1
INFO 11-29 01:58:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-52', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-52', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:58:43 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_45_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.4924415487616967

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.7_0.4_3_1/generated_contents/2
INFO 11-29 01:59:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-78', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-78', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 01:59:42 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_20_45_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.5003451634470869

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_20_45_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-26
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-52
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_20_45_0.7_0.4_3_1/checkpoint-78
searching parameters: task1598_30_15_45_0.8_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_45_0.8_0.3_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_30_15_45_0.8_0.3_3_1/config.json
generate_and_write_inputs!
INFO 11-29 02:00:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:00:53 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 02:03:53 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:04:07 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 131
expected_example_num: 450
selection_ratio: 0.2911111111111111
finetune_vicuna!
{'loss': 1.6347, 'learning_rate': 4.696969696969697e-05, 'epoch': 0.18}
{'loss': 0.8145, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.36}
{'loss': 0.4796, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.55}
{'loss': 0.5513, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.73}
{'loss': 0.4436, 'learning_rate': 3.484848484848485e-05, 'epoch': 0.91}
{'loss': 0.3174, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.09}
{'loss': 0.2328, 'learning_rate': 2.878787878787879e-05, 'epoch': 1.27}
{'loss': 0.2055, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.45}
{'loss': 0.1762, 'learning_rate': 2.272727272727273e-05, 'epoch': 1.64}
{'loss': 0.1861, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.82}
{'loss': 0.2104, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1155, 'learning_rate': 1.3636363636363637e-05, 'epoch': 2.18}
{'loss': 0.092, 'learning_rate': 1.0606060606060607e-05, 'epoch': 2.36}
{'loss': 0.0992, 'learning_rate': 7.5757575757575764e-06, 'epoch': 2.55}
{'loss': 0.0894, 'learning_rate': 4.5454545454545455e-06, 'epoch': 2.73}
{'loss': 0.0995, 'learning_rate': 1.5151515151515152e-06, 'epoch': 2.91}
{'train_runtime': 194.9261, 'train_samples_per_second': 2.016, 'train_steps_per_second': 0.339, 'train_loss': 0.3519926260818135, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-44/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-22/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-66/optimizer.pt
validate!
last validate 0.
INFO 11-29 02:08:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-22', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-22', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:09:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_45_0.8_0.3_3_1 epoch 1

------------------------------------------------

0.47999404404458773

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_45_0.8_0.3_3_1/generated_contents/1
INFO 11-29 02:09:57 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-44', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-44', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:10:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_45_0.8_0.3_3_1 epoch 2

------------------------------------------------

0.4838524633095939

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_45_0.8_0.3_3_1/generated_contents/2
INFO 11-29 02:10:59 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-66', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-66', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:11:15 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_30_15_45_0.8_0.3_3_1 epoch 3

------------------------------------------------

0.4880568763829935

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_30_15_45_0.8_0.3_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-22
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-44
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_30_15_45_0.8_0.3_3_1/checkpoint-66
searching parameters: task1598_10_20_45_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_20_45_0.7_0.4_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_10_20_45_0.7_0.4_3_1/config.json
generate_and_write_inputs!
INFO 11-29 02:12:07 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:12:22 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 02:13:34 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:13:51 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 59
expected_example_num: 200
selection_ratio: 0.295
finetune_vicuna!
{'loss': 1.7791, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}
{'loss': 0.8302, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.4884, 'learning_rate': 3e-05, 'epoch': 1.2}
{'loss': 0.3647, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.2444, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'loss': 0.1658, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.1433, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}
{'train_runtime': 174.0887, 'train_samples_per_second': 1.017, 'train_steps_per_second': 0.172, 'train_loss': 0.5447849343220393, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-10/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-20/optimizer.pt
validate!
last validate 0.
INFO 11-29 02:17:52 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-10', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:18:06 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_20_45_0.7_0.4_3_1 epoch 1

------------------------------------------------

0.44645943629916923

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_20_45_0.7_0.4_3_1/generated_contents/1
INFO 11-29 02:19:06 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-20', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-20', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:19:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_20_45_0.7_0.4_3_1 epoch 2

------------------------------------------------

0.4747490610685969

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_20_45_0.7_0.4_3_1/generated_contents/2
INFO 11-29 02:20:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:20:20 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_10_20_45_0.7_0.4_3_1 epoch 3

------------------------------------------------

0.4788525920211016

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_10_20_45_0.7_0.4_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-10
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-20
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_10_20_45_0.7_0.4_3_1/checkpoint-30
searching parameters: task1598_20_15_45_0.8_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_15_45_0.8_0.35_3_1
/home/cyzhao/NI_task1598_exp_1/task1598_20_15_45_0.8_0.35_3_1/config.json
generate_and_write_inputs!
INFO 11-29 02:21:08 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:21:23 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
annotate_and_write_outputs!
INFO 11-29 02:23:03 llm_engine.py:72] Initializing an LLM engine with config: model='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer='/data/ckpts/huggingface/models/models--lmsys--vicuna-7b-v1.5/snapshots/de56c35b1763eaae20f4d60efd64af0a9091ebe5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:23:19 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024
generated_example_num: 87
expected_example_num: 300
selection_ratio: 0.29
finetune_vicuna!
{'loss': 1.4117, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}
{'loss': 0.8192, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}
{'loss': 0.491, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
{'loss': 0.3946, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}
{'loss': 0.2515, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}
{'loss': 0.158, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
{'loss': 0.1965, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}
{'loss': 0.1536, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}
{'loss': 0.0766, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0811, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}
{'loss': 0.0931, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}
{'train_runtime': 184.1275, 'train_samples_per_second': 1.417, 'train_steps_per_second': 0.244, 'train_loss': 0.3685267698433664, 'epoch': 3.0}
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-30/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-45/optimizer.pt
Deleting /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-15/optimizer.pt
validate!
last validate 0.
INFO 11-29 02:27:42 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-15', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-15', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:27:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_15_45_0.8_0.35_3_1 epoch 1

------------------------------------------------

0.4825853959445559

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_15_45_0.8_0.35_3_1/generated_contents/1
INFO 11-29 02:28:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-30', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-30', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:28:57 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_15_45_0.8_0.35_3_1 epoch 2

------------------------------------------------

0.4880527194046581

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_15_45_0.8_0.35_3_1/generated_contents/2
INFO 11-29 02:29:44 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-45', tokenizer='/data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-45', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:30:00 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of task1598_20_15_45_0.8_0.35_3_1 epoch 3

------------------------------------------------

0.4937573473633056

------------------------------------------------


Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/task1598_20_15_45_0.8_0.35_3_1/generated_contents/3
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-15
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-30
rm -rf /data2/cyzhao/ckpt_data_p2ms/task1598_20_15_45_0.8_0.35_3_1/checkpoint-45
{'generation_epochs': 30, 'generation_batch_size': 15, 'generation_top_k': 40, 'generation_temperature': 0.8, 'min_frequency': 0.3, 'training_epochs': 3}
test best ckpt.
INFO 11-29 02:30:46 llm_engine.py:72] Initializing an LLM engine with config: model='/data2/cyzhao/best_ckpt/NI_task1598_exp_1', tokenizer='/data2/cyzhao/best_ckpt/NI_task1598_exp_1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, seed=0)
INFO 11-29 02:31:03 llm_engine.py:207] # GPU blocks: 16502, # CPU blocks: 1024


result of /data2/cyzhao/best_ckpt/NI_task1598_exp_1

------------------------------------------------

0.5007488457348604

------------------------------------------------


The best ckpt on test set gain 0.5007488457348604
Genrated contents are stored in /home/cyzhao/NI_task1598_exp_1/best_ckpt_generated_content
